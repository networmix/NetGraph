{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"NetGraph","text":"<p>Scenario-driven network modeling and analysis framework combining Python's flexibility with high-performance C++ algorithms.</p>"},{"location":"#overview","title":"Overview","text":"<p>NetGraph enables declarative modeling of network topologies, traffic matrices, and failure scenarios. It delegates computationally intensive graph algorithms to NetGraph-Core while providing a rich Python API and CLI for orchestration.</p>"},{"location":"#architecture","title":"Architecture","text":"<p>NetGraph employs a hybrid Python+C++ architecture:</p> <ul> <li>Python layer (NetGraph): Scenario DSL parsing, workflow orchestration, result aggregation, and high-level APIs.</li> <li>C++ layer (NetGraph-Core): Performance-critical graph algorithms (SPF, KSP, Max-Flow) executing in optimized C++ with the GIL released.</li> </ul>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#modeling-dsl","title":"Modeling &amp; DSL","text":"<ul> <li>Declarative Scenarios: Define topology, traffic, and workflows in validated YAML.</li> <li>Blueprints: Reusable topology templates (e.g., Clos fabrics, regions) with parameterized expansion.</li> <li>Strict Multigraph: Deterministic graph representation with stable edge IDs.</li> </ul>"},{"location":"#failure-analysis","title":"Failure Analysis","text":"<ul> <li>Policy Engine: Weighted failure modes with multiple policy rules per mode.</li> <li>Non-Destructive: Runtime exclusions simulate failures without modifying the base topology.</li> <li>Risk Groups: Model shared fate (e.g., fiber cuts, power zones).</li> </ul>"},{"location":"#traffic-engineering","title":"Traffic Engineering","text":"<ul> <li>Routing Modes: Unified modeling of IP Routing (static costs, oblivious to congestion) and Traffic Engineering (dynamic residuals, congestion-aware).</li> <li>Flow Placement: Strategies for ECMP (Equal-Cost Multi-Path) and WCMP (Weighted Cost Multi-Path).</li> <li>Capacity Analysis: Compute max-flow envelopes and demand allocation with configurable placement policies.</li> </ul>"},{"location":"#workflow-integration","title":"Workflow &amp; Integration","text":"<ul> <li>Structured Results: Export analysis artifacts to JSON for downstream processing.</li> <li>CLI: Comprehensive command-line interface for validation and execution.</li> <li>Python API: Full programmatic access to all modeling and solving capabilities.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Installation Guide - Python package installation</li> <li>Tutorial - Run scenarios (CLI) and code examples</li> </ul>"},{"location":"#examples","title":"Examples","text":"<ul> <li>Bundled Scenarios - Ready-to-run scenarios (<code>square_mesh</code>, <code>backbone_clos</code>, <code>nsfnet</code>)</li> <li>Basic Example - Simple graph example</li> <li>Clos Fabric Analysis - Analyze a 3-tier Clos network</li> </ul>"},{"location":"#reference-documentation","title":"Reference Documentation","text":"<ul> <li>Design - Architecture, model, algorithms, and workflow</li> <li>DSL Reference - YAML syntax guide</li> <li>Workflow Reference - Analysis workflow configuration</li> <li>CLI Reference - Command-line interface</li> <li>Schema Reference - JSON Schema and validation</li> <li>API Reference - Python API documentation</li> <li>Auto-Generated API Reference - Complete API docs</li> </ul>"},{"location":"examples/basic/","title":"Basic Example","text":"<p>This example builds a tiny topology inline to show APIs. For real analysis, prefer running a provided scenario and generating metrics via the CLI.</p> <p>See Tutorial for CLI usage and bundled scenarios.</p>"},{"location":"examples/basic/#creating-a-simple-network","title":"Creating a Simple Network","text":"<p>Network Topology:</p> <pre><code>             [1,1] &amp; [1,2]     [1,1] &amp; [1,2]\n      A -------------------- B ---------------- C\n      |                                         |\n      |    [2,3]                                | [2,3]\n      +-------------------- D -----------------+\n\n[1,1] and [1,2] are parallel edges between A and B.\nThey have the same metric of 1 but different capacities (1 and 2).\n</code></pre> <p>Let's create this network by using NetGraph's scenario system:</p> <pre><code>from ngraph.scenario import Scenario\nfrom ngraph import analyze, Mode, FlowPlacement\n\n# Define network topology with parallel paths\nscenario_yaml = \"\"\"\nseed: 1234  # Optional: ensures reproducible results\n\nnetwork:\n  name: \"fundamentals_example\"\n\n  # Create individual nodes\n  nodes:\n    A: {}\n    B: {}\n    C: {}\n    D: {}\n\n  # Create links with different capacities and costs\n  links:\n    # Parallel edges between A-&gt;B\n    - source: A\n      target: B\n      capacity: 1\n      cost: 1\n    - source: A\n      target: B\n      capacity: 2\n      cost: 1\n\n    # Parallel edges between B-&gt;C\n    - source: B\n      target: C\n      capacity: 1\n      cost: 1\n    - source: B\n      target: C\n      capacity: 2\n      cost: 1\n\n    # Alternative path A-&gt;D-&gt;C\n    - source: A\n      target: D\n      capacity: 3\n      cost: 2\n    - source: D\n      target: C\n      capacity: 3\n      cost: 2\n\"\"\"\n\n# Create the network\nscenario = Scenario.from_yaml(scenario_yaml)\nnetwork = scenario.network\n</code></pre> <p>Note that here we used a simple <code>nodes</code> and <code>links</code> structure to directly define the network topology. The optional <code>seed</code> parameter ensures reproducible results when using randomized workflow steps. In more complex scenarios, you would typically use node groups with <code>count</code> and <code>template</code> to define groups of nodes and link rules to define their connections, or even leverage the <code>blueprints</code> to create reusable components. This advanced functionality is explained in the DSL Reference and used in the Clos Fabric Analysis example.</p>"},{"location":"examples/basic/#flow-analysis-variants","title":"Flow Analysis Variants","text":"<p>Now let's run MaxFlow using the <code>analyze()</code> API:</p> <pre><code># 1. \"True\" maximum flow (uses all available paths)\nmax_flow_all = analyze(network).max_flow(\"^A$\", \"^C$\", mode=Mode.COMBINE)\nprint(f\"Maximum flow (all paths): {max_flow_all}\")\n# Result: {('^A$', '^C$'): 6.0} (uses both A-&gt;B-&gt;C path capacity of 3 and A-&gt;D-&gt;C path capacity of 3)\n\n# 2. Flow along shortest paths only\nmax_flow_shortest = analyze(network).max_flow(\n    \"^A$\",\n    \"^C$\",\n    mode=Mode.COMBINE,\n    shortest_path=True\n)\nprint(f\"Flow on shortest paths: {max_flow_shortest}\")\n# Result: {('^A$', '^C$'): 3.0} (only uses A-&gt;B-&gt;C path, ignoring higher-cost A-&gt;D-&gt;C path)\n\n# 3. Equal-balanced flow placement on shortest paths\nmax_flow_shortest_balanced = analyze(network).max_flow(\n    \"^A$\",\n    \"^C$\",\n    mode=Mode.COMBINE,\n    shortest_path=True,\n    flow_placement=FlowPlacement.EQUAL_BALANCED\n)\nprint(f\"Equal-balanced flow: {max_flow_shortest_balanced}\")\n# Result: {('^A$', '^C$'): 2.0} (splits flow equally across parallel edges in A-&gt;B and B-&gt;C)\n</code></pre>"},{"location":"examples/basic/#results-interpretation","title":"Results Interpretation","text":"<ul> <li>\"True\" MaxFlow: Uses all available paths regardless of their cost</li> <li>Shortest Path: Only uses paths with the minimum cost</li> <li>EQUAL_BALANCED Flow Placement: Distributes flows equally across all parallel paths. The total flow can be limited by the smallest capacity path.</li> </ul> <p><code>EQUAL_BALANCED</code> flow placement is typically used with <code>shortest_path=True</code> to simulate traditional ECMP behavior, where flows are split equally across equal-cost paths.</p>"},{"location":"examples/basic/#cost-distribution","title":"Cost Distribution","text":"<p>Cost distribution shows how flow splits across path costs for latency/span analysis:</p> <pre><code># Get flow analysis with cost distribution\nresult = analyze(network).max_flow_detailed(\n    \"^A$\",\n    \"^C$\",\n    mode=Mode.COMBINE\n)\n\n# Extract flow value and summary\n(src_label, target_label), summary = next(iter(result.items()))\n\nprint(f\"Total flow: {summary.total_flow}\")\nprint(f\"Cost distribution: {summary.cost_distribution}\")\n\n# Example output:\n# Total flow: 6.0\n# Cost distribution: {2.0: 3.0, 4.0: 3.0}\n#\n# This means:\n# - 3.0 units of flow use paths with total cost 2.0 (A-&gt;B-&gt;C path)\n# - 3.0 units of flow use paths with total cost 4.0 (A-&gt;D-&gt;C path)\n</code></pre>"},{"location":"examples/basic/#latency-span-analysis","title":"Latency Span Analysis","text":"<p>If link costs approximate latency, derive span summary from cost distribution:</p> <pre><code># Example cost distribution analysis\ncost_dist = summary.cost_distribution  # {2.0: 3.0, 4.0: 3.0}\ntotal_flow = summary.total_flow        # 6.0\n\n# Calculate weighted average latency\navg_latency = sum(cost * flow for cost, flow in cost_dist.items()) / total_flow\nprint(f\"Average latency: {avg_latency}\")  # 3.0\n\n# Find min/max latency tiers\nmin_latency = min(cost_dist.keys())\nmax_latency = max(cost_dist.keys())\nprint(f\"Latency range: {min_latency} - {max_latency}\")  # 2.0 - 4.0\n</code></pre>"},{"location":"examples/basic/#efficient-repeated-analysis","title":"Efficient Repeated Analysis","text":"<p>For scenarios requiring multiple analyses with different exclusions (e.g., failure testing), use a bound context:</p> <pre><code># Create bound context - graph built once\nctx = analyze(network, source=\"^A$\", sink=\"^C$\", mode=Mode.COMBINE)\n\n# Baseline capacity\nbaseline = ctx.max_flow()\nprint(f\"Baseline: {baseline}\")\n\n# Test various failure scenarios\nfor node in [\"B\", \"D\"]:\n    degraded = ctx.max_flow(excluded_nodes={node})\n    print(f\"Without {node}: {degraded}\")\n\n# Output:\n# Baseline: {('^A$', '^C$'): 6.0}\n# Without B: {('^A$', '^C$'): 3.0}\n# Without D: {('^A$', '^C$'): 3.0}\n</code></pre>"},{"location":"examples/basic/#sensitivity-analysis","title":"Sensitivity Analysis","text":"<p>Identify which edges are critical for the flow:</p> <pre><code># Get sensitivity analysis\nsensitivity = analyze(network).sensitivity(\n    \"^A$\",\n    \"^C$\",\n    mode=Mode.COMBINE,\n    shortest_path=False  # Full max-flow mode\n)\n\nfor pair, edge_impacts in sensitivity.items():\n    print(f\"Critical edges for {pair}:\")\n    for edge_key, flow_reduction in sorted(edge_impacts.items(), key=lambda x: -x[1]):\n        print(f\"  {edge_key}: -{flow_reduction:.1f}\")\n</code></pre>"},{"location":"examples/basic/#shortest-paths","title":"Shortest Paths","text":"<p>Get actual path objects for routing analysis:</p> <pre><code>from ngraph import EdgeSelect\n\n# Get all equal-cost shortest paths\npaths = analyze(network).shortest_paths(\n    \"^A$\",\n    \"^C$\",\n    mode=Mode.COMBINE,\n    edge_select=EdgeSelect.ALL_MIN_COST\n)\n\nfor pair, path_list in paths.items():\n    print(f\"Paths from {pair[0]} to {pair[1]}:\")\n    for path in path_list:\n        nodes = [elem[0] for elem in path.path]\n        print(f\"  {' -&gt; '.join(nodes)} (cost: {path.cost})\")\n\n# Get k-shortest paths\nk_paths = analyze(network).k_shortest_paths(\n    \"^A$\",\n    \"^C$\",\n    max_k=3,\n    mode=Mode.PAIRWISE\n)\n\nfor pair, path_list in k_paths.items():\n    print(f\"Top {len(path_list)} paths from {pair[0]} to {pair[1]}:\")\n    for i, path in enumerate(path_list, 1):\n        print(f\"  {i}. Cost: {path.cost}\")\n</code></pre>"},{"location":"examples/bundled-scenarios/","title":"Bundled Scenarios","text":"<p>NetGraph ships with ready-to-run scenarios that demonstrate the DSL, workflow steps, and results export. Use these to validate your environment and as starting points for your own models.</p>"},{"location":"examples/bundled-scenarios/#how-to-run","title":"How to run","text":"<p>Inspect first, then run:</p> <pre><code># Inspect (structure, steps, demands, failure policies)\nngraph inspect scenarios/backbone_clos.yml --detail\n\n# Run and write JSON results next to the scenario (or under --output)\nngraph run scenarios/backbone_clos.yml --output out\n</code></pre> <p>You can filter output by workflow step names with <code>--keys</code> (see each scenario section for step names).</p>"},{"location":"examples/bundled-scenarios/#scenariossquare_meshyaml","title":"<code>scenarios/square_mesh.yaml</code>","text":"<ul> <li>Purpose: Toy 4-node full mesh to exercise MSD search, TM placement, and pairwise MaxFlow.</li> <li> <p>Highlights:</p> </li> <li> <p>Failure policy: single link choice (<code>failures.single_link_failure</code>)</p> </li> <li>Demand set: pairwise demands across all nodes (<code>baseline_traffic_matrix</code>)</li> <li>Workflow steps: <code>msd_baseline</code>, <code>tm_placement</code>, <code>node_to_node_capacity_matrix</code></li> </ul> <p>Run:</p> <pre><code>ngraph inspect scenarios/square_mesh.yaml --detail\nngraph run scenarios/square_mesh.yaml --output out\n\n# Filter to MSD only and print to stdout\nngraph run scenarios/square_mesh.yaml --keys msd_baseline --stdout\n</code></pre>"},{"location":"examples/bundled-scenarios/#scenariosbackbone_closyml","title":"<code>scenarios/backbone_clos.yml</code>","text":"<ul> <li>Purpose: Small Clos/metro fabric with components, SRLG-like risk groups, and multi-step workflow.</li> <li> <p>Highlights:</p> </li> <li> <p>Uses <code>blueprints</code>, attribute-based link selectors, and hardware component attrs</p> </li> <li>Failure policy: weighted multi-mode (<code>failures.weighted_modes</code>)</li> <li>Demand set: inter-metro DC flows with TE/WCMP policy</li> <li>Workflow steps: <code>network_statistics</code>, <code>msd_baseline</code>, <code>tm_placement</code>, <code>cost_power</code></li> </ul> <p>Run:</p> <pre><code>ngraph inspect scenarios/backbone_clos.yml --detail\nngraph run scenarios/backbone_clos.yml --output out\n\n# Export only selected steps\nngraph run scenarios/backbone_clos.yml --keys network_statistics tm_placement --results clos_filtered.json\n</code></pre>"},{"location":"examples/bundled-scenarios/#scenariosnsfnetyaml","title":"<code>scenarios/nsfnet.yaml</code>","text":"<ul> <li>Purpose: Historic NSFNET T3 (1992) backbone with parallel circuits and SRLG-style risk groups.</li> <li> <p>Highlights:</p> </li> <li> <p>Explicit nodes/links with capacities and costs; rich <code>risk_groups</code></p> </li> <li>Failure policies: single-link and availability-based random failures</li> <li>Workflow steps: <code>node_to_node_capacity_matrix_1</code>, <code>node_to_node_capacity_matrix_2</code></li> </ul> <p>Run:</p> <pre><code>ngraph inspect scenarios/nsfnet.yaml --detail\nngraph run scenarios/nsfnet.yaml --output out\n\n# Filter to a specific matrix computation\nngraph run scenarios/nsfnet.yaml --keys node_to_node_capacity_matrix_1 --stdout\n</code></pre>"},{"location":"examples/bundled-scenarios/#notes-on-results","title":"Notes on results","text":"<p>All runs emit a consistent JSON shape with <code>workflow</code>, <code>steps</code>, and <code>scenario</code> sections. Steps like <code>MaxFlow</code> and <code>TrafficMatrixPlacement</code> store per-iteration lists under <code>data.flow_results</code> with <code>summary</code> and optional <code>cost_distribution</code> or <code>min_cut</code> fields. See Reference -&gt; Workflow for the exact schema.</p>"},{"location":"examples/clos-fabric/","title":"Clos Fabric Analysis","text":"<p>This example demonstrates analysis of a 3-tier Clos fabric. For production use, run the bundled scenario and generate metrics via CLI, then iterate in Python if needed.</p> <p>Refer to Tutorial for running bundled scenarios via CLI.</p>"},{"location":"examples/clos-fabric/#scenario-overview","title":"Scenario Overview","text":"<p>We'll create two separate 3-tier Clos networks and analyze the maximum flow capacity between them. This scenario showcases:</p> <ul> <li>Hierarchical blueprint composition</li> <li>Complex link patterns</li> <li>Flow analysis with different placement policies</li> </ul>"},{"location":"examples/clos-fabric/#programmatic-scenario","title":"Programmatic scenario","text":"<pre><code>from ngraph.scenario import Scenario\nfrom ngraph import analyze, Mode, FlowPlacement\n\nscenario_yaml = \"\"\"\nblueprints:\n  brick_2tier:\n    nodes:\n      t1:\n        count: 8\n        template: \"t1-{n}\"\n      t2:\n        count: 8\n        template: \"t2-{n}\"\n\n    links:\n      - source: /t1\n        target: /t2\n        pattern: mesh\n        capacity: 2\n        cost: 1\n\n  3tier_clos:\n    nodes:\n      b1:\n        blueprint: brick_2tier\n      b2:\n        blueprint: brick_2tier\n      spine:\n        count: 64\n        template: \"t3-{n}\"\n\n    links:\n      - source: b1/t2\n        target: spine\n        pattern: one_to_one\n        capacity: 2\n        cost: 1\n      - source: b2/t2\n        target: spine\n        pattern: one_to_one\n        capacity: 2\n        cost: 1\n\nnetwork:\n  name: \"3tier_clos_network\"\n  version: 1.0\n\n  nodes:\n    my_clos1:\n      blueprint: 3tier_clos\n\n    my_clos2:\n      blueprint: 3tier_clos\n\n  links:\n    - source: my_clos1/spine\n      target: my_clos2/spine\n      pattern: one_to_one\n      count: 4\n      capacity: 1\n      cost: 1\n\"\"\"\n\n# Create and analyze the scenario\nscenario = Scenario.from_yaml(scenario_yaml)\nnetwork = scenario.network\n\n# Calculate maximum flow with ECMP (Equal Cost Multi-Path)\nmax_flow_ecmp = analyze(network).max_flow(\n    r\"my_clos1.*(b[0-9]*)/t1\",\n    r\"my_clos2.*(b[0-9]*)/t1\",\n    mode=Mode.COMBINE,\n    shortest_path=True,\n    flow_placement=FlowPlacement.EQUAL_BALANCED,\n)\n\nprint(f\"Maximum flow with ECMP: {max_flow_ecmp}\")\n# Result: {('b1|b2', 'b1|b2'): 256.0}\n</code></pre>"},{"location":"examples/clos-fabric/#understanding-the-results","title":"Understanding the Results","text":"<p>The result <code>{('b1|b2', 'b1|b2'): 256.0}</code> means:</p> <ul> <li>Source: All t1 nodes in both b1 and b2 segments of my_clos1</li> <li>Target: All t1 nodes in both b1 and b2 segments of my_clos2</li> <li>Capacity: Maximum flow of 256.0 units</li> </ul>"},{"location":"examples/clos-fabric/#ecmp-vs-wcmp-impact-of-link-failures","title":"ECMP vs WCMP: Impact of Link Failures","text":"<p>NetGraph supports different flow placement policies:</p> <ul> <li><code>FlowPlacement.EQUAL_BALANCED</code>: Equal split across equal-cost paths</li> <li><code>FlowPlacement.PROPORTIONAL</code>: Capacity-weighted split across equal-cost paths</li> </ul> <p>Combined with the path selection settings (shortest_path=True|False), we can achieve different flow placement policies emulating ECMP, WCMP, and TE behavior in IP/MPLS networks.</p> <p>In this example, we use the <code>FlowPlacement.EQUAL_BALANCED</code> policy and <code>shortest_path=True</code> to emulate ECMP behavior and we will compare it with WCMP <code>FlowPlacement.PROPORTIONAL</code> (capacity-weighted split across equal-cost paths) under two conditions:</p> <ul> <li>Baseline: symmetric parallel inter-spine links -&gt; ECMP = WCMP (256.0).</li> <li>Uneven links: make capacities within each equal-cost bundle different -&gt; WCMP   achieves higher throughput than ECMP, which is limited by equal splitting.</li> </ul> <p>We emulate partial inter-spine degradation by making capacities uneven across the 4 parallel spine-to-spine links per pair while keeping equal costs. This isolates the effect of the splitting policy.</p> <pre><code>from ngraph import analyze, Mode, FlowPlacement\nfrom ngraph.scenario import Scenario\n\nscenario_yaml = \"\"\"\nblueprints:\n  brick_2tier:\n    nodes:\n      t1: {count: 8, template: \"t1-{n}\"}\n      t2: {count: 8, template: \"t2-{n}\"}\n    links:\n      - {source: /t1, target: /t2, pattern: mesh, capacity: 2, cost: 1}\n  3tier_clos:\n    nodes:\n      b1: {blueprint: brick_2tier}\n      b2: {blueprint: brick_2tier}\n      spine: {count: 64, template: \"t3-{n}\"}\n    links:\n      - {source: b1/t2, target: spine, pattern: one_to_one, capacity: 2, cost: 1}\n      - {source: b2/t2, target: spine, pattern: one_to_one, capacity: 2, cost: 1}\nnetwork:\n  name: 3tier_clos_network\n  nodes:\n    my_clos1: {blueprint: 3tier_clos}\n    my_clos2: {blueprint: 3tier_clos}\n  links:\n    - {source: my_clos1/spine, target: my_clos2/spine, pattern: one_to_one, count: 4, capacity: 1, cost: 1}\n\"\"\"\n\nscenario = Scenario.from_yaml(scenario_yaml)\nnetwork = scenario.network\n\n# Baseline (symmetric)\nbaseline_ecmp = analyze(network).max_flow(\n    r\"my_clos1.*(b[0-9]*)/t1\",\n    r\"my_clos2.*(b[0-9]*)/t1\",\n    mode=Mode.COMBINE, shortest_path=True,\n    flow_placement=FlowPlacement.EQUAL_BALANCED,\n)\nbaseline_wcmp = analyze(network).max_flow(\n    r\"my_clos1.*(b[0-9]*)/t1\",\n    r\"my_clos2.*(b[0-9]*)/t1\",\n    mode=Mode.COMBINE, shortest_path=True,\n    flow_placement=FlowPlacement.PROPORTIONAL,\n)\n\n# Make parallel inter-spine links uneven (keeps equal cost)\nfrom collections import defaultdict\ngroups = defaultdict(list)\nfor lk in network.links.values():\n    s, t = lk.source, lk.target\n    if (s.startswith(\"my_clos1/spine\") and t.startswith(\"my_clos2/spine\")) or \\\n       (s.startswith(\"my_clos2/spine\") and t.startswith(\"my_clos1/spine\")):\n        groups[(s, t)].append(lk)\nfor i, key in enumerate(sorted(groups.keys())):\n    links = sorted(groups[key], key=lambda x: (x.source, x.target, id(x)))\n    caps = [4.0, 0.25, 0.25, 0.25] if i % 2 == 0 else [2.0, 1.0, 0.5, 0.25]\n    for lk, cap in zip(links, caps):\n        lk.capacity = cap\n\necmp = analyze(network).max_flow(\n    r\"my_clos1.*(b[0-9]*)/t1\",\n    r\"my_clos2.*(b[0-9]*)/t1\",\n    mode=Mode.COMBINE, shortest_path=True,\n    flow_placement=FlowPlacement.EQUAL_BALANCED,\n)\nwcmp = analyze(network).max_flow(\n    r\"my_clos1.*(b[0-9]*)/t1\",\n    r\"my_clos2.*(b[0-9]*)/t1\",\n    mode=Mode.COMBINE, shortest_path=True,\n    flow_placement=FlowPlacement.PROPORTIONAL,\n)\nprint(\"Baseline ECMP:\", baseline_ecmp)\nprint(\"Baseline WCMP:\", baseline_wcmp)\nprint(\"Uneven ECMP:\", ecmp)\nprint(\"Uneven WCMP:\", wcmp)\n</code></pre> <p>Example output:</p> <pre><code>Baseline ECMP: {('b1|b2', 'b1|b2'): 256.0}\nBaseline WCMP: {('b1|b2', 'b1|b2'): 256.0}\nUneven ECMP: {('b1|b2', 'b1|b2'): 64.0}\nUneven WCMP: {('b1|b2', 'b1|b2'): 248.0}\n</code></pre> <p>As expected, WCMP achieves higher throughput than ECMP when parallel links within equal-cost bundles have uneven capacities. ECMP is limited by the link with the lowest capacity in the equal-cost group.</p>"},{"location":"examples/clos-fabric/#network-structure-analysis","title":"Network Structure Analysis","text":"<p>We can also analyze the network structure using the NetworkExplorer:</p> <pre><code>from ngraph.explorer import NetworkExplorer\n\n# Explore the network topology\nexplorer = NetworkExplorer.explore_network(network)\nexplorer.print_tree(skip_leaves=True, detailed=False)\n\n# The explorer shows hierarchical structure and connectivity patterns\n# For detailed path analysis, use max_flow_detailed to get flow details\n# including cost distribution and path information\n</code></pre>"},{"location":"examples/clos-fabric/#next-steps","title":"Next Steps","text":"<ul> <li>Bundled Scenarios - Ready-to-run examples</li> <li>Workflow Reference - Analysis workflows and Monte Carlo simulation</li> <li>DSL Reference - YAML syntax reference</li> <li>API Reference - Explore the Python API in detail</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>NetGraph is a hybrid Python+C++ framework. The Python package (<code>ngraph</code>) automatically installs the C++ performance layer (<code>netgraph-core</code>) as a dependency.</p>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.11 or higher</li> <li>C++ compiler (for building netgraph-core from source if needed)</li> <li>Linux: GCC 10+ or Clang 12+</li> <li>macOS: Xcode Command Line Tools (Apple Clang)</li> <li>Windows: Visual Studio 2019+ with C++ tools</li> </ul>"},{"location":"getting-started/installation/#from-pypi","title":"From PyPI","text":"<p>Create and activate a virtual environment:</p> <pre><code>python -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n</code></pre> <p>Install NetGraph:</p> <pre><code>pip install ngraph\n</code></pre> <p>This installs:</p> <ol> <li>The Python <code>ngraph</code> package</li> <li><code>netgraph-core</code> (pre-built wheels for common platforms, or builds from source)</li> <li>Dependencies (networkx, pyyaml, pandas, jsonschema)</li> </ol> <p>Verify installation:</p> <pre><code>ngraph --help\n</code></pre>"},{"location":"getting-started/installation/#from-source","title":"From Source","text":"<p>For development or if you need the latest changes:</p> <pre><code># Clone both repositories\ngit clone https://github.com/networmix/NetGraph-Core\ngit clone https://github.com/networmix/NetGraph\n\n# Install NetGraph-Core first\ncd NetGraph-Core\npip install -e .\n\n# Install NetGraph\ncd ../NetGraph\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#platform-notes","title":"Platform Notes","text":"<p>Pre-built wheels: Available for Linux (x86_64, aarch64), macOS (x86_64, arm64), and Windows (x86_64).</p> <p>Building from source: Requires CMake 3.15+. Builds automatically during <code>pip install</code> if no compatible wheel is available.</p> <p>Next: See Tutorial for running scenarios and programmatic usage examples.</p>"},{"location":"getting-started/tutorial/","title":"Tutorial","text":"<p>This guide shows the fastest way to run a scenario from the CLI and a minimal programmatic example. See the Examples section for detailed scenarios and policies.</p>"},{"location":"getting-started/tutorial/#cli-run-and-inspect","title":"CLI: run and inspect","text":"<pre><code># Inspect (validate and preview structure, steps, demands)\nngraph inspect scenarios/square_mesh.yaml --detail\n\n# Run and store results (JSON) next to the scenario or under --output\nngraph run scenarios/square_mesh.yaml --output out\n\n# Filter exported results by workflow step names\nngraph run scenarios/square_mesh.yaml --keys msd_baseline --stdout\n</code></pre> <p>See also: <code>scenarios/backbone_clos.yml</code> and <code>scenarios/nsfnet.yaml</code>.</p>"},{"location":"getting-started/tutorial/#programmatic-minimal-example","title":"Programmatic: minimal example","text":"<pre><code>from ngraph.scenario import Scenario\n\nscenario_yaml = \"\"\"\nnetwork:\n  nodes:\n    A: {}\n    B: {}\n  links:\n    - {source: A, target: B, capacity: 10.0, cost: 1.0}\nworkflow:\n  - type: NetworkStats\n    name: baseline_stats\n\"\"\"\n\nscenario = Scenario.from_yaml(scenario_yaml)\nscenario.run()\n\nexported = scenario.results.to_dict()\nprint(list(exported[\"steps\"].keys()))\n</code></pre>"},{"location":"getting-started/tutorial/#results-structure","title":"Results structure","text":"<p>Results are exported with a fixed structure containing <code>workflow</code>, <code>steps</code>, and <code>scenario</code> sections. Steps such as <code>MaxFlow</code>, <code>TrafficMatrixPlacement</code>, and <code>MaximumSupportedDemand</code> write their outputs under their step name. See the Workflow Reference for field details.</p>"},{"location":"getting-started/tutorial/#next-steps","title":"Next steps","text":"<ul> <li>Bundled Scenarios - Ready-to-run example scenarios</li> <li>DSL Reference - YAML scenario syntax</li> <li>Workflow Reference - Analysis step configuration</li> <li>CLI Reference - Command-line interface details</li> </ul>"},{"location":"reference/api-full/","title":"API Full","text":""},{"location":"reference/api-full/#netgraph-api-reference-auto-generated","title":"NetGraph API Reference (Auto-Generated)","text":"<p>This is the complete auto-generated API documentation for NetGraph. For a curated, example-driven API guide, see api.md.</p> <p>Quick links:</p> <ul> <li>Main API Guide (api.md)</li> <li>This Document (api-full.md)</li> <li>CLI Reference</li> <li>DSL Reference</li> </ul> <p>Generated from source code on: February 08, 2026 at 18:49 UTC</p> <p>Modules auto-discovered: 52</p>"},{"location":"reference/api-full/#ngraphcli","title":"ngraph.cli","text":"<p>Command-line interface for NetGraph.</p>"},{"location":"reference/api-full/#mainargv-optionalliststr-none-none","title":"main(argv: 'Optional[List[str]]' = None) -&gt; 'None'","text":"<p>Entry point for the <code>ngraph</code> command.</p> <p>Args:     argv: Optional list of command-line arguments. If <code>None</code>, <code>sys.argv</code>         is used.</p>"},{"location":"reference/api-full/#ngraphexplorer","title":"ngraph.explorer","text":"<p>NetworkExplorer class for analyzing network hierarchy and structure.</p>"},{"location":"reference/api-full/#externallinkbreakdown","title":"ExternalLinkBreakdown","text":"<p>Holds stats for external links to a particular other subtree.</p> <p>Attributes:     link_count (int): Number of links to that other subtree.     link_capacity (float): Sum of capacities for those links.</p> <p>Attributes:</p> <ul> <li><code>link_count</code> (int) = 0</li> <li><code>link_capacity</code> (float) = 0.0</li> </ul>"},{"location":"reference/api-full/#linkcapacityissue","title":"LinkCapacityIssue","text":"<p>Represents a link capacity constraint violation in active topology.</p> <p>Attributes:     source: Source node name.     target: Target node name.     capacity: Configured link capacity.     limit: Effective capacity limit from per-end hardware (min of ends).     reason: Brief reason tag.</p> <p>Attributes:</p> <ul> <li><code>source</code> (str)</li> <li><code>target</code> (str)</li> <li><code>capacity</code> (float)</li> <li><code>limit</code> (float)</li> <li><code>reason</code> (str)</li> </ul>"},{"location":"reference/api-full/#networkexplorer","title":"NetworkExplorer","text":"<p>Provides hierarchical exploration of a Network, computing statistics in two modes: 'all' (ignores disabled) and 'active' (only enabled).</p> <p>Methods:</p> <ul> <li><code>explore_network(network: 'Network', components_library: 'Optional[ComponentsLibrary]' = None, strict_validation: 'bool' = True) -&gt; 'NetworkExplorer'</code> - Build a NetworkExplorer, constructing a tree plus 'all' and 'active' stats.</li> <li><code>get_bom(self, include_disabled: 'bool' = True) -&gt; 'Dict[str, float]'</code> - Return aggregated hardware BOM for the whole network.</li> <li><code>get_bom_by_path(self, path: 'str', include_disabled: 'bool' = True) -&gt; 'Dict[str, float]'</code> - Return the hardware BOM for a specific hierarchy path.</li> <li><code>get_bom_map(self, include_disabled: 'bool' = True, include_root: 'bool' = True, root_label: 'str' = '') -&gt; 'Dict[str, Dict[str, float]]'</code> - Return a mapping from hierarchy path to BOM for each subtree.</li> <li><code>get_link_issues(self) -&gt; 'List[LinkCapacityIssue]'</code> - Return recorded link capacity issues discovered in non-strict mode.</li> <li><code>get_node_utilization(self, include_disabled: 'bool' = True) -&gt; 'List[NodeUtilization]'</code> - Return hardware utilization per node based on active topology.</li> <li><code>print_tree(self, node: 'Optional[TreeNode]' = None, indent: 'int' = 0, max_depth: 'Optional[int]' = None, skip_leaves: 'bool' = False, detailed: 'bool' = False, include_disabled: 'bool' = True, max_external_lines: 'Optional[int]' = None, line_prefix: 'str' = '') -&gt; 'None'</code> - Print the hierarchy from 'node' down (default: root).</li> </ul>"},{"location":"reference/api-full/#nodeutilization","title":"NodeUtilization","text":"<p>Per-node hardware utilization snapshot based on active topology.</p> <p>Attributes:     node_name: Fully qualified node name.     component_name: Hardware component name if present.     hw_count: Hardware multiplicity used for capacity/power scaling.     capacity_supported: Total capacity supported by node hardware.     attached_capacity_active: Sum of capacities of enabled adjacent links where the         opposite endpoint is also enabled.     capacity_utilization: Ratio of attached to supported capacity (0.0 when N/A).     ports_available: Total port equivalents available on the node (0.0 when N/A).     ports_used: Sum of port equivalents used by per-end link optics attached to this         node on active links.     ports_utilization: Ratio of used to available ports (0.0 when N/A).     capacity_violation: True if attached capacity exceeds supported capacity.     ports_violation: True if used ports exceed available ports.     disabled: True if the node itself is disabled.</p> <p>Attributes:</p> <ul> <li><code>node_name</code> (str)</li> <li><code>component_name</code> (Optional[str])</li> <li><code>hw_count</code> (float)</li> <li><code>capacity_supported</code> (float)</li> <li><code>attached_capacity_active</code> (float)</li> <li><code>capacity_utilization</code> (float)</li> <li><code>ports_available</code> (float)</li> <li><code>ports_used</code> (float)</li> <li><code>ports_utilization</code> (float)</li> <li><code>capacity_violation</code> (bool)</li> <li><code>ports_violation</code> (bool)</li> <li><code>disabled</code> (bool)</li> </ul>"},{"location":"reference/api-full/#treenode","title":"TreeNode","text":"<p>Represents a node in the hierarchical tree.</p> <p>Attributes:     name (str): Name/label of this node.     parent (Optional[TreeNode]): Pointer to the parent tree node.     children (Dict[str, TreeNode]): Mapping of child name -&gt; child TreeNode.     subtree_nodes (Set[str]): Node names in the subtree (all nodes, ignoring disabled).     active_subtree_nodes (Set[str]): Node names in the subtree (only enabled).     stats (TreeStats): Aggregated stats for \"all\" view.     active_stats (TreeStats): Aggregated stats for \"active\" (only enabled) view.     raw_nodes (List[Node]): Direct Node objects at this hierarchy level.</p> <p>Attributes:</p> <ul> <li><code>name</code> (str)</li> <li><code>parent</code> (Optional[TreeNode])</li> <li><code>children</code> (Dict[str, TreeNode]) = {}</li> <li><code>subtree_nodes</code> (Set[str]) = set()</li> <li><code>active_subtree_nodes</code> (Set[str]) = set()</li> <li><code>stats</code> (TreeStats) = TreeStats(node_count=0, internal_link_count=0, internal_link_capacity=0.0, external_link_count=0, external_link_capacity=0.0, external_link_details={}, total_capex=0.0, total_power=0.0, bom={}, active_bom={})</li> <li><code>active_stats</code> (TreeStats) = TreeStats(node_count=0, internal_link_count=0, internal_link_capacity=0.0, external_link_count=0, external_link_capacity=0.0, external_link_details={}, total_capex=0.0, total_power=0.0, bom={}, active_bom={})</li> <li><code>raw_nodes</code> (List[Node]) = []</li> </ul> <p>Methods:</p> <ul> <li><code>add_child(self, child_name: 'str') -&gt; 'TreeNode'</code> - Ensure a child node named 'child_name' exists and return it.</li> <li><code>is_leaf(self) -&gt; 'bool'</code> - Return True if this node has no children.</li> </ul>"},{"location":"reference/api-full/#treestats","title":"TreeStats","text":"<p>Aggregated statistics for a single tree node (subtree).</p> <p>Attributes:     node_count (int): Total number of nodes in this subtree.     internal_link_count (int): Number of internal links in this subtree.     internal_link_capacity (float): Sum of capacities for those internal links.     external_link_count (int): Number of external links from this subtree to another.     external_link_capacity (float): Sum of capacities for those external links.     external_link_details (Dict[str, ExternalLinkBreakdown]): Breakdown by other subtree path.     total_capex (float): Cumulative capex (nodes + links).     total_power (float): Cumulative power (nodes + links).</p> <p>Attributes:</p> <ul> <li><code>node_count</code> (int) = 0</li> <li><code>internal_link_count</code> (int) = 0</li> <li><code>internal_link_capacity</code> (float) = 0.0</li> <li><code>external_link_count</code> (int) = 0</li> <li><code>external_link_capacity</code> (float) = 0.0</li> <li><code>external_link_details</code> (Dict[str, ExternalLinkBreakdown]) = {}</li> <li><code>total_capex</code> (float) = 0.0</li> <li><code>total_power</code> (float) = 0.0</li> <li><code>bom</code> (Dict[str, float]) = {}</li> <li><code>active_bom</code> (Dict[str, float]) = {}</li> </ul>"},{"location":"reference/api-full/#ngraphlogging","title":"ngraph.logging","text":"<p>Centralized logging configuration for NetGraph.</p>"},{"location":"reference/api-full/#disable_debug_logging-none","title":"disable_debug_logging() -&gt; None","text":"<p>Disable debug logging, set to INFO level.</p>"},{"location":"reference/api-full/#enable_debug_logging-none","title":"enable_debug_logging() -&gt; None","text":"<p>Enable debug logging for the entire package.</p>"},{"location":"reference/api-full/#get_loggername-str-logginglogger","title":"get_logger(name: str) -&gt; logging.Logger","text":"<p>Get a logger with NetGraph's standard configuration.</p> <p>This is the main function that should be used throughout the package. All loggers will inherit from the root 'ngraph' logger configuration.</p> <p>Args:     name: Logger name (typically name from calling module).</p> <p>Returns:     Configured logger instance.</p>"},{"location":"reference/api-full/#reset_logging-none","title":"reset_logging() -&gt; None","text":"<p>Reset logging configuration (mainly for testing).</p>"},{"location":"reference/api-full/#set_global_log_levellevel-int-none","title":"set_global_log_level(level: int) -&gt; None","text":"<p>Set the log level for all NetGraph loggers.</p> <p>Args:     level: Logging level (e.g., logging.DEBUG, logging.INFO).</p>"},{"location":"reference/api-full/#setup_root_loggerlevel-int-20-format_string-optionalstr-none-handler-optionallogginghandler-none-none","title":"setup_root_logger(level: int = 20, format_string: Optional[str] = None, handler: Optional[logging.Handler] = None) -&gt; None","text":"<p>Set up the root NetGraph logger with a single handler.</p> <p>This should only be called once to avoid duplicate handlers.</p> <p>Args:     level: Logging level (default: INFO).     format_string: Custom format string (optional).     handler: Custom handler (optional, defaults to StreamHandler).</p>"},{"location":"reference/api-full/#ngraphscenario","title":"ngraph.scenario","text":"<p>Scenario class for defining network analysis workflows from YAML.</p>"},{"location":"reference/api-full/#scenario","title":"Scenario","text":"<p>Represents a complete scenario for building and executing network workflows.</p> <p>This scenario includes:</p> <ul> <li>A network (nodes/links), constructed via blueprint expansion.</li> <li>A failure policy set (one or more named failure policies).</li> <li>A traffic matrix set containing one or more named traffic matrices.</li> <li>A list of workflow steps to execute.</li> <li>A results container for storing outputs.</li> <li>A components_library for hardware/optics definitions.</li> <li>A seed for reproducible random operations (optional).</li> </ul> <p>Typical usage example:</p> <pre><code>scenario = Scenario.from_yaml(yaml_str, default_components=default_lib)\nscenario.run()\n# Inspect scenario.results\n</code></pre> <p>Attributes:</p> <ul> <li><code>network</code> (Network)</li> <li><code>workflow</code> (List[WorkflowStep])</li> <li><code>failure_policy_set</code> (FailurePolicySet) = FailurePolicySet(policies={})</li> <li><code>demand_set</code> (DemandSet) = DemandSet(sets={})</li> <li><code>results</code> (Results) = Results(_store={}, _metadata={}, _active_step=None, _scenario={})</li> <li><code>components_library</code> (ComponentsLibrary) = ComponentsLibrary(components={})</li> <li><code>seed</code> (Optional[int])</li> <li><code>_execution_counter</code> (int) = 0</li> </ul> <p>Methods:</p> <ul> <li><code>from_yaml(yaml_str: 'str', default_components: 'Optional[ComponentsLibrary]' = None) -&gt; 'Scenario'</code> - Constructs a Scenario from a YAML string, optionally merging</li> <li><code>run(self) -&gt; 'None'</code> - Executes the scenario's workflow steps in order.</li> </ul>"},{"location":"reference/api-full/#ngraphmodelcomponents","title":"ngraph.model.components","text":"<p>Component and ComponentsLibrary classes for hardware capex/power modeling.</p>"},{"location":"reference/api-full/#component","title":"Component","text":"<p>A generic component that can represent chassis, line cards, optics, etc. Components can have nested children, each with their own capex, power, etc.</p> <p>Attributes:     name (str): Name of the component (e.g., \"SpineChassis\" or \"400G-LR4\").     component_type (str): A string label (e.g., \"chassis\", \"linecard\", \"optic\").     description (str): A human-readable description of this component.     capex (float): Monetary capex of a single instance of this component.     power_watts (float): Typical/nominal power usage (watts) for one instance.     power_watts_max (float): Maximum/peak power usage (watts) for one instance.     capacity (float): A generic capacity measure (e.g., platform capacity).     ports (int): Number of ports if relevant for this component.     count (int): How many identical copies of this component are present.     attrs (Dict[str, Any]): Arbitrary key-value attributes for extra metadata.     children (Dict[str, Component]): Nested child components (e.g., line cards         inside a chassis), keyed by child name.</p> <p>Attributes:</p> <ul> <li><code>name</code> (str)</li> <li><code>component_type</code> (str) = generic</li> <li><code>description</code> (str)</li> <li><code>capex</code> (float) = 0.0</li> <li><code>power_watts</code> (float) = 0.0</li> <li><code>power_watts_max</code> (float) = 0.0</li> <li><code>capacity</code> (float) = 0.0</li> <li><code>ports</code> (int) = 0</li> <li><code>count</code> (int) = 1</li> <li><code>attrs</code> (Dict[str, Any]) = {}</li> <li><code>children</code> (Dict[str, Component]) = {}</li> </ul> <p>Methods:</p> <ul> <li><code>as_dict(self, include_children: 'bool' = True) -&gt; 'Dict[str, Any]'</code> - Returns a dictionary containing all properties of this component.</li> <li><code>total_capacity(self) -&gt; 'float'</code> - Computes the total (recursive) capacity of this component,</li> <li><code>total_capex(self) -&gt; 'float'</code> - Computes total capex including children, multiplied by count.</li> <li><code>total_power(self) -&gt; 'float'</code> - Computes the total typical (recursive) power usage of this component,</li> <li><code>total_power_max(self) -&gt; 'float'</code> - Computes the total peak (recursive) power usage of this component,</li> </ul>"},{"location":"reference/api-full/#componentslibrary","title":"ComponentsLibrary","text":"<p>Holds a collection of named Components. Each entry is a top-level \"template\" that can be referenced for cost/power/capacity lookups, possibly with nested children.</p> <p>Example (YAML-like):     components:       BigSwitch:         component_type: chassis         cost: 20000         power_watts: 1750         capacity: 25600         children:           PIM16Q-16x200G:             component_type: linecard             cost: 1000             power_watts: 10             ports: 16             count: 8       200G-FR4:         component_type: optic         cost: 2000         power_watts: 6         power_watts_max: 6.5</p> <p>Attributes:</p> <ul> <li><code>components</code> (Dict[str, Component]) = {}</li> </ul> <p>Methods:</p> <ul> <li><code>clone(self) -&gt; 'ComponentsLibrary'</code> - Creates a deep copy of this ComponentsLibrary.</li> <li><code>from_dict(data: 'Dict[str, Any]') -&gt; 'ComponentsLibrary'</code> - Constructs a ComponentsLibrary from a dictionary of raw component definitions.</li> <li><code>from_yaml(yaml_str: 'str') -&gt; 'ComponentsLibrary'</code> - Constructs a ComponentsLibrary from a YAML string. If the YAML contains</li> <li><code>get(self, name: 'str') -&gt; 'Optional[Component]'</code> - Retrieves a Component by its name from the library.</li> <li><code>merge(self, other: 'ComponentsLibrary', override: 'bool' = True) -&gt; 'ComponentsLibrary'</code> - Merges another ComponentsLibrary into this one. By default (override=True),</li> </ul>"},{"location":"reference/api-full/#resolve_link_end_componentsattrs-dictstr-any-library-componentslibrary-tupletupleoptionalcomponent-float-bool-tupleoptionalcomponent-float-bool-bool","title":"resolve_link_end_components(attrs: 'Dict[str, Any]', library: 'ComponentsLibrary') -&gt; 'tuple[tuple[Optional[Component], float, bool], tuple[Optional[Component], float, bool], bool]'","text":"<p>Resolve per-end hardware components for a link.</p> <p>Input format inside <code>link.attrs</code>:</p> <p>Structured mapping under <code>hardware</code> key only:   <code>{\"hardware\": {\"source\": {\"component\": NAME, \"count\": N},                    \"target\": {\"component\": NAME, \"count\": N}}}</code></p> <p>Args:     attrs: Link attributes mapping.     library: Components library for lookups.</p> <p>Exclusive usage:</p> <ul> <li> <p>Optional <code>exclusive: true</code> per end indicates unsharable usage.</p> <p>For exclusive ends, validation and BOM counting should round-up counts to integers.</p> </li> </ul> <p>Returns:     ((src_comp, src_count, src_exclusive), (dst_comp, dst_count, dst_exclusive), per_end_specified)     where components may be <code>None</code> if name is absent/unknown. <code>per_end_specified</code>     is True when a structured per-end mapping is present.</p>"},{"location":"reference/api-full/#resolve_node_hardwareattrs-dictstr-any-library-componentslibrary-tupleoptionalcomponent-float","title":"resolve_node_hardware(attrs: 'Dict[str, Any]', library: 'ComponentsLibrary') -&gt; 'Tuple[Optional[Component], float]'","text":"<p>Resolve node hardware from <code>attrs['hardware']</code>.</p> <p>Expects the mapping: <code>{\"hardware\": {\"component\": NAME, \"count\": N}}</code>. <code>count</code> defaults to 1 if missing or invalid. If <code>component</code> is missing or unknown, returns <code>(None, 1.0)</code>.</p> <p>Args:     attrs: Node attributes mapping.     library: Component library used for lookups.</p> <p>Returns:     Tuple of (component or None, positive multiplier).</p>"},{"location":"reference/api-full/#totals_with_multipliercomp-component-hw_count-float-tuplefloat-float-float","title":"totals_with_multiplier(comp: 'Component', hw_count: 'float') -&gt; 'Tuple[float, float, float]'","text":"<p>Return (capex, power_watts, capacity) totals multiplied by <code>hw_count</code>.</p> <p>Args:     comp: Component definition (may include nested children and internal <code>count</code>).     hw_count: External multiplier (e.g., number of modules used for a link or node).</p> <p>Returns:     Tuple of total capex, total power (typical), and total capacity as floats.</p>"},{"location":"reference/api-full/#ngraphmodeldemandbuilder","title":"ngraph.model.demand.builder","text":"<p>Builders for demand sets.</p> <p>Construct <code>DemandSet</code> from raw dictionaries (e.g. parsed YAML).</p>"},{"location":"reference/api-full/#build_demand_setraw-dictstr-listdict-demandset","title":"build_demand_set(raw: 'Dict[str, List[dict]]') -&gt; 'DemandSet'","text":"<p>Build a <code>DemandSet</code> from a mapping of name -&gt; list of dicts.</p> <p>Args:     raw: Mapping where each key is a demand set name and each value is a list of         dictionaries with <code>TrafficDemand</code> constructor fields.</p> <p>Returns:     Initialized <code>DemandSet</code> with constructed <code>TrafficDemand</code> objects.</p> <p>Raises:     ValueError: If <code>raw</code> is not a mapping of name -&gt; list[dict],         or if required fields are missing.</p>"},{"location":"reference/api-full/#ngraphmodeldemandmatrix","title":"ngraph.model.demand.matrix","text":"<p>Demand set containers.</p> <p>Provides <code>DemandSet</code>, a named collection of <code>TrafficDemand</code> lists used as input to demand expansion and placement. This module contains input containers, not analysis results.</p>"},{"location":"reference/api-full/#demandset","title":"DemandSet","text":"<p>Named collection of TrafficDemand lists.</p> <p>This mutable container maps set names to lists of TrafficDemand objects, allowing management of multiple demand sets for analysis.</p> <p>Attributes:     sets: Dictionary mapping set names to TrafficDemand lists.</p> <p>Attributes:</p> <ul> <li><code>sets</code> (dict[str, list[TrafficDemand]]) = {}</li> </ul> <p>Methods:</p> <ul> <li><code>add(self, name: 'str', demands: 'list[TrafficDemand]') -&gt; 'None'</code> - Add a demand list to the collection.</li> <li><code>get_all_demands(self) -&gt; 'list[TrafficDemand]'</code> - Get all traffic demands from all sets combined.</li> <li><code>get_default_set(self) -&gt; 'list[TrafficDemand]'</code> - Get default demand set.</li> <li><code>get_set(self, name: 'str') -&gt; 'list[TrafficDemand]'</code> - Get a specific demand set by name.</li> <li><code>to_dict(self) -&gt; 'dict[str, Any]'</code> - Convert to dictionary for JSON serialization.</li> </ul>"},{"location":"reference/api-full/#ngraphmodeldemandspec","title":"ngraph.model.demand.spec","text":"<p>Traffic demand specification.</p> <p>Defines <code>TrafficDemand</code>, a user-facing specification used by demand expansion and placement. It can carry either a concrete <code>FlowPolicy</code> instance or a <code>FlowPolicyPreset</code> enum to construct one.</p>"},{"location":"reference/api-full/#trafficdemand","title":"TrafficDemand","text":"<p>Traffic demand specification using unified selectors.</p> <p>Attributes:     source: Source node selector (string path or selector dict).     target: Target node selector (string path or selector dict).     volume: Total demand volume.     volume_placed: Portion of this demand placed so far.     priority: Priority class (lower = higher priority).     mode: Node pairing mode (\"combine\" or \"pairwise\").     group_mode: How grouped nodes produce demands         (\"flatten\", \"per_group\", \"group_pairwise\").     flow_policy: Policy preset for routing.     flow_policy_obj: Concrete policy instance (overrides flow_policy).     attrs: Arbitrary user metadata.     id: Unique identifier. Auto-generated if empty.</p> <p>Attributes:</p> <ul> <li><code>source</code> (Union)</li> <li><code>target</code> (Union)</li> <li><code>volume</code> (float) = 0.0</li> <li><code>volume_placed</code> (float) = 0.0</li> <li><code>priority</code> (int) = 0</li> <li><code>mode</code> (str) = combine</li> <li><code>group_mode</code> (str) = flatten</li> <li><code>flow_policy</code> (Optional)</li> <li><code>flow_policy_obj</code> (Optional)</li> <li><code>attrs</code> (Dict) = {}</li> <li><code>id</code> (str)</li> </ul>"},{"location":"reference/api-full/#ngraphmodelfailuregenerate","title":"ngraph.model.failure.generate","text":"<p>Dynamic risk group generation from entity attributes.</p> <p>Provides functionality to auto-generate risk groups based on unique attribute values from nodes or links.</p>"},{"location":"reference/api-full/#generatespec","title":"GenerateSpec","text":"<p>Parsed generate block specification.</p> <p>Attributes:     scope: Type of entities to group (\"node\" or \"link\").     path: Optional regex pattern to filter entities by name.     group_by: Attribute name to group by (supports dot-notation).     name: Template for generated group names. Use ${value}         as placeholder for the attribute value.     attrs: Optional static attributes for generated groups.</p> <p>Attributes:</p> <ul> <li><code>scope</code> (Literal['node', 'link'])</li> <li><code>group_by</code> (str)</li> <li><code>name</code> (str)</li> <li><code>path</code> (Optional[str])</li> <li><code>attrs</code> (Dict[str, Any]) = {}</li> </ul>"},{"location":"reference/api-full/#generate_risk_groupsnetwork-network-spec-generatespec-listriskgroup","title":"generate_risk_groups(network: \"'Network'\", spec: 'GenerateSpec') -&gt; 'List[RiskGroup]'","text":"<p>Generate risk groups from unique attribute values.</p> <p>For each unique value of the specified attribute, creates a new risk group and adds all matching entities to it.</p> <p>Args:     network: Network with nodes and links populated.     spec: Generation specification.</p> <p>Returns:     List of newly created RiskGroup objects.</p> <p>Note:     This function modifies entity risk_groups sets in place.</p>"},{"location":"reference/api-full/#parse_generate_specraw-dictstr-any-generatespec","title":"parse_generate_spec(raw: 'Dict[str, Any]') -&gt; 'GenerateSpec'","text":"<p>Parse raw generate dict into a GenerateSpec.</p> <p>Args:     raw: Raw generate dict from YAML.</p> <p>Returns:     Parsed GenerateSpec.</p> <p>Raises:     ValueError: If required fields are missing or invalid.</p>"},{"location":"reference/api-full/#ngraphmodelfailuremembership","title":"ngraph.model.failure.membership","text":"<p>Risk group membership rule resolution.</p> <p>Provides functionality to resolve policy-based membership rules that auto-assign entities (nodes, links, risk groups) to risk groups based on attribute conditions.</p>"},{"location":"reference/api-full/#membershipspec","title":"MembershipSpec","text":"<p>Parsed membership rule specification.</p> <p>Attributes:     scope: Type of entities to match (\"node\", \"link\", or \"risk_group\").     path: Optional regex pattern to filter entities by name.     match: Match specification with conditions.</p> <p>Attributes:</p> <ul> <li><code>scope</code> (EntityScope)</li> <li><code>path</code> (Optional[str])</li> <li><code>match</code> (Optional[MatchSpec])</li> </ul>"},{"location":"reference/api-full/#resolve_membership_rulesnetwork-network-none","title":"resolve_membership_rules(network: \"'Network'\") -&gt; 'None'","text":"<p>Apply membership rules to populate entity risk_groups sets.</p> <p>For each risk group with a <code>_membership_raw</code> specification:</p> <ul> <li>If scope is \"node\" or \"link\": adds the risk group name to each</li> </ul> <p>matched entity's risk_groups set.</p> <ul> <li>If scope is \"risk_group\": adds matched risk groups as children</li> </ul> <p>of this risk group (hierarchical membership).</p> <p>Args:     network: Network with risk_groups, nodes, and links populated.</p> <p>Note:     This function modifies entities in place. It should be called after     all risk groups are registered but before validation.</p>"},{"location":"reference/api-full/#ngraphmodelfailureparser","title":"ngraph.model.failure.parser","text":"<p>Parsers for FailurePolicySet and related failure modeling structures.</p>"},{"location":"reference/api-full/#build_failure_policyfp_data-dictstr-any-policy_name-str-derive_seed-callablestr-optionalint-failurepolicy","title":"build_failure_policy(fp_data: 'Dict[str, Any]', *, policy_name: 'str', derive_seed: 'Callable[[str], Optional[int]]') -&gt; 'FailurePolicy'","text":"<p>Build a FailurePolicy from a raw configuration dictionary.</p> <p>Parses modes, rules, and conditions from the policy definition and constructs a fully initialized FailurePolicy object.</p> <p>Args:     fp_data: Policy definition dict with keys: modes (required), attrs,         expand_groups, expand_children. Each mode contains weight and rules.     policy_name: Name identifier for this policy (used for seed derivation).     derive_seed: Callable to derive deterministic seeds from component names.</p> <p>Returns:     FailurePolicy: Configured policy with parsed modes and rules.</p> <p>Raises:     ValueError: If modes is empty or malformed, or if rules are invalid.</p>"},{"location":"reference/api-full/#build_failure_policy_setraw-dictstr-any-derive_seed-callablestr-optionalint-failurepolicyset","title":"build_failure_policy_set(raw: 'Dict[str, Any]', *, derive_seed: 'Callable[[str], Optional[int]]') -&gt; 'FailurePolicySet'","text":"<p>Build a FailurePolicySet from raw config data.</p> <p>Args:     raw: Mapping of policy name -&gt; policy definition dict.     derive_seed: Callable to derive deterministic seeds from component names.</p> <p>Returns:     Configured FailurePolicySet.</p> <p>Raises:     ValueError: If raw is not a dict or contains invalid policy definitions.</p>"},{"location":"reference/api-full/#build_risk_groupsrg_data-listany-tuplelistriskgroup-listdictstr-any","title":"build_risk_groups(rg_data: 'List[Any]') -&gt; 'tuple[List[RiskGroup], List[Dict[str, Any]]]'","text":"<p>Build RiskGroup objects from raw config data.</p> <p>Supports:</p> <ul> <li>String shorthand: \"GroupName\" is equivalent to {name: \"GroupName\"}</li> <li>Bracket expansion: {name: \"DC[1-3]_Power\"} creates DC1_Power, DC2_Power, DC3_Power</li> <li>Children are also expanded recursively</li> <li>Generate blocks: {generate: {...}} for dynamic group creation</li> </ul> <p>Args:     rg_data: List of risk group definitions (strings or dicts).</p> <p>Returns:     Tuple of (explicit_risk_groups, generate_specs_raw):</p> <ul> <li>explicit_risk_groups: List of RiskGroup objects with names expanded.</li> <li>generate_specs_raw: List of raw generate block dicts for deferred processing.</li> </ul>"},{"location":"reference/api-full/#ngraphmodelfailurepolicy","title":"ngraph.model.failure.policy","text":"<p>Failure policy primitives.</p> <p>Defines <code>FailureRule</code> and <code>FailurePolicy</code> for expressing how nodes, links, and risk groups fail in analyses. Conditions match on top-level attributes with simple operators; rules select matches using \"all\", probabilistic \"random\" (with <code>probability</code>), or fixed-size \"choice\" (with <code>count</code>). Policies can optionally expand failures by shared risk groups or by risk-group children.</p>"},{"location":"reference/api-full/#failuremode","title":"FailureMode","text":"<p>A weighted mode that encapsulates a set of rules applied together.</p> <p>Exactly one mode is selected per failure iteration according to the mode weights. Within a mode, all contained rules are applied and their selections are unioned into the failure set.</p> <p>Attributes:     weight: Non-negative weight used for mode selection. All weights are         normalized internally. Modes with zero weight are never selected.     rules: A list of <code>FailureRule</code> applied together when this mode is chosen.     attrs: Optional metadata.</p> <p>Attributes:</p> <ul> <li><code>weight</code> (float)</li> <li><code>rules</code> (List[FailureRule]) = []</li> <li><code>attrs</code> (Dict[str, Any]) = {}</li> </ul>"},{"location":"reference/api-full/#failurepolicy","title":"FailurePolicy","text":"<p>A container for failure modes plus optional metadata in <code>attrs</code>.</p> <p>The main entry point is <code>apply_failures</code>, which:   1) Build a single RNG for the entire call (from <code>seed</code> or <code>self.seed</code>).   2) Select a mode based on weights (one RNG draw).   3) For each rule in the mode, gather relevant entities.   4) Match based on rule conditions using 'and' or 'or' logic.   5) Apply the selection strategy (all, random, or choice) drawing      from the same RNG, ensuring statistical independence across rules.   6) Collect the union of all failed entities across all rules.   7) Optionally expand failures by shared-risk groups or sub-risks.</p> <p>Attributes:     attrs: Arbitrary metadata about this policy.     expand_groups: If True, expand failures among entities sharing         risk groups with failed entities.     expand_children: If True, expand failed risk groups to include         their children recursively.     seed: Default seed for reproducible random operations. Overridden         by the <code>seed</code> parameter on <code>apply_failures</code> when provided.     modes: List of weighted failure modes.</p> <p>Attributes:</p> <ul> <li><code>attrs</code> (Dict[str, Any]) = {}</li> <li><code>expand_groups</code> (bool) = False</li> <li><code>expand_children</code> (bool) = False</li> <li><code>seed</code> (Optional[int])</li> <li><code>modes</code> (List[FailureMode]) = []</li> </ul> <p>Methods:</p> <ul> <li><code>apply_failures(self, network_nodes: 'Dict[str, Any]', network_links: 'Dict[str, Any]', network_risk_groups: 'Dict[str, Any] | None' = None, *, seed: 'Optional[int]' = None, failure_trace: 'Optional[Dict[str, Any]]' = None) -&gt; 'List[str]'</code> - Identify which entities fail for this iteration.</li> <li><code>to_dict(self) -&gt; 'Dict[str, Any]'</code> - Convert to dictionary for JSON serialization.</li> </ul>"},{"location":"reference/api-full/#failurerule","title":"FailureRule","text":"<p>Defines how to match and then select entities for failure.</p> <p>Attributes:     scope: The type of entities this rule applies to: \"node\", \"link\",         or \"risk_group\".     conditions: A list of conditions to filter matching entities.     logic: \"and\" (all must be true) or \"or\" (any must be true, default).     mode: The selection strategy among the matched set:</p> <ul> <li>\"random\": each matched entity is chosen with probability.</li> <li>\"choice\": pick exactly <code>count</code> items (random sample).</li> <li> <p>\"all\": select every matched entity.</p> <p>probability: Probability in [0,1], used if mode=\"random\". count: Number of entities to pick if mode=\"choice\". weight_by: Optional attribute for weighted sampling in choice mode. path: Optional regex pattern to filter entities by name.</p> </li> </ul> <p>Attributes:</p> <ul> <li><code>scope</code> (EntityScope)</li> <li><code>conditions</code> (List[Condition]) = []</li> <li><code>logic</code> (Literal['and', 'or']) = or</li> <li><code>mode</code> (Literal['random', 'choice', 'all']) = all</li> <li><code>probability</code> (float) = 1.0</li> <li><code>count</code> (int) = 1</li> <li><code>weight_by</code> (Optional[str])</li> <li><code>path</code> (Optional[str])</li> </ul>"},{"location":"reference/api-full/#ngraphmodelfailurepolicy_set","title":"ngraph.model.failure.policy_set","text":"<p>Failure policy containers.</p> <p>Provides <code>FailurePolicySet</code>, a named collection of <code>FailurePolicy</code> objects used as input to failure analysis workflows. This module contains input containers, not analysis results.</p>"},{"location":"reference/api-full/#failurepolicyset","title":"FailurePolicySet","text":"<p>Named collection of FailurePolicy objects.</p> <p>This mutable container maps failure policy names to FailurePolicy objects, allowing management of multiple failure policies for analysis.</p> <p>Attributes:     policies: Dictionary mapping failure policy names to FailurePolicy objects.</p> <p>Attributes:</p> <ul> <li><code>policies</code> (dict[str, FailurePolicy]) = {}</li> </ul> <p>Methods:</p> <ul> <li><code>add(self, name: 'str', policy: 'FailurePolicy') -&gt; 'None'</code> - Add a failure policy to the collection.</li> <li><code>get_all_policies(self) -&gt; 'list[FailurePolicy]'</code> - Get all failure policies from the collection.</li> <li><code>get_policy(self, name: 'str') -&gt; 'FailurePolicy'</code> - Get a specific failure policy by name.</li> <li><code>to_dict(self) -&gt; 'dict[str, Any]'</code> - Convert to dictionary for JSON serialization.</li> </ul>"},{"location":"reference/api-full/#ngraphmodelfailurevalidation","title":"ngraph.model.failure.validation","text":"<p>Risk group reference validation.</p> <p>Validates that all risk group references in nodes and links resolve to defined risk groups. Catches typos and missing definitions early.</p> <p>Also provides cycle detection for risk group hierarchies.</p>"},{"location":"reference/api-full/#validate_risk_group_hierarchynetwork-network-none","title":"validate_risk_group_hierarchy(network: \"'Network'\") -&gt; 'None'","text":"<p>Detect circular references in risk group parent-child relationships.</p> <p>Uses DFS-based cycle detection to find any risk group that is part of a cycle in the children hierarchy. This can happen when membership rules with scope='risk_group' create mutual parent-child relationships.</p> <p>Args:     network: Network with risk_groups populated (after membership resolution).</p> <p>Raises:     ValueError: If a cycle is detected, with details about the cycle path.</p>"},{"location":"reference/api-full/#validate_risk_group_referencesnetwork-network-none","title":"validate_risk_group_references(network: \"'Network'\") -&gt; 'None'","text":"<p>Ensure all risk group references resolve to defined groups.</p> <p>Checks that every risk group name referenced by nodes and links exists in network.risk_groups. This catches typos and missing definitions that would otherwise cause silent failures in simulations.</p> <p>Args:     network: Network with nodes, links, and risk_groups populated.</p> <p>Raises:     ValueError: If any node or link references an undefined risk group.         The error message lists up to 10 violations with entity names         and the undefined group names.</p>"},{"location":"reference/api-full/#ngraphmodelflowpolicy_config","title":"ngraph.model.flow.policy_config","text":"<p>Flow policy preset configurations for NetGraph.</p> <p>Provides convenient factory functions to create common FlowPolicy configurations using NetGraph-Core's FlowPolicy and FlowPolicyConfig.</p>"},{"location":"reference/api-full/#flowpolicypreset","title":"FlowPolicyPreset","text":"<p>Enumerates common flow policy presets for traffic routing.</p> <p>These presets map to specific combinations of path algorithms, flow placement strategies, and edge selection modes provided by NetGraph-Core.</p>"},{"location":"reference/api-full/#create_flow_policyalgorithms-netgraph_corealgorithms-graph-netgraph_coregraph-preset-flowpolicypreset-node_masknone-edge_masknone-netgraph_coreflowpolicy","title":"create_flow_policy(algorithms: 'netgraph_core.Algorithms', graph: 'netgraph_core.Graph', preset: 'FlowPolicyPreset', node_mask=None, edge_mask=None) -&gt; 'netgraph_core.FlowPolicy'","text":"<p>Create a FlowPolicy instance from a preset configuration.</p> <p>Args:     algorithms: NetGraph-Core Algorithms instance.     graph: NetGraph-Core Graph handle.     preset: FlowPolicyPreset enum value specifying the desired policy.     node_mask: Optional numpy bool array for node exclusions (True = include).     edge_mask: Optional numpy bool array for edge exclusions (True = include).</p> <p>Returns:     netgraph_core.FlowPolicy: Configured policy instance.</p> <p>Raises:     ValueError: If an unknown FlowPolicyPreset value is provided.</p> <p>Example:     &gt;&gt;&gt; backend = netgraph_core.Backend.cpu()     &gt;&gt;&gt; algs = netgraph_core.Algorithms(backend)     &gt;&gt;&gt; graph = algs.build_graph(strict_multidigraph)     &gt;&gt;&gt; policy = create_flow_policy(algs, graph, FlowPolicyPreset.SHORTEST_PATHS_ECMP)</p>"},{"location":"reference/api-full/#serialize_policy_presetcfg-any-optionalstr","title":"serialize_policy_preset(cfg: 'Any') -&gt; 'Optional[str]'","text":"<p>Serialize a FlowPolicyPreset to its string name for JSON storage.</p> <p>Handles FlowPolicyPreset enum values, integer enum values, and string inputs. Returns None for None input.</p> <p>Args:     cfg: FlowPolicyPreset enum, integer, or other value to serialize.</p> <p>Returns:     String name of the preset (e.g., \"SHORTEST_PATHS_ECMP\"), or None if input is None.</p>"},{"location":"reference/api-full/#ngraphmodelnetwork","title":"ngraph.model.network","text":"<p>Network topology modeling with Node, Link, RiskGroup, and Network classes.</p> <p>This module provides the core network model classes (Node, Link, RiskGroup, Network) that can be used independently.</p>"},{"location":"reference/api-full/#link","title":"Link","text":"<p>Represents one directed link between two nodes.</p> <p>The model stores a single direction (<code>source</code> -&gt; <code>target</code>). When building the working graph for analysis, a reverse edge is added by default to provide bidirectional connectivity. Disable with <code>add_reverse=False</code> in <code>Network.to_strict_multidigraph</code>.</p> <p>Attributes:     source (str): Name of the source node.     target (str): Name of the target node.     capacity (float): Link capacity (default 1.0).     cost (float): Link cost (default 1.0).     disabled (bool): Whether the link is disabled.     risk_groups (Set[str]): Set of risk group names this link belongs to.     attrs (Dict[str, Any]): Additional metadata (e.g., distance).     id (str): Auto-generated unique identifier: \"{source}|{target}|\". <p>Attributes:</p> <ul> <li><code>source</code> (str)</li> <li><code>target</code> (str)</li> <li><code>capacity</code> (float) = 1.0</li> <li><code>cost</code> (float) = 1.0</li> <li><code>disabled</code> (bool) = False</li> <li><code>risk_groups</code> (Set[str]) = set()</li> <li><code>attrs</code> (Dict[str, Any]) = {}</li> <li><code>id</code> (str)</li> </ul>"},{"location":"reference/api-full/#network","title":"Network","text":"<p>A container for network nodes and links.</p> <p>Network represents the scenario-level topology with persistent state (nodes/links that are disabled in the scenario configuration). For temporary exclusion of nodes/links during analysis (e.g., failure simulation), use node_mask and edge_mask parameters when calling NetGraph-Core algorithms.</p> <p>Attributes:     nodes (Dict[str, Node]): Mapping from node name -&gt; Node object.     links (Dict[str, Link]): Mapping from link ID -&gt; Link object.     risk_groups (Dict[str, RiskGroup]): Top-level risk groups by name.     attrs (Dict[str, Any]): Optional metadata about the network.</p> <p>Attributes:</p> <ul> <li><code>nodes</code> (Dict[str, Node]) = {}</li> <li><code>links</code> (Dict[str, Link]) = {}</li> <li><code>risk_groups</code> (Dict[str, RiskGroup]) = {}</li> <li><code>attrs</code> (Dict[str, Any]) = {}</li> <li><code>_selection_cache</code> (Dict[str, Dict[str, List[Node]]]) = {}</li> </ul> <p>Methods:</p> <ul> <li><code>add_link(self, link: 'Link') -&gt; 'None'</code> - Add a link to the network (keyed by the link's auto-generated ID).</li> <li><code>add_node(self, node: 'Node') -&gt; 'None'</code> - Add a node to the network (keyed by node.name).</li> <li><code>disable_all(self) -&gt; 'None'</code> - Mark all nodes and links as disabled.</li> <li><code>disable_link(self, link_id: 'str') -&gt; 'None'</code> - Mark a link as disabled.</li> <li><code>disable_node(self, node_name: 'str') -&gt; 'None'</code> - Mark a node as disabled.</li> <li><code>disable_risk_group(self, name: 'str', recursive: 'bool' = True) -&gt; 'None'</code> - Disable all nodes/links that have 'name' in their risk_groups.</li> <li><code>enable_all(self) -&gt; 'None'</code> - Mark all nodes and links as enabled.</li> <li><code>enable_link(self, link_id: 'str') -&gt; 'None'</code> - Mark a link as enabled.</li> <li><code>enable_node(self, node_name: 'str') -&gt; 'None'</code> - Mark a node as enabled.</li> <li><code>enable_risk_group(self, name: 'str', recursive: 'bool' = True) -&gt; 'None'</code> - Enable all nodes/links that have 'name' in their risk_groups.</li> <li><code>find_links(self, source_regex: 'Optional[str]' = None, target_regex: 'Optional[str]' = None, any_direction: 'bool' = False) -&gt; 'List[Link]'</code> - Search for links using optional regex patterns for source or target node names.</li> <li><code>get_links_between(self, source: 'str', target: 'str') -&gt; 'List[str]'</code> - Retrieve all link IDs that connect the specified source node</li> <li><code>select_node_groups_by_path(self, path: 'str') -&gt; 'Dict[str, List[Node]]'</code> - Select and group nodes by regex pattern on node name.</li> </ul>"},{"location":"reference/api-full/#node","title":"Node","text":"<p>Represents a node in the network.</p> <p>Each node is uniquely identified by its name, which is used as the key in the Network's node dictionary.</p> <p>Attributes:     name (str): Unique identifier for the node.     disabled (bool): Whether the node is disabled in the scenario configuration.     risk_groups (Set[str]): Set of risk group names this node belongs to.     attrs (Dict[str, Any]): Additional metadata (e.g., coordinates, region).</p> <p>Attributes:</p> <ul> <li><code>name</code> (str)</li> <li><code>disabled</code> (bool) = False</li> <li><code>risk_groups</code> (Set[str]) = set()</li> <li><code>attrs</code> (Dict[str, Any]) = {}</li> </ul>"},{"location":"reference/api-full/#riskgroup","title":"RiskGroup","text":"<p>Represents a shared-risk or failure domain, which may have nested children.</p> <p>Risk groups model correlated failures: when a risk group fails, all entities (nodes, links) in that group fail together. Hierarchical children enable cascading failures (parent failure implies all descendants fail).</p> <p>Risk groups can be created three ways:</p> <ol> <li>Direct definition: Explicitly named in YAML risk_groups section</li> <li>Membership rules: Auto-assign entities based on attribute matching</li> <li>Generate blocks: Auto-create groups from unique attribute values</li> </ol> <p>Attributes:     name (str): Unique name of this risk group.     children (List[RiskGroup]): Subdomains in a nested structure.     disabled (bool): Whether this group was declared disabled on load.     attrs (Dict[str, Any]): Additional metadata for the risk group.     _membership_raw (Optional[Dict[str, Any]]): Raw membership rule for         deferred resolution. Internal use only.</p> <p>Attributes:</p> <ul> <li><code>name</code> (str)</li> <li><code>children</code> (List[RiskGroup]) = []</li> <li><code>disabled</code> (bool) = False</li> <li><code>attrs</code> (Dict[str, Any]) = {}</li> <li><code>_membership_raw</code> (Optional[Dict[str, Any]])</li> </ul>"},{"location":"reference/api-full/#ngraphmodelpath","title":"ngraph.model.path","text":"<p>Lightweight representation of a single routing path.</p> <p>The <code>Path</code> dataclass stores a node-and-parallel-edges sequence and a numeric cost. Cached properties expose derived sequences for nodes and edges, and helpers provide equality, ordering by cost, and sub-path extraction with cost recalculation.</p>"},{"location":"reference/api-full/#path","title":"Path","text":"<p>Represents a single path in the network.</p> <p>Attributes:     path: Sequence of (node_name, (edge_refs...)) tuples representing the path.           The final element typically has an empty tuple of edge refs.     cost: Total numeric cost (e.g., distance or metric) of the path.     edges: Set of all EdgeRefs encountered in the path.     nodes: Set of all node names encountered in the path.     edge_tuples: Set of all tuples of parallel EdgeRefs from each path element.</p> <p>Attributes:</p> <ul> <li><code>path</code> (Tuple[Tuple[str, Tuple[EdgeRef, ...]], ...])</li> <li><code>cost</code> (Cost)</li> <li><code>edges</code> (Set[EdgeRef]) = set()</li> <li><code>nodes</code> (Set[str]) = set()</li> <li><code>edge_tuples</code> (Set[Tuple[EdgeRef, ...]]) = set()</li> </ul> <p>Methods:</p> <ul> <li><code>get_sub_path(self, dst_node: 'str') -&gt; 'Path'</code> - Create a sub-path ending at the specified destination node.</li> </ul>"},{"location":"reference/api-full/#ngraphworkflowbase","title":"ngraph.workflow.base","text":"<p>Base classes for workflow automation.</p> <p>Defines the workflow step abstraction, registration decorator, and execution lifecycle. Steps implement <code>run()</code> and are executed via <code>execute()</code> which handles timing, logging, and metadata recording. Failures are logged and re-raised.</p>"},{"location":"reference/api-full/#workflowstep","title":"WorkflowStep","text":"<p>Base class for all workflow steps.</p> <p>All workflow steps are automatically logged with execution timing information. All workflow steps support seeding for reproducible random operations. Workflow metadata is automatically stored in scenario.results for analysis.</p> <p>YAML Configuration:     <pre><code>workflow:\n  - type: &lt;StepTypeName&gt;\n\n    name: \"optional_step_name\"  # Optional: Custom name for this step instance\n    seed: 42                    # Optional: Seed for reproducible random operations\n    # ... step-specific parameters ...\n</code></pre></p> <p>Attributes:     name: Optional custom identifier for this workflow step instance,         used for logging and result storage purposes.     seed: Optional seed for reproducible random operations. If None,         random operations will be non-deterministic.</p> <p>Attributes:</p> <ul> <li><code>name</code> (str)</li> <li><code>seed</code> (Optional[int])</li> <li><code>_seed_source</code> (str)</li> </ul> <p>Methods:</p> <ul> <li><code>execute(self, scenario: \"'Scenario'\") -&gt; 'None'</code> - Execute the workflow step with logging and metadata storage.</li> <li><code>run(self, scenario: \"'Scenario'\") -&gt; 'None'</code> - Execute the workflow step logic.</li> </ul>"},{"location":"reference/api-full/#register_workflow_stepstep_type-str","title":"register_workflow_step(step_type: 'str')","text":"<p>Return a decorator that registers a <code>WorkflowStep</code> subclass.</p> <p>Args:     step_type: Registry key used to instantiate steps from configuration.</p> <p>Returns:     A class decorator that adds the class to <code>WORKFLOW_STEP_REGISTRY</code>.</p>"},{"location":"reference/api-full/#resolve_parallelismparallelism-unionint-str-int","title":"resolve_parallelism(parallelism: 'Union[int, str]') -&gt; 'int'","text":"<p>Resolve parallelism setting to a concrete worker count.</p> <p>Args:     parallelism: Either an integer worker count or \"auto\" for CPU count.</p> <p>Returns:     Positive integer worker count (minimum 1).</p>"},{"location":"reference/api-full/#ngraphworkflowbuild_graph","title":"ngraph.workflow.build_graph","text":"<p>Graph building workflow component.</p> <p>Validates and exports network topology as a node-link representation using NetworkX. Actual graph building for analysis happens in analysis functions; this step primarily validates the network and stores a serializable representation for inspection.</p> <p>YAML Configuration Example:     <pre><code>workflow:\n  - type: BuildGraph\n\n    name: \"build_network_graph\"  # Optional: Custom name for this step\n    add_reverse: true  # Optional: Add reverse edges (default: true)\n</code></pre></p> <p>The <code>add_reverse</code> parameter controls whether reverse edges are added for each link. When <code>True</code> (default), each Link(A\u2192B) gets both forward(A\u2192B) and reverse(B\u2192A) edges for bidirectional connectivity. Set to <code>False</code> for directed-only graphs.</p> <p>Results stored in <code>scenario.results</code> under the step name as two keys:</p> <ul> <li>metadata: Step-level execution metadata (node/link counts)</li> <li>data: { graph: node-link JSON dict, context: { add_reverse: bool } }</li> </ul>"},{"location":"reference/api-full/#buildgraph","title":"BuildGraph","text":"<p>Validates network topology and stores node-link representation.</p> <p>This step validates the network structure and stores a JSON-serializable node-link representation using NetworkX. Core graph building happens in analysis functions as needed.</p> <p>Attributes:     add_reverse: If True, adds reverse edges for bidirectional connectivity.                  Defaults to True.</p> <p>Attributes:</p> <ul> <li><code>name</code> (str)</li> <li><code>seed</code> (Optional[int])</li> <li><code>_seed_source</code> (str)</li> <li><code>add_reverse</code> (bool) = True</li> </ul> <p>Methods:</p> <ul> <li><code>execute(self, scenario: \"'Scenario'\") -&gt; 'None'</code> - Execute the workflow step with logging and metadata storage.</li> <li><code>run(self, scenario: 'Scenario') -&gt; 'None'</code> - Validate network and store node-link representation.</li> </ul>"},{"location":"reference/api-full/#ngraphworkflowcost_power","title":"ngraph.workflow.cost_power","text":"<p>CostPower workflow step: collect capex and power by hierarchy level.</p> <p>This step aggregates capex and power from the network hardware inventory without performing any normalization or reporting. It separates contributions into two categories:</p> <ul> <li>platform_*: node hardware (e.g., chassis, linecards) resolved from node attrs</li> <li>optics_*: per-end link hardware (e.g., optics) resolved from link attrs</li> </ul> <p>Aggregation is computed at hierarchy levels 0..N where level 0 is the global root (path \"\"), and higher levels correspond to prefixes of node names split by \"/\". For example, for node \"dc1/plane1/leaf/leaf-1\":</p> <ul> <li>level 1 path is \"dc1\"</li> <li>level 2 path is \"dc1/plane1\"</li> <li>etc.</li> </ul> <p>Disabled handling:</p> <ul> <li>When include_disabled is False, only enabled nodes and links are considered.</li> <li>Optics are counted only when the endpoint node has platform hardware.</li> </ul> <p>YAML Configuration Example:     <pre><code>workflow:\n  - type: CostPower\n\n    name: \"cost_power\"           # Optional custom name\n    include_disabled: false       # Default: only enabled nodes/links\n    aggregation_level: 2          # Produce levels: 0, 1, 2\n</code></pre></p> <p>Results stored in <code>scenario.results</code> under this step namespace:     data:       context:         include_disabled: bool         aggregation_level: int       levels:         \"0\":</p> <ul> <li>path: \"\"<pre><code>    platform_capex: float\n    platform_power_watts: float\n    optics_capex: float\n    optics_power_watts: float\n    capex_total: float\n    power_total_watts: float\n\"1\": [ ... ]\n\"2\": [ ... ]\n</code></pre> </li> </ul>"},{"location":"reference/api-full/#costpower","title":"CostPower","text":"<p>Collect platform and optics capex/power by aggregation level.</p> <p>Attributes:     include_disabled: If True, include disabled nodes and links.     aggregation_level: Inclusive depth for aggregation. 0=root only.</p> <p>Attributes:</p> <ul> <li><code>name</code> (str)</li> <li><code>seed</code> (Optional[int])</li> <li><code>_seed_source</code> (str)</li> <li><code>include_disabled</code> (bool) = False</li> <li><code>aggregation_level</code> (int) = 2</li> </ul> <p>Methods:</p> <ul> <li><code>execute(self, scenario: \"'Scenario'\") -&gt; 'None'</code> - Execute the workflow step with logging and metadata storage.</li> <li><code>run(self, scenario: 'Any') -&gt; 'None'</code> - Aggregate capex and power by hierarchy levels 0..N.</li> </ul>"},{"location":"reference/api-full/#ngraphworkflowmax_flow_step","title":"ngraph.workflow.max_flow_step","text":"<p>MaxFlow workflow step.</p> <p>Monte Carlo analysis of maximum flow capacity between node groups using FailureManager. Produces unified <code>flow_results</code> per iteration under <code>data.flow_results</code>.</p> <p>Baseline (no failures) is always run first as a separate reference. The <code>iterations</code> parameter specifies how many failure scenarios to run.</p> <p>YAML Configuration Example:</p> <pre><code>workflow:\n</code></pre> <ul> <li>type: MaxFlow<pre><code>name: \"maxflow_dc_to_edge\"\nsource: \"^datacenter/.*\"\ntarget: \"^edge/.*\"\nmode: \"combine\"\nfailure_policy: \"random_failures\"\niterations: 100\nparallelism: auto\nshortest_path: false\nrequire_capacity: true           # false for true IP/IGP semantics\nflow_placement: \"PROPORTIONAL\"\nseed: 42\nstore_failure_patterns: false\ninclude_flow_details: false      # cost_distribution\ninclude_min_cut: false           # min-cut edges list\n</code></pre> </li> </ul>"},{"location":"reference/api-full/#maxflow","title":"MaxFlow","text":"<p>Maximum flow Monte Carlo workflow step.</p> <p>Baseline (no failures) is always run first as a separate reference. Results are returned with baseline in a separate field. The flow_results list contains unique failure patterns (deduplicated); each result has occurrence_count indicating how many iterations matched that pattern.</p> <p>Attributes:     source: Source node selector (string path or selector dict).     target: Target node selector (string path or selector dict).     mode: Flow analysis mode (\"combine\" or \"pairwise\").     failure_policy: Name of failure policy in scenario.failure_policy_set.     iterations: Number of failure iterations to run.     parallelism: Number of parallel worker processes.     shortest_path: Whether to use shortest paths only.     require_capacity: If True (default), path selection considers capacity.         If False, path selection is cost-only (true IP/IGP semantics).     flow_placement: Flow placement strategy.     seed: Optional seed for reproducible results.     store_failure_patterns: Whether to store failure patterns in results.     include_flow_details: Whether to collect cost distribution per flow.     include_min_cut: Whether to include min-cut edges per flow.</p> <p>Attributes:</p> <ul> <li><code>name</code> (str)</li> <li><code>seed</code> (int | None)</li> <li><code>_seed_source</code> (str)</li> <li><code>source</code> (Union[str, Dict[str, Any]])</li> <li><code>target</code> (Union[str, Dict[str, Any]])</li> <li><code>mode</code> (str) = combine</li> <li><code>failure_policy</code> (str | None)</li> <li><code>iterations</code> (int) = 1</li> <li><code>parallelism</code> (int | str) = auto</li> <li><code>shortest_path</code> (bool) = False</li> <li><code>require_capacity</code> (bool) = True</li> <li><code>flow_placement</code> (FlowPlacement | str) = 1</li> <li><code>store_failure_patterns</code> (bool) = False</li> <li><code>include_flow_details</code> (bool) = False</li> <li><code>include_min_cut</code> (bool) = False</li> </ul> <p>Methods:</p> <ul> <li><code>execute(self, scenario: \"'Scenario'\") -&gt; 'None'</code> - Execute the workflow step with logging and metadata storage.</li> <li><code>run(self, scenario: \"'Scenario'\") -&gt; 'None'</code> - Execute the workflow step logic.</li> </ul>"},{"location":"reference/api-full/#ngraphworkflowmaximum_supported_demand_step","title":"ngraph.workflow.maximum_supported_demand_step","text":"<p>MaximumSupportedDemand workflow step.</p> <p>Searches for the maximum uniform traffic multiplier <code>alpha_star</code> that is fully placeable for a given demand set. Stores results under <code>data</code> as:</p> <ul> <li><code>alpha_star</code>: float</li> <li><code>context</code>: parameters used for the search</li> <li><code>base_demands</code>: serialized base demand specs</li> <li><code>probes</code>: bracket/bisect evaluations with feasibility</li> </ul> <p>Performance: AnalysisContext is built once at search start and reused across all binary search probes. Only demand volumes change per probe.</p> <p>YAML Configuration Example:     <pre><code>workflow:\n  - type: MaximumSupportedDemand\n\n    name: \"msd_search\"\n    demand_set: \"default\"\n    resolution: 0.01        # Convergence threshold\n    max_bisect_iters: 50    # Maximum bisection iterations\n    alpha_start: 1.0        # Starting multiplier\n    growth_factor: 2.0      # Bracket expansion factor\n</code></pre></p>"},{"location":"reference/api-full/#maximumsupporteddemand","title":"MaximumSupportedDemand","text":"<p>Finds the maximum uniform traffic multiplier that is fully placeable.</p> <p>Uses binary search to find alpha_star, the maximum multiplier for all demands in the set that can still be fully placed on the network.</p> <p>Attributes:     demand_set: Name of the demand set to analyze.     acceptance_rule: Currently only \"hard\" is implemented.     alpha_start: Starting multiplier for binary search.     growth_factor: Factor for bracket expansion.     alpha_min: Minimum allowed alpha value.     alpha_max: Maximum allowed alpha value.     resolution: Convergence threshold for binary search.     max_bracket_iters: Maximum iterations for bracketing phase.     max_bisect_iters: Maximum iterations for bisection phase.     placement_rounds: Placement optimization rounds.</p> <p>Attributes:</p> <ul> <li><code>name</code> (str)</li> <li><code>seed</code> (Optional[int])</li> <li><code>_seed_source</code> (str)</li> <li><code>demand_set</code> (str) = default</li> <li><code>acceptance_rule</code> (str) = hard</li> <li><code>alpha_start</code> (float) = 1.0</li> <li><code>growth_factor</code> (float) = 2.0</li> <li><code>alpha_min</code> (float) = 1e-06</li> <li><code>alpha_max</code> (float) = 1000000000.0</li> <li><code>resolution</code> (float) = 0.01</li> <li><code>max_bracket_iters</code> (int) = 32</li> <li><code>max_bisect_iters</code> (int) = 32</li> <li><code>placement_rounds</code> (int | str) = auto</li> </ul> <p>Methods:</p> <ul> <li><code>execute(self, scenario: \"'Scenario'\") -&gt; 'None'</code> - Execute the workflow step with logging and metadata storage.</li> <li><code>run(self, scenario: \"'Any'\") -&gt; 'None'</code> - Execute the workflow step logic.</li> </ul>"},{"location":"reference/api-full/#ngraphworkflownetwork_stats","title":"ngraph.workflow.network_stats","text":"<p>Workflow step for basic node and link statistics.</p> <p>Computes and stores network statistics including node/link counts, capacity distributions, cost distributions, and degree distributions. Supports optional exclusion simulation and disabled entity handling.</p> <p>YAML Configuration Example:     <pre><code>workflow:\n  - type: NetworkStats\n\n    name: \"network_statistics\"           # Optional: Custom name for this step\n    include_disabled: false              # Include disabled nodes/links in stats\n    excluded_nodes: [\"node1\", \"node2\"]   # Optional: Temporary node exclusions\n    excluded_links: [\"link1\", \"link3\"]   # Optional: Temporary link exclusions\n</code></pre></p> <p>Results stored in <code>scenario.results</code>:</p> <ul> <li>Node statistics: node_count</li> <li> <p>Link statistics: link_count, total_capacity, mean_capacity, median_capacity,</p> <p>min_capacity, max_capacity, mean_cost, median_cost, min_cost, max_cost</p> </li> <li> <p>Degree statistics: mean_degree, median_degree, min_degree, max_degree</p> </li> </ul>"},{"location":"reference/api-full/#networkstats","title":"NetworkStats","text":"<p>Compute basic node and link statistics for the network.</p> <p>Supports optional exclusion simulation without modifying the base network.</p> <p>Attributes:     include_disabled: If True, include disabled nodes and links in statistics.         If False, only consider enabled entities.     excluded_nodes: Optional list of node names to exclude (temporary exclusion).     excluded_links: Optional list of link IDs to exclude (temporary exclusion).</p> <p>Attributes:</p> <ul> <li><code>name</code> (str)</li> <li><code>seed</code> (Optional[int])</li> <li><code>_seed_source</code> (str)</li> <li><code>include_disabled</code> (bool) = False</li> <li><code>excluded_nodes</code> (Iterable[str]) = ()</li> <li><code>excluded_links</code> (Iterable[str]) = ()</li> </ul> <p>Methods:</p> <ul> <li><code>execute(self, scenario: \"'Scenario'\") -&gt; 'None'</code> - Execute the workflow step with logging and metadata storage.</li> <li><code>run(self, scenario: 'Scenario') -&gt; 'None'</code> - Compute and store network statistics.</li> </ul>"},{"location":"reference/api-full/#ngraphworkflowparse","title":"ngraph.workflow.parse","text":"<p>Workflow parsing helpers.</p> <p>Converts a normalized workflow section (list[dict]) into WorkflowStep instances using the WORKFLOW_STEP_REGISTRY and attaches unique names/seeds.</p>"},{"location":"reference/api-full/#build_workflow_stepsworkflow_data-listdictstr-any-derive_seed-callablestr-optionalint-listworkflowstep","title":"build_workflow_steps(workflow_data: 'List[Dict[str, Any]]', derive_seed: 'Callable[[str], Optional[int]]') -&gt; 'List[WorkflowStep]'","text":"<p>Instantiate workflow steps from normalized dictionaries.</p> <p>Args:     workflow_data: List of step dicts; each must have \"type\".     derive_seed: Callable that takes a step name and returns a seed or None.</p> <p>Returns:     A list of WorkflowStep instances with unique names and optional seeds.</p>"},{"location":"reference/api-full/#ngraphworkflowtraffic_matrix_placement_step","title":"ngraph.workflow.traffic_matrix_placement_step","text":"<p>TrafficMatrixPlacement workflow step.</p> <p>Runs Monte Carlo demand placement using a named demand set and produces unified <code>flow_results</code> per iteration under <code>data.flow_results</code>.</p> <p>Baseline (no failures) is always run first as a separate reference. The <code>iterations</code> parameter specifies how many failure scenarios to run.</p> <p>YAML Configuration Example:     <pre><code>workflow:\n  - type: TrafficMatrixPlacement\n\n    name: \"tm_analysis\"\n    demand_set: \"default\"\n    failure_policy: \"single_link\"    # Optional: failure policy name\n    iterations: 100                  # Number of failure scenarios\n    parallelism: 4                   # Worker processes (or \"auto\")\n    alpha: 1.0                       # Demand volume multiplier\n    include_flow_details: true       # Include cost distribution per flow\n</code></pre></p>"},{"location":"reference/api-full/#trafficmatrixplacement","title":"TrafficMatrixPlacement","text":"<p>Monte Carlo demand placement using a named demand set.</p> <p>Baseline (no failures) is always run first as a separate reference. Results are returned with baseline in a separate field. The flow_results list contains unique failure patterns (deduplicated); each result has occurrence_count indicating how many iterations matched that pattern.</p> <p>Attributes:     demand_set: Name of the demand set to analyze.     failure_policy: Optional failure policy name in scenario.failure_policy_set.     iterations: Number of failure iterations to run.     parallelism: Number of parallel worker processes.     placement_rounds: Placement optimization rounds (int or \"auto\").     seed: Optional seed for reproducibility.     store_failure_patterns: Whether to store failure pattern results.     include_flow_details: When True, include cost_distribution per flow.     include_used_edges: When True, include set of used edges per demand in entry data.     alpha: Numeric scale for demands in the set.     alpha_from_step: Optional producer step name to read alpha from.     alpha_from_field: Dotted field path in producer step (default: \"data.alpha_star\").</p> <p>Attributes:</p> <ul> <li><code>name</code> (str)</li> <li><code>seed</code> (int | None)</li> <li><code>_seed_source</code> (str)</li> <li><code>demand_set</code> (str)</li> <li><code>failure_policy</code> (str | None)</li> <li><code>iterations</code> (int) = 1</li> <li><code>parallelism</code> (int | str) = auto</li> <li><code>placement_rounds</code> (int | str) = auto</li> <li><code>store_failure_patterns</code> (bool) = False</li> <li><code>include_flow_details</code> (bool) = False</li> <li><code>include_used_edges</code> (bool) = False</li> <li><code>alpha</code> (float) = 1.0</li> <li><code>alpha_from_step</code> (str | None)</li> <li><code>alpha_from_field</code> (str) = data.alpha_star</li> </ul> <p>Methods:</p> <ul> <li><code>execute(self, scenario: \"'Scenario'\") -&gt; 'None'</code> - Execute the workflow step with logging and metadata storage.</li> <li><code>run(self, scenario: \"'Scenario'\") -&gt; 'None'</code> - Execute the workflow step logic.</li> </ul>"},{"location":"reference/api-full/#ngraphdslblueprintsexpand","title":"ngraph.dsl.blueprints.expand","text":"<p>Network topology blueprints and generation.</p>"},{"location":"reference/api-full/#blueprint","title":"Blueprint","text":"<p>Represents a reusable blueprint for hierarchical sub-topologies.</p> <p>A blueprint may contain multiple node definitions (each can have count and template), plus link definitions describing how those nodes connect.</p> <p>Attributes:     name: Unique identifier of this blueprint.     nodes: A mapping of node_name -&gt; node definition.     links: A list of link definitions.</p> <p>Attributes:</p> <ul> <li><code>name</code> (str)</li> <li><code>nodes</code> (Dict[str, Any])</li> <li><code>links</code> (List[Dict[str, Any]])</li> </ul>"},{"location":"reference/api-full/#dslexpansioncontext","title":"DSLExpansionContext","text":"<p>Carries the blueprint definitions and the final Network instance to be populated during DSL expansion.</p> <p>Attributes:     blueprints: Dictionary of blueprint-name -&gt; Blueprint.     network: The Network into which expanded nodes/links are inserted.     pending_bp_links: Deferred blueprint link expansions.</p> <p>Attributes:</p> <ul> <li><code>blueprints</code> (Dict[str, Blueprint])</li> <li><code>network</code> (Network)</li> <li><code>pending_bp_links</code> (List[tuple[Dict[str, Any], str]]) = []</li> </ul>"},{"location":"reference/api-full/#expand_network_dsldata-dictstr-any-network","title":"expand_network_dsl(data: 'Dict[str, Any]') -&gt; 'Network'","text":"<p>Expands a combined blueprint + network DSL into a complete Network object.</p> <p>Overall flow:   1) Parse \"blueprints\" into Blueprint objects.   2) Build a Network from \"network\" metadata (e.g. name, version).   3) Expand 'network[\"nodes\"]' (collect blueprint links for later).</p> <ul> <li> <p>If a node group references a blueprint, incorporate that blueprint's</p> <p>nodes while merging parent's attrs + disabled + risk_groups.    Blueprint links are deferred and processed after node rules.</p> </li> <li> <p>Otherwise, directly create nodes (a \"direct node group\").</p> </li> </ul> <p>4) Process node rules (in order if multiple rules match).   5) Expand deferred blueprint links.   6) Expand link definitions in 'network[\"links\"]'.   7) Process link rules (in order if multiple rules match).</p> <p>Field validation rules:</p> <ul> <li>Only certain top-level fields are permitted in each structure.</li> <li>Link properties are flat (capacity, cost, etc. at link level).</li> <li> <p>For node definitions: count, template, attrs, disabled, risk_groups,</p> <p>or blueprint for blueprint-based nodes.</p> </li> </ul> <p>Args:     data: The YAML-parsed dictionary containing optional \"blueprints\" + \"network\".</p> <p>Returns:     The expanded Network object with all nodes and links.</p>"},{"location":"reference/api-full/#ngraphdslblueprintsparser","title":"ngraph.dsl.blueprints.parser","text":"<p>Parsing helpers for the network DSL.</p> <p>This module factors out pure parsing/validation helpers from the expansion module so they can be tested independently and reused.</p>"},{"location":"reference/api-full/#check_link_keyslink_def-dictstr-any-context-str-none","title":"check_link_keys(link_def: 'Dict[str, Any]', context: 'str') -&gt; 'None'","text":"<p>Ensure link definitions only contain recognized keys.</p>"},{"location":"reference/api-full/#check_no_extra_keysdata_dict-dictstr-any-allowed-setstr-context-str-none","title":"check_no_extra_keys(data_dict: 'Dict[str, Any]', allowed: 'set[str]', context: 'str') -&gt; 'None'","text":"<p>Raise if <code>data_dict</code> contains keys outside <code>allowed</code>.</p> <p>Args:     data_dict: The dict to check.     allowed: Set of recognized keys.     context: Short description used in error messages.</p>"},{"location":"reference/api-full/#join_pathsparent_path-str-rel_path-str-str","title":"join_paths(parent_path: 'str', rel_path: 'str') -&gt; 'str'","text":"<p>Join two path segments according to DSL conventions.</p> <p>The DSL has no concept of absolute paths. All paths are relative to the current context (parent_path). A leading \"/\" on rel_path is stripped and has no functional effect - it serves only as a visual indicator that the path starts from the current scope's root.</p> <p>Behavior:</p> <ul> <li>Leading \"/\" on rel_path is stripped (not treated as filesystem root)</li> <li>Result is always: \"{parent_path}/{stripped_rel_path}\" if parent_path is non-empty</li> <li> <p>Examples:</p> <p>join_paths(\"\", \"/leaf\") -&gt; \"leaf\" join_paths(\"pod1\", \"/leaf\") -&gt; \"pod1/leaf\" join_paths(\"pod1\", \"leaf\") -&gt; \"pod1/leaf\"  (same result)</p> </li> </ul> <p>Args:     parent_path: Parent path prefix (e.g., \"pod1\" when expanding a blueprint).     rel_path: Path to join. Leading \"/\" is stripped if present.</p> <p>Returns:     Combined path string.</p>"},{"location":"reference/api-full/#ngraphdslexpansionbrackets","title":"ngraph.dsl.expansion.brackets","text":"<p>Bracket expansion for name patterns.</p> <p>Provides expand_name_patterns() for expanding bracket expressions like \"fa[1-3]\" into [\"fa1\", \"fa2\", \"fa3\"].</p>"},{"location":"reference/api-full/#expand_name_patternsname-str-liststr","title":"expand_name_patterns(name: 'str') -&gt; 'List[str]'","text":"<p>Expand bracket expressions in a group name.</p> <p>Supports:</p> <ul> <li>Ranges: [1-3] -&gt; 1, 2, 3</li> <li>Lists: [a,b,c] -&gt; a, b, c</li> <li>Mixed: [1,3,5-7] -&gt; 1, 3, 5, 6, 7</li> <li>Multiple brackets: Cartesian product</li> </ul> <p>Args:     name: Name pattern with optional bracket expressions.</p> <p>Returns:     List of expanded names.</p> <p>Examples:     &gt;&gt;&gt; expand_name_patterns(\"fa[1-3]\")     [\"fa1\", \"fa2\", \"fa3\"]     &gt;&gt;&gt; expand_name_patterns(\"dc[1,3,5-6]\")     [\"dc1\", \"dc3\", \"dc5\", \"dc6\"]     &gt;&gt;&gt; expand_name_patterns(\"fa[1-2]_plane[5-6]\")     [\"fa1_plane5\", \"fa1_plane6\", \"fa2_plane5\", \"fa2_plane6\"]</p>"},{"location":"reference/api-full/#expand_risk_group_refsrg_list-iterablestr-setstr","title":"expand_risk_group_refs(rg_list: 'Iterable[str]') -&gt; 'Set[str]'","text":"<p>Expand bracket patterns in a list of risk group references.</p> <p>Takes an iterable of risk group names (possibly containing bracket expressions) and returns a set of all expanded names.</p> <p>Args:     rg_list: Iterable of risk group name patterns.</p> <p>Returns:     Set of expanded risk group names.</p> <p>Examples:     &gt;&gt;&gt; expand_risk_group_refs([\"RG1\"])     {\"RG1\"}     &gt;&gt;&gt; expand_risk_group_refs([\"RG[1-3]\"])     {\"RG1\", \"RG2\", \"RG3\"}     &gt;&gt;&gt; expand_risk_group_refs([\"A[1-2]\", \"B[a,b]\"])     {\"A1\", \"A2\", \"Ba\", \"Bb\"}</p>"},{"location":"reference/api-full/#ngraphdslexpansionschema","title":"ngraph.dsl.expansion.schema","text":"<p>Schema definitions for variable expansion.</p> <p>Provides dataclasses for template expansion configuration.</p>"},{"location":"reference/api-full/#expansionspec","title":"ExpansionSpec","text":"<p>Specification for variable-based expansion.</p> <p>Attributes:     vars: Mapping of variable names to lists of values.     mode: How to combine variable values.</p> <ul> <li>\"cartesian\": All combinations (default)</li> <li>\"zip\": Pair values by position</li> </ul> <p>Attributes:</p> <ul> <li><code>vars</code> (Dict[str, List[Any]]) = {}</li> <li><code>mode</code> (Literal['cartesian', 'zip']) = cartesian</li> </ul> <p>Methods:</p> <ul> <li><code>from_dict(data: 'Dict[str, Any]') -&gt; \"Optional['ExpansionSpec']\"</code> - Extract expand: block from dict.</li> <li><code>is_empty(self) -&gt; 'bool'</code> - Check if no variables are defined.</li> </ul>"},{"location":"reference/api-full/#ngraphdslexpansionvariables","title":"ngraph.dsl.expansion.variables","text":"<p>Variable expansion for templates.</p> <p>Provides substitution of $var and ${var} placeholders in strings, with recursive substitution in nested structures.</p>"},{"location":"reference/api-full/#expand_blockblock-dictstr-any-spec-optionalexpansionspec-iteratordictstr-any","title":"expand_block(block: 'Dict[str, Any]', spec: \"Optional['ExpansionSpec']\") -&gt; 'Iterator[Dict[str, Any]]'","text":"<p>Expand a DSL block, yielding one dict per variable combination.</p> <p>If no expand spec is provided or it has no vars, yields the original block. Otherwise, yields a deep copy with all strings substituted for each variable combination.</p> <p>Args:     block: DSL block (dict) that may contain template strings.     spec: Optional expansion specification.</p> <p>Yields:     Dict with variable substitutions applied.</p>"},{"location":"reference/api-full/#expand_templatestemplates-dictstr-str-spec-expansionspec-iteratordictstr-str","title":"expand_templates(templates: 'Dict[str, str]', spec: \"'ExpansionSpec'\") -&gt; 'Iterator[Dict[str, str]]'","text":"<p>Expand template strings with variable substitution.</p> <p>Uses $var or ${var} syntax only.</p> <p>Args:     templates: Dict of template strings.     spec: Expansion specification with variables and mode.</p> <p>Yields:     Dicts with same keys as templates, values substituted.</p> <p>Raises:     ValueError: If zip mode has mismatched list lengths or expansion exceeds limit.     KeyError: If a template references an undefined variable.</p>"},{"location":"reference/api-full/#substitute_varsobj-any-var_dict-dictstr-any-any","title":"substitute_vars(obj: 'Any', var_dict: 'Dict[str, Any]') -&gt; 'Any'","text":"<p>Recursively substitute ${var} in all strings within obj.</p> <p>Args:     obj: Any value (string, dict, list, or primitive).     var_dict: Mapping of variable names to values.</p> <p>Returns:     Object with all string values having variables substituted.</p>"},{"location":"reference/api-full/#ngraphdslloader","title":"ngraph.dsl.loader","text":"<p>YAML loader + schema validation for Scenario DSL.</p> <p>Provides a single entrypoint to parse a YAML string, normalize keys where needed, validate against the packaged JSON schema, and return a canonical dictionary suitable for downstream expansion/parsing.</p>"},{"location":"reference/api-full/#load_scenario_yamlyaml_str-str-dictstr-any","title":"load_scenario_yaml(yaml_str: 'str') -&gt; 'Dict[str, Any]'","text":"<p>Load, normalize, and validate a Scenario YAML string.</p> <p>Returns a canonical dictionary representation that downstream parsers can consume without worrying about YAML-specific quirks (e.g., boolean-like keys) and with schema shape already enforced.</p>"},{"location":"reference/api-full/#ngraphdslselectorsconditions","title":"ngraph.dsl.selectors.conditions","text":"<p>Condition evaluation for node/entity filtering.</p> <p>Provides evaluation logic for attribute conditions used in selectors and failure policies. Supports operators: ==, !=, &lt;, &lt;=, &gt;, &gt;=, contains, not_contains, in, not_in, exists, not_exists.</p> <p>Supports dot-notation for nested attribute access (e.g., \"hardware.vendor\").</p>"},{"location":"reference/api-full/#evaluate_conditionattrs-dictstr-any-cond-condition-bool","title":"evaluate_condition(attrs: 'Dict[str, Any]', cond: \"'Condition'\") -&gt; 'bool'","text":"<p>Evaluate a single condition against an attribute dict.</p> <p>Supports dot-notation for nested attribute access (e.g., \"hardware.vendor\").</p> <p>Args:     attrs: Mapping of entity attributes (may contain nested dicts).     cond: Condition to evaluate.</p> <p>Returns:     True if condition passes, False otherwise.</p> <p>Raises:     ValueError: If operator is unknown or value type is invalid.</p>"},{"location":"reference/api-full/#evaluate_conditionsattrs-dictstr-any-conditions-iterablecondition-logic-str-or-bool","title":"evaluate_conditions(attrs: 'Dict[str, Any]', conditions: \"Iterable['Condition']\", logic: 'str' = 'or') -&gt; 'bool'","text":"<p>Evaluate multiple conditions with AND/OR logic.</p> <p>Args:     attrs: Flat mapping of entity attributes.     conditions: Iterable of Condition objects.     logic: \"and\" (all must match) or \"or\" (any must match).</p> <p>Returns:     True if combined predicate passes.</p> <p>Raises:     ValueError: If logic is not \"and\" or \"or\".</p>"},{"location":"reference/api-full/#resolve_attr_pathattrs-dictstr-any-path-str-tuplebool-any","title":"resolve_attr_path(attrs: 'Dict[str, Any]', path: 'str') -&gt; 'Tuple[bool, Any]'","text":"<p>Resolve a dot-notation attribute path.</p> <p>Supports nested attribute access like \"hardware.vendor\" which resolves to attrs[\"hardware\"][\"vendor\"].</p> <p>Args:     attrs: Attribute dict (may contain nested dicts).     path: Attribute path, optionally with dots for nesting.</p> <p>Returns:     Tuple of (found, value). If found is False, value is None.</p> <p>Examples:     &gt;&gt;&gt; resolve_attr_path({\"role\": \"spine\"}, \"role\")     (True, \"spine\")     &gt;&gt;&gt; resolve_attr_path({\"hardware\": {\"vendor\": \"Acme\"}}, \"hardware.vendor\")     (True, \"Acme\")     &gt;&gt;&gt; resolve_attr_path({\"role\": \"spine\"}, \"missing\")     (False, None)</p>"},{"location":"reference/api-full/#ngraphdslselectorsnormalize","title":"ngraph.dsl.selectors.normalize","text":"<p>Selector parsing and normalization.</p> <p>Provides the single entry point for converting raw selector values (strings or dicts) into NodeSelector objects.</p>"},{"location":"reference/api-full/#normalize_selectorraw-unionstr-dictstr-any-nodeselector-context-str-nodeselector","title":"normalize_selector(raw: 'Union[str, Dict[str, Any], NodeSelector]', context: 'str') -&gt; 'NodeSelector'","text":"<p>Normalize a raw selector (string or dict) to a NodeSelector.</p> <p>This is the single entry point for all selector parsing. All downstream code works with NodeSelector objects only.</p> <p>Args:     raw: Either a regex string, selector dict, or existing NodeSelector.     context: Usage context (\"adjacency\", \"demand\", \"override\", \"workflow\").         Determines the default for active_only.</p> <p>Returns:     Normalized NodeSelector instance.</p> <p>Raises:     ValueError: If selector format is invalid or context is unknown.</p>"},{"location":"reference/api-full/#parse_match_specraw-dictstr-any-default_logic-literaland-or-or-require_conditions-bool-false-context-str-match-matchspec","title":"parse_match_spec(raw: 'Dict[str, Any]', *, default_logic: \"Literal['and', 'or']\" = 'or', require_conditions: 'bool' = False, context: 'str' = 'match') -&gt; 'MatchSpec'","text":"<p>Parse a match specification from raw dict.</p> <p>Unified match specification parser for use across adjacency, demands, membership rules, and failure policies.</p> <p>Args:     raw: Dict with 'conditions' list and optional 'logic'.     default_logic: Default when 'logic' not specified.     require_conditions: If True, raise when conditions list is empty.     context: Used in error messages.</p> <p>Returns:     Parsed MatchSpec.</p> <p>Raises:     ValueError: If validation fails.</p>"},{"location":"reference/api-full/#ngraphdslselectorsschema","title":"ngraph.dsl.selectors.schema","text":"<p>Schema definitions for unified node selection.</p> <p>Provides dataclasses for node selection configuration used across network rules, demands, and workflow steps.</p>"},{"location":"reference/api-full/#condition","title":"Condition","text":"<p>A single attribute condition for filtering.</p> <p>Supports dot-notation for nested attribute access (e.g., \"hardware.vendor\" resolves to attrs[\"hardware\"][\"vendor\"]).</p> <p>Attributes:     attr: Attribute name to match (supports dot-notation for nested attrs).     op: Comparison operator.     value: Right-hand operand (unused for exists/not_exists).</p> <p>Attributes:</p> <ul> <li><code>attr</code> (str)</li> <li><code>op</code> (Literal['==', '!=', '&lt;', '&lt;=', '&gt;', '&gt;=', 'contains', 'not_contains', 'in', 'not_in', 'exists', 'not_exists'])</li> <li><code>value</code> (Any)</li> </ul>"},{"location":"reference/api-full/#matchspec","title":"MatchSpec","text":"<p>Specification for filtering nodes by attribute conditions.</p> <p>Attributes:     conditions: List of conditions to evaluate.     logic: How to combine conditions (\"and\" = all, \"or\" = any).</p> <p>Attributes:</p> <ul> <li><code>conditions</code> (List[Condition]) = []</li> <li><code>logic</code> (Literal['and', 'or']) = or</li> </ul>"},{"location":"reference/api-full/#nodeselector","title":"NodeSelector","text":"<p>Unified node selection specification.</p> <p>Evaluation order:</p> <ol> <li>Select nodes matching <code>path</code> regex (default \".*\" if omitted)</li> <li>Filter by <code>match</code> conditions</li> <li>Filter by <code>active_only</code> flag</li> <li>Group by <code>group_by</code> attribute (if specified)</li> </ol> <p>At least one of path, group_by, or match must be specified.</p> <p>Attributes:     path: Regex pattern on node.name.     group_by: Attribute name to group nodes by.     match: Attribute-based filtering conditions.     active_only: Whether to exclude disabled nodes. None uses context default.</p> <p>Attributes:</p> <ul> <li><code>path</code> (Optional[str])</li> <li><code>group_by</code> (Optional[str])</li> <li><code>match</code> (Optional[MatchSpec])</li> <li><code>active_only</code> (Optional[bool])</li> </ul>"},{"location":"reference/api-full/#ngraphdslselectorsselect","title":"ngraph.dsl.selectors.select","text":"<p>Node selection and evaluation.</p> <p>Provides the unified select_nodes() function that handles regex matching, attribute filtering, active-only filtering, and grouping.</p>"},{"location":"reference/api-full/#flatten_link_attrslink-link-link_id-str-dictstr-any","title":"flatten_link_attrs(link: \"'Link'\", link_id: 'str') -&gt; 'Dict[str, Any]'","text":"<p>Build flat attribute dict for condition evaluation on links.</p> <p>Merges link's top-level fields with link.attrs. Top-level fields take precedence on key conflicts.</p> <p>Args:     link: Link object to flatten.     link_id: The link's ID in the network.</p> <p>Returns:     Flat dict suitable for condition evaluation.</p>"},{"location":"reference/api-full/#flatten_node_attrsnode-node-dictstr-any","title":"flatten_node_attrs(node: \"'Node'\") -&gt; 'Dict[str, Any]'","text":"<p>Build flat attribute dict for condition evaluation.</p> <p>Merges node's top-level fields (name, disabled, risk_groups) with node.attrs. Top-level fields take precedence on key conflicts.</p> <p>Args:     node: Node object to flatten.</p> <p>Returns:     Flat dict suitable for condition evaluation.</p>"},{"location":"reference/api-full/#flatten_risk_group_attrsrg-unionriskgroup-dictstr-any-dictstr-any","title":"flatten_risk_group_attrs(rg: \"Union['RiskGroup', Dict[str, Any]]\") -&gt; 'Dict[str, Any]'","text":"<p>Build flat attribute dict for condition evaluation on risk groups.</p> <p>Merges risk group's top-level fields (name, disabled, children) with rg.attrs. Top-level fields take precedence on key conflicts.</p> <p>Supports both RiskGroup objects and dict representations (for flexibility in failure policy matching).</p> <p>Args:     rg: RiskGroup object or dict representation.</p> <p>Returns:     Flat dict suitable for condition evaluation.</p>"},{"location":"reference/api-full/#match_entity_idsentity_attrs-dictstr-dictstr-any-conditions-listcondition-logic-str-or-setstr","title":"match_entity_ids(entity_attrs: 'Dict[str, Dict[str, Any]]', conditions: 'List[Condition]', logic: 'str' = 'or') -&gt; 'Set[str]'","text":"<p>Match entity IDs by attribute conditions.</p> <p>General primitive for condition-based entity selection. Works with any entity type as long as attributes are pre-flattened.</p> <p>Args:     entity_attrs: Mapping of {entity_id: flattened_attrs_dict}     conditions: List of conditions to evaluate     logic: \"and\" (all must match) or \"or\" (any must match)</p> <p>Returns:     Set of matching entity IDs. Returns all IDs if conditions is empty.</p>"},{"location":"reference/api-full/#select_nodesnetwork-network-selector-nodeselector-default_active_only-bool-excluded_nodes-optionalsetstr-none-dictstr-listnode","title":"select_nodes(network: \"'Network'\", selector: 'NodeSelector', default_active_only: 'bool', excluded_nodes: 'Optional[Set[str]]' = None) -&gt; \"Dict[str, List['Node']]\"","text":"<p>Unified entry point for node selection.</p> <p>Evaluation order:</p> <ol> <li>Select nodes matching <code>path</code> regex (or all nodes if path is None)</li> <li>Filter by <code>match</code> conditions</li> <li>Filter by <code>active_only</code> flag and excluded_nodes</li> <li>Group by <code>group_by</code> attribute (overrides regex capture grouping)</li> </ol> <p>Args:     network: The network graph.     selector: Node selection specification.     default_active_only: Context-aware default for active_only flag.         Required parameter to prevent silent bugs.     excluded_nodes: Additional node names to exclude.</p> <p>Returns:     Dict mapping group labels to lists of nodes.</p>"},{"location":"reference/api-full/#ngraphresultsartifacts","title":"ngraph.results.artifacts","text":"<p>Serializable result artifacts for analysis workflows.</p> <p>This module defines dataclasses that capture outputs from analyses and simulations in a JSON-serializable form:</p> <ul> <li><code>CapacityEnvelope</code>: frequency-based capacity distributions and optional</li> </ul> <p>aggregated flow statistics</p> <ul> <li><code>FailurePatternResult</code>: capacity results for specific failure patterns</li> </ul>"},{"location":"reference/api-full/#capacityenvelope","title":"CapacityEnvelope","text":"<p>Frequency-based capacity envelope that stores capacity values as frequencies.</p> <p>This approach is memory-efficient for Monte Carlo analysis where we care about statistical distributions rather than individual sample order.</p> <p>Attributes:     source_pattern: Regex pattern used to select source nodes.     sink_pattern: Regex pattern used to select sink nodes.     mode: Flow analysis mode (\"combine\" or \"pairwise\").     frequencies: Dictionary mapping capacity values to their occurrence counts.     min_capacity: Minimum observed capacity.     max_capacity: Maximum observed capacity.     mean_capacity: Mean capacity across all samples.     stdev_capacity: Standard deviation of capacity values.     total_samples: Total number of samples represented.     flow_summary_stats: Optional dictionary with aggregated FlowSummary statistics.                        Contains cost_distribution_stats and other flow analytics.</p> <p>Attributes:</p> <ul> <li><code>source_pattern</code> (str)</li> <li><code>sink_pattern</code> (str)</li> <li><code>mode</code> (str)</li> <li><code>frequencies</code> (Dict[float, int])</li> <li><code>min_capacity</code> (float)</li> <li><code>max_capacity</code> (float)</li> <li><code>mean_capacity</code> (float)</li> <li><code>stdev_capacity</code> (float)</li> <li><code>total_samples</code> (int)</li> <li><code>flow_summary_stats</code> (Dict[str, Any]) = {}</li> </ul> <p>Methods:</p> <ul> <li><code>expand_to_values(self) -&gt; 'List[float]'</code> - Expand frequency map back to individual values.</li> <li><code>from_dict(data: 'Dict[str, Any]') -&gt; \"'CapacityEnvelope'\"</code> - Construct a CapacityEnvelope from a dictionary.</li> <li><code>from_values(source_pattern: 'str', sink_pattern: 'str', mode: 'str', values: 'List[float]', flow_summaries: 'List[Any] | None' = None) -&gt; \"'CapacityEnvelope'\"</code> - Create envelope from capacity values and optional flow summaries.</li> <li><code>get_percentile(self, percentile: 'float') -&gt; 'float'</code> - Calculate percentile from frequency distribution.</li> <li><code>to_dict(self) -&gt; 'Dict[str, Any]'</code> - Convert to dictionary for JSON serialization.</li> </ul>"},{"location":"reference/api-full/#failurepatternresult","title":"FailurePatternResult","text":"<p>Result for a unique failure pattern with associated capacity matrix.</p> <p>Attributes:     excluded_nodes: List of failed node IDs.     excluded_links: List of failed link IDs.     capacity_matrix: Dictionary mapping flow keys to capacity values.     count: Number of times this pattern occurred.     is_baseline: Whether this represents the baseline (no failures) case.</p> <p>Attributes:</p> <ul> <li><code>excluded_nodes</code> (List[str])</li> <li><code>excluded_links</code> (List[str])</li> <li><code>capacity_matrix</code> (Dict[str, float])</li> <li><code>count</code> (int)</li> <li><code>is_baseline</code> (bool) = False</li> <li><code>_pattern_key_cache</code> (str)</li> </ul> <p>Methods:</p> <ul> <li><code>from_dict(data: 'Dict[str, Any]') -&gt; \"'FailurePatternResult'\"</code> - Construct FailurePatternResult from a dictionary.</li> <li><code>to_dict(self) -&gt; 'Dict[str, Any]'</code> - Convert to dictionary for JSON serialization.</li> </ul>"},{"location":"reference/api-full/#ngraphresultsflow","title":"ngraph.results.flow","text":"<p>Unified flow result containers for failure-analysis iterations.</p> <p>Defines small, serializable dataclasses that capture per-iteration outcomes for capacity and demand-placement style analyses in a unit-agnostic form.</p> <p>Objects expose <code>to_dict()</code> that returns JSON-safe primitives. Float-keyed distributions are normalized to string keys via <code>_fmt_float_key()</code>, and arbitrary <code>data</code> payloads are sanitized. These dicts are written under <code>data.flow_results</code> by steps.</p> <p>Utilities:     _fmt_float_key: Formats floats as stable string keys for JSON serialization.         Uses fixed-point notation with trailing zeros stripped for human-readable,         canonical representations of numeric keys like cost distributions.</p>"},{"location":"reference/api-full/#flowentry","title":"FlowEntry","text":"<p>Represents a single source\u2192destination flow outcome within an iteration.</p> <p>Fields are unit-agnostic. Callers can interpret numbers as needed for presentation (e.g., Gbit/s).</p> <p>Args:     source: Source identifier.     destination: Destination identifier.     priority: Priority/class for traffic placement scenarios. Zero when not applicable.     demand: Requested volume for this flow.     placed: Delivered volume for this flow.     dropped: Unmet volume (<code>demand - placed</code>).     cost_distribution: Optional distribution of placed volume by path cost.     data: Optional per-flow details (e.g., min-cut edges, used edges).</p> <p>Attributes:</p> <ul> <li><code>source</code> (str)</li> <li><code>destination</code> (str)</li> <li><code>priority</code> (int)</li> <li><code>demand</code> (float)</li> <li><code>placed</code> (float)</li> <li><code>dropped</code> (float)</li> <li><code>cost_distribution</code> (Dict[float, float]) = {}</li> <li><code>data</code> (Dict[str, Any]) = {}</li> </ul> <p>Methods:</p> <ul> <li><code>to_dict(self) -&gt; 'Dict[str, Any]'</code> - Return a JSON-serializable dictionary representation.</li> </ul>"},{"location":"reference/api-full/#flowiterationresult","title":"FlowIterationResult","text":"<p>Container for per-iteration analysis results.</p> <p>Args:     failure_id: Stable identifier for the failure scenario (hash of excluded         components, or \"\" for no exclusions).     failure_state: Optional excluded components for the iteration.     failure_trace: Optional trace info (mode_index, selections, expansion) when         store_failure_patterns=True. None for baseline or when tracing disabled.     occurrence_count: Number of Monte Carlo iterations that produced this exact         failure pattern. Used with deduplication to avoid re-running identical         analyses. Defaults to 1.     flows: List of flow entries for this iteration.     summary: Aggregated summary across <code>flows</code>.     data: Optional per-iteration extras.</p> <p>Attributes:</p> <ul> <li><code>failure_id</code> (str)</li> <li><code>failure_state</code> (Optional[Dict[str, List[str]]])</li> <li><code>failure_trace</code> (Optional[Dict[str, Any]])</li> <li><code>occurrence_count</code> (int) = 1</li> <li><code>flows</code> (List[FlowEntry]) = []</li> <li><code>summary</code> (FlowSummary) = FlowSummary(total_demand=0.0, total_placed=0.0, overall_ratio=1.0, dropped_flows=0, num_flows=0)</li> <li><code>data</code> (Dict[str, Any]) = {}</li> </ul> <p>Methods:</p> <ul> <li><code>to_dict(self) -&gt; 'Dict[str, Any]'</code> - Return a JSON-serializable dictionary representation.</li> </ul>"},{"location":"reference/api-full/#flowsummary","title":"FlowSummary","text":"<p>Aggregated metrics across all flows in one iteration.</p> <p>Args:     total_demand: Sum of all demands in this iteration.     total_placed: Sum of all delivered volumes in this iteration.     overall_ratio: <code>total_placed / total_demand</code> when demand &gt; 0, else 1.0.     dropped_flows: Number of flow entries with non-zero drop.     num_flows: Total number of flows considered.</p> <p>Attributes:</p> <ul> <li><code>total_demand</code> (float)</li> <li><code>total_placed</code> (float)</li> <li><code>overall_ratio</code> (float)</li> <li><code>dropped_flows</code> (int)</li> <li><code>num_flows</code> (int)</li> </ul> <p>Methods:</p> <ul> <li><code>to_dict(self) -&gt; 'Dict[str, Any]'</code> - Return a JSON-serializable dictionary representation.</li> </ul>"},{"location":"reference/api-full/#ngraphresultssnapshot","title":"ngraph.results.snapshot","text":"<p>Scenario snapshot helpers.</p> <p>Build a concise dictionary snapshot of failure policies and demand sets for export into results without keeping heavy domain objects.</p>"},{"location":"reference/api-full/#build_scenario_snapshot-seed-int-none-failure_policy_set-demand_set-dictstr-any","title":"build_scenario_snapshot(*, seed: 'int | None', failure_policy_set, demand_set) -&gt; 'Dict[str, Any]'","text":"<p>Build a concise dictionary snapshot of the scenario state.</p> <p>Creates a serializable representation of the scenario's failure policies and demand sets, suitable for export into results without keeping heavy domain objects.</p> <p>Args:     seed: Scenario-level seed for reproducibility, or None if unseeded.     failure_policy_set: FailurePolicySet containing named failure policies.     demand_set: DemandSet containing named demand collections.</p> <p>Returns:     Dict containing: seed, failures (policy snapshots), demands (demand snapshots).</p>"},{"location":"reference/api-full/#ngraphresultsstore","title":"ngraph.results.store","text":"<p>Generic results store for workflow steps and their metadata.</p> <p><code>Results</code> organizes outputs by workflow step name and records <code>WorkflowStepMetadata</code> for execution context. Storage is strictly step-scoped: steps must write two keys under their namespace:</p> <ul> <li><code>metadata</code>: step-level metadata (dict)</li> <li><code>data</code>: step-specific payload (dict)</li> </ul> <p>Export with :meth:<code>Results.to_dict</code>, which returns a JSON-safe structure with shape <code>{workflow, steps, scenario}</code>. During export, objects with a <code>to_dict()</code> method are converted, dictionary keys are coerced to strings, tuples are emitted as lists, and only JSON primitives are produced.</p>"},{"location":"reference/api-full/#results","title":"Results","text":"<p>Step-scoped results container with deterministic export shape.</p> <p>Structure:</p> <ul> <li>workflow: step metadata registry</li> <li>steps: per-step results with enforced keys {\"metadata\", \"data\"}</li> <li>scenario: optional scenario snapshot set once at load time</li> </ul> <p>Attributes:</p> <ul> <li><code>_store</code> (Dict) = {}</li> <li><code>_metadata</code> (Dict) = {}</li> <li><code>_active_step</code> (Optional)</li> <li><code>_scenario</code> (Dict) = {}</li> </ul> <p>Methods:</p> <ul> <li><code>enter_step(self, step_name: str) -&gt; None</code> - Enter step scope. Subsequent put/get are scoped to this step.</li> <li><code>exit_step(self) -&gt; None</code> - Exit step scope.</li> <li><code>get(self, key: str, default: Any = None) -&gt; Any</code> - Get a value from the active step scope.</li> <li><code>get_all_step_metadata(self) -&gt; Dict[str, ngraph.results.store.WorkflowStepMetadata]</code> - Get metadata for all workflow steps.</li> <li><code>get_step(self, step_name: str) -&gt; Dict[str, Any]</code> - Return the raw dict for a given step name (for cross-step reads).</li> <li><code>get_step_metadata(self, step_name: str) -&gt; Optional[ngraph.results.store.WorkflowStepMetadata]</code> - Get metadata for a workflow step.</li> <li><code>get_steps_by_execution_order(self) -&gt; list[str]</code> - Get step names ordered by their execution order.</li> <li><code>put(self, key: str, value: Any) -&gt; None</code> - Store a value in the active step under an allowed key.</li> <li><code>put_step_metadata(self, step_name: str, step_type: str, execution_order: int, *, scenario_seed: Optional[int] = None, step_seed: Optional[int] = None, seed_source: str = 'none', active_seed: Optional[int] = None) -&gt; None</code> - Store metadata for a workflow step.</li> <li><code>set_scenario_snapshot(self, snapshot: Dict[str, Any]) -&gt; None</code> - Attach a normalized scenario snapshot for export.</li> <li><code>to_dict(self) -&gt; Dict[str, Any]</code> - Return exported results with shape: {workflow, steps, scenario}.</li> </ul>"},{"location":"reference/api-full/#workflowstepmetadata","title":"WorkflowStepMetadata","text":"<p>Metadata for a workflow step execution.</p> <p>Attributes:     step_type: The workflow step class name (e.g., 'CapacityEnvelopeAnalysis').     step_name: The instance name of the step.     execution_order: Order in which this step was executed (0-based).     scenario_seed: Scenario-level seed provided in the YAML (if any).     step_seed: Seed assigned to this step (explicit or scenario-derived).     seed_source: Source for the step seed. One of:</p> <ul> <li>\"scenario-derived\": seed was derived from scenario.seed</li> <li>\"explicit-step\": seed was explicitly provided for the step</li> <li> <p>\"none\": no seed provided/active for this step</p> <p>active_seed: The effective base seed used by the step, if any. For steps     that use Monte Carlo execution, per-iteration seeds are derived from     active_seed (e.g., active_seed + iteration_index).</p> </li> </ul> <p>Attributes:</p> <ul> <li><code>step_type</code> (str)</li> <li><code>step_name</code> (str)</li> <li><code>execution_order</code> (int)</li> <li><code>scenario_seed</code> (Optional)</li> <li><code>step_seed</code> (Optional)</li> <li><code>seed_source</code> (str) = none</li> <li><code>active_seed</code> (Optional)</li> </ul>"},{"location":"reference/api-full/#ngraphprofilingprofiler","title":"ngraph.profiling.profiler","text":"<p>Profiling for NetGraph workflow execution.</p> <p>Provides CPU and wall-clock timing per workflow step using <code>cProfile</code> and optionally peak memory via <code>tracemalloc</code>. Aggregates results into structured summaries and identifies time-dominant steps (bottlenecks).</p>"},{"location":"reference/api-full/#performanceprofiler","title":"PerformanceProfiler","text":"<p>CPU profiler for NetGraph workflow execution.</p> <p>Profiles workflow steps using cProfile and identifies bottlenecks.</p> <p>Methods:</p> <ul> <li><code>analyze_performance(self) -&gt; 'None'</code> - Analyze profiling results and identify bottlenecks.</li> <li><code>end_scenario(self) -&gt; 'None'</code> - End profiling for the entire scenario execution.</li> <li><code>get_top_functions(self, step_name: 'str', limit: 'int' = 10) -&gt; 'List[Tuple[str, float, int]]'</code> - Get the top CPU-consuming functions for a specific step.</li> <li><code>merge_child_profiles(self, profile_dir: 'Path', step_name: 'str') -&gt; 'None'</code> - Merge child worker profiles into the parent step profile.</li> <li><code>profile_step(self, step_name: 'str', step_type: 'str') -&gt; 'Generator[None, None, None]'</code> - Context manager for profiling individual workflow steps.</li> <li><code>save_detailed_profile(self, output_path: 'Path', step_name: 'Optional[str]' = None) -&gt; 'None'</code> - Save detailed profiling data to a file.</li> <li><code>start_scenario(self) -&gt; 'None'</code> - Start profiling for the entire scenario execution.</li> </ul>"},{"location":"reference/api-full/#performancereporter","title":"PerformanceReporter","text":"<p>Format and render performance profiling results.</p> <p>Generates plain-text reports with timing analysis, bottleneck identification, and practical performance tuning suggestions.</p> <p>Methods:</p> <ul> <li><code>generate_report(self) -&gt; 'str'</code> - Generate performance report.</li> </ul>"},{"location":"reference/api-full/#profileresults","title":"ProfileResults","text":"<p>Profiling results for a scenario execution.</p> <p>Attributes:     step_profiles: List of individual step performance profiles.     total_wall_time: Total wall-clock time for entire scenario.     total_cpu_time: Total CPU time across all steps.     total_function_calls: Total function calls across all steps.     bottlenecks: List of performance bottlenecks (&gt;10% execution time).     analysis_summary: Performance metrics and statistics.</p> <p>Attributes:</p> <ul> <li><code>step_profiles</code> (List[StepProfile]) = []</li> <li><code>total_wall_time</code> (float) = 0.0</li> <li><code>total_cpu_time</code> (float) = 0.0</li> <li><code>total_function_calls</code> (int) = 0</li> <li><code>bottlenecks</code> (List[Dict[str, Any]]) = []</li> <li><code>analysis_summary</code> (Dict[str, Any]) = {}</li> </ul>"},{"location":"reference/api-full/#stepprofile","title":"StepProfile","text":"<p>Performance profile data for a single workflow step.</p> <p>Attributes:     step_name: Name of the workflow step.     step_type: Type/class name of the workflow step.     wall_time: Total wall-clock time in seconds.     cpu_time: CPU time spent in step execution.     function_calls: Number of function calls during execution.     memory_peak: Peak memory usage during step in bytes (if available).     cprofile_stats: Detailed cProfile statistics object.     worker_profiles_merged: Number of worker profiles merged into this step.</p> <p>Attributes:</p> <ul> <li><code>step_name</code> (str)</li> <li><code>step_type</code> (str)</li> <li><code>wall_time</code> (float)</li> <li><code>cpu_time</code> (float)</li> <li><code>function_calls</code> (int)</li> <li><code>memory_peak</code> (Optional[float])</li> <li><code>cprofile_stats</code> (Optional[pstats.Stats])</li> <li><code>worker_profiles_merged</code> (int) = 0</li> </ul>"},{"location":"reference/api-full/#ngraphtypesbase","title":"ngraph.types.base","text":"<p>Base classes and enums for network analysis algorithms.</p>"},{"location":"reference/api-full/#edgeselect","title":"EdgeSelect","text":"<p>Edge selection criteria for shortest-path algorithms.</p> <p>Determines which edges are considered when finding paths between nodes. These map to NetGraph-Core's EdgeSelection configuration.</p>"},{"location":"reference/api-full/#flowplacement","title":"FlowPlacement","text":"<p>Strategies to distribute flow across parallel equal-cost paths.</p>"},{"location":"reference/api-full/#mode","title":"Mode","text":"<p>Analysis mode for source/sink group handling.</p> <p>Determines how multiple source and sink nodes are combined for analysis.</p>"},{"location":"reference/api-full/#ngraphtypesdto","title":"ngraph.types.dto","text":"<p>Types and data structures for algorithm analytics.</p> <p>Defines immutable summary containers for algorithm outputs.</p>"},{"location":"reference/api-full/#edgeref","title":"EdgeRef","text":"<p>Reference to a directed edge via scenario link_id and direction.</p> <p>Provides stable, scenario-native edge identification across Core reorderings using the link's unique ID rather than node name tuples.</p> <p>Attributes:     link_id: Scenario link identifier (matches Network.links keys)     direction: 'fwd' for source\u2192target as defined in Link; 'rev' for reverse</p> <p>Attributes:</p> <ul> <li><code>link_id</code> (str)</li> <li><code>direction</code> (EdgeDir)</li> </ul>"},{"location":"reference/api-full/#maxflowresult","title":"MaxFlowResult","text":"<p>Result of max-flow computation between a source/sink pair.</p> <p>Captures total flow, cost distribution, and optionally min-cut edges.</p> <p>Attributes:     total_flow: Maximum flow value achieved.     cost_distribution: Mapping of path cost to flow volume placed at that cost.     min_cut: Saturated edges forming the min-cut (None if not computed).</p> <p>Attributes:</p> <ul> <li><code>total_flow</code> (float)</li> <li><code>cost_distribution</code> (Dict[Cost, float])</li> <li><code>min_cut</code> (Tuple[EdgeRef, ...] | None)</li> </ul>"},{"location":"reference/api-full/#ngraphutilsids","title":"ngraph.utils.ids","text":""},{"location":"reference/api-full/#new_base64_uuid-str","title":"new_base64_uuid() -&gt; 'str'","text":"<p>Return a 22-character URL-safe Base64-encoded UUID without padding.</p> <p>The function generates a random version 4 UUID, encodes the 16 raw bytes using URL-safe Base64, removes the two trailing padding characters, and decodes to ASCII. The resulting string length is 22 characters.</p> <p>Returns:     A 22-character URL-safe Base64 representation of a UUID4 without     padding.</p>"},{"location":"reference/api-full/#ngraphutilsoutput_paths","title":"ngraph.utils.output_paths","text":"<p>Utilities for building CLI artifact output paths.</p> <p>This module centralizes logic for composing file and directory paths for artifacts produced by the NetGraph CLI. Paths are built from an optional output directory, a prefix (usually derived from the scenario file or results file), and a per-artifact suffix.</p>"},{"location":"reference/api-full/#build_artifact_pathoutput_dir-optionalpath-prefix-str-suffix-str-path","title":"build_artifact_path(output_dir: 'Optional[Path]', prefix: 'str', suffix: 'str') -&gt; 'Path'","text":"<p>Compose an artifact path as output_dir / (prefix + suffix).</p> <p>If <code>output_dir</code> is None, the path is created relative to the current working directory.</p> <p>Args:     output_dir: Base directory for outputs; if None, use CWD.     prefix: Filename prefix; usually derived from scenario or results stem.     suffix: Per-artifact suffix including the dot (e.g. \".results.json\").</p> <p>Returns:     The composed path.</p>"},{"location":"reference/api-full/#ensure_parent_dirpath-path-none","title":"ensure_parent_dir(path: 'Path') -&gt; 'None'","text":"<p>Ensure the parent directory exists for a file path.</p>"},{"location":"reference/api-full/#profiles_dir_for_runscenario_path-path-output_dir-optionalpath-path","title":"profiles_dir_for_run(scenario_path: 'Path', output_dir: 'Optional[Path]') -&gt; 'Path'","text":"<p>Return the directory for child worker profiles for <code>run --profile</code>.</p> <p>Args:     scenario_path: The scenario YAML path.     output_dir: Optional base output directory.</p> <p>Returns:     Directory path where worker profiles should be stored.</p>"},{"location":"reference/api-full/#resolve_override_pathoverride-optionalpath-output_dir-optionalpath-optionalpath","title":"resolve_override_path(override: 'Optional[Path]', output_dir: 'Optional[Path]') -&gt; 'Optional[Path]'","text":"<p>Resolve an override path with respect to an optional output directory.</p> <ul> <li>Absolute override paths are returned as-is.</li> <li>Relative override paths are interpreted as relative to <code>output_dir</code></li> </ul> <p>when provided; otherwise relative to the current working directory.</p> <p>Args:     override: Path provided by the user to override the default.     output_dir: Optional base directory for relative overrides.</p> <p>Returns:     The resolved path or None if no override was provided.</p>"},{"location":"reference/api-full/#results_path_for_runscenario_path-path-output_dir-optionalpath-results_override-optionalpath-path","title":"results_path_for_run(scenario_path: 'Path', output_dir: 'Optional[Path]', results_override: 'Optional[Path]') -&gt; 'Path'","text":"<p>Determine the results JSON path for the <code>run</code> command.</p> <p>Behavior:</p> <ul> <li>If <code>results_override</code> is provided, return it (resolved relative to</li> </ul> <p><code>output_dir</code> when that is specified, otherwise as-is).</p> <ul> <li>Else if <code>output_dir</code> is provided, return <code>output_dir/&lt;prefix&gt;.results.json</code>.</li> <li>Else, return <code>&lt;scenario_stem&gt;.results.json</code> in the current working directory.</li> </ul> <p>Args:     scenario_path: The scenario YAML file path.     output_dir: Optional base output directory.     results_override: Optional explicit results file path.</p> <p>Returns:     The path where results should be written.</p>"},{"location":"reference/api-full/#scenario_prefix_from_pathscenario_path-path-str","title":"scenario_prefix_from_path(scenario_path: 'Path') -&gt; 'str'","text":"<p>Return a safe prefix derived from a scenario file path.</p> <p>Args:     scenario_path: The scenario YAML file path.</p> <p>Returns:     The scenario filename stem, trimmed of extensions.</p>"},{"location":"reference/api-full/#ngraphutilsseed_manager","title":"ngraph.utils.seed_manager","text":"<p>Deterministic seed derivation to avoid global random.seed() order dependencies.</p>"},{"location":"reference/api-full/#seedmanager","title":"SeedManager","text":"<p>Manages deterministic seed derivation for isolated component reproducibility.</p> <p>Global random.seed() creates order dependencies and component interference. SeedManager derives unique seeds per component from a master seed using SHA-256, ensuring reproducible results regardless of execution order or parallelism.</p> <p>Usage:     seed_mgr = SeedManager(42)     failure_seed = seed_mgr.derive_seed(\"failure_policy\", \"default\")</p> <p>Methods:</p> <ul> <li><code>derive_seed(self, *components: 'Any') -&gt; 'Optional[int]'</code> - Derive a deterministic seed from master seed and component identifiers.</li> </ul>"},{"location":"reference/api-full/#ngraphutilsyaml_utils","title":"ngraph.utils.yaml_utils","text":"<p>Utilities for handling YAML parsing quirks and common operations.</p>"},{"location":"reference/api-full/#normalize_yaml_dict_keysdata-dictany-v-dictstr-v","title":"normalize_yaml_dict_keys(data: Dict[Any, ~V]) -&gt; Dict[str, ~V]","text":"<p>Normalize dictionary keys from YAML parsing to ensure consistent string keys.</p> <p>YAML 1.1 boolean keys (e.g., true, false, yes, no, on, off) get converted to Python True/False boolean values. This function converts them to predictable string representations (\"True\"/\"False\") and ensures all keys are strings.</p> <p>Args:     data: Dictionary that may contain boolean or other non-string keys from YAML parsing</p> <p>Returns:     Dictionary with all keys converted to strings, boolean keys converted to \"True\"/\"False\"</p> <p>Examples:     &gt;&gt;&gt; normalize_yaml_dict_keys({True: \"value1\", False: \"value2\", \"normal\": \"value3\"})     {\"True\": \"value1\", \"False\": \"value2\", \"normal\": \"value3\"}</p> <pre><code>&gt;&gt;&gt; # In YAML: true:, yes:, on: all become Python True\n&gt;&gt;&gt; # In YAML: false:, no:, off: all become Python False\n</code></pre>"},{"location":"reference/api-full/#ngraphanalysiscontext","title":"ngraph.analysis.context","text":"<p>AnalysisContext: Prepared state for efficient network analysis.</p> <p>This module provides the primary API for network analysis in NetGraph. AnalysisContext encapsulates Core graph infrastructure and provides methods for max-flow, shortest paths, and sensitivity analysis.</p> <p>Usage:     # One-off analysis     from ngraph import analyze     flow = analyze(network).max_flow(\"^A$\", \"^B$\")</p> <pre><code># Efficient repeated analysis (bound context)\nctx = analyze(network, source=\"^A$\", sink=\"^B$\")\nbaseline = ctx.max_flow()\ndegraded = ctx.max_flow(excluded_links=failed_links)\n</code></pre>"},{"location":"reference/api-full/#analysiscontext","title":"AnalysisContext","text":"<p>Prepared state for efficient network analysis.</p> <p>Encapsulates Core graph infrastructure. Supports two usage patterns:</p> <p>Unbound - flexible, specify source/sink per-call:</p> <pre><code>ctx = AnalysisContext.from_network(network)\ncost = ctx.shortest_path_cost(\"A\", \"B\")\nflow = ctx.max_flow(\"A\", \"B\")  # Builds pseudo-nodes each call\n</code></pre> <p>Bound - optimized for repeated analysis with same groups:</p> <pre><code>ctx = AnalysisContext.from_network(\n    network,\n    source=\"^dc/\",\n    sink=\"^edge/\"\n)\nbaseline = ctx.max_flow()  # Uses pre-built pseudo-nodes\ndegraded = ctx.max_flow(excluded_links=failed)\n</code></pre> <p>Thread Safety:     Immutable after creation. Safe for concurrent analysis calls     with different exclusion sets.</p> <p>Attributes:     network: Reference to source Network (read-only).     is_bound: True if source/sink groups are pre-configured.</p> <p>Attributes:</p> <ul> <li><code>_network</code> ('Network')</li> <li><code>_handle</code> (netgraph_core.Graph)</li> <li><code>_multidigraph</code> (netgraph_core.StrictMultiDiGraph)</li> <li><code>_node_mapper</code> (_NodeMapper)</li> <li><code>_edge_mapper</code> (_EdgeMapper)</li> <li><code>_algorithms</code> (netgraph_core.Algorithms)</li> <li><code>_disabled_node_ids</code> (FrozenSet[int])</li> <li><code>_disabled_link_ids</code> (FrozenSet[str])</li> <li><code>_link_id_to_edge_indices</code> (Mapping[str, Tuple[int, ...]])</li> <li><code>_source</code> (Optional[Union[str, Dict[str, Any]]])</li> <li><code>_sink</code> (Optional[Union[str, Dict[str, Any]]])</li> <li><code>_mode</code> (Optional[Mode])</li> <li><code>_pseudo_context</code> (Optional[_PseudoNodeContext])</li> </ul> <p>Methods:</p> <ul> <li><code>from_network(network: \"'Network'\", *, source: 'Optional[Union[str, Dict[str, Any]]]' = None, sink: 'Optional[Union[str, Dict[str, Any]]]' = None, mode: 'Mode' = &lt;Mode.COMBINE: 1&gt;, augmentations: 'Optional[List[AugmentationEdge]]' = None) -&gt; \"'AnalysisContext'\"</code> - Create analysis context from network.</li> <li><code>k_shortest_paths(self, source: 'Optional[Union[str, Dict[str, Any]]]' = None, sink: 'Optional[Union[str, Dict[str, Any]]]' = None, *, mode: 'Mode' = &lt;Mode.PAIRWISE: 2&gt;, max_k: 'int' = 3, edge_select: 'EdgeSelect' = &lt;EdgeSelect.ALL_MIN_COST: 1&gt;, max_path_cost: 'float' = inf, max_path_cost_factor: 'Optional[float]' = None, split_parallel_edges: 'bool' = False, excluded_nodes: 'Optional[Set[str]]' = None, excluded_links: 'Optional[Set[str]]' = None) -&gt; 'Dict[Tuple[str, str], List[Path]]'</code> - Compute up to K shortest paths per group pair.</li> <li><code>max_flow(self, source: 'Optional[Union[str, Dict[str, Any]]]' = None, sink: 'Optional[Union[str, Dict[str, Any]]]' = None, *, mode: 'Mode' = &lt;Mode.COMBINE: 1&gt;, shortest_path: 'bool' = False, require_capacity: 'bool' = True, flow_placement: 'FlowPlacement' = &lt;FlowPlacement.PROPORTIONAL: 1&gt;, excluded_nodes: 'Optional[Set[str]]' = None, excluded_links: 'Optional[Set[str]]' = None) -&gt; 'Dict[Tuple[str, str], float]'</code> - Compute maximum flow between node groups.</li> <li><code>max_flow_detailed(self, source: 'Optional[Union[str, Dict[str, Any]]]' = None, sink: 'Optional[Union[str, Dict[str, Any]]]' = None, *, mode: 'Mode' = &lt;Mode.COMBINE: 1&gt;, shortest_path: 'bool' = False, require_capacity: 'bool' = True, flow_placement: 'FlowPlacement' = &lt;FlowPlacement.PROPORTIONAL: 1&gt;, excluded_nodes: 'Optional[Set[str]]' = None, excluded_links: 'Optional[Set[str]]' = None, include_min_cut: 'bool' = False) -&gt; 'Dict[Tuple[str, str], MaxFlowResult]'</code> - Compute max flow with detailed results including cost distribution.</li> <li><code>sensitivity(self, source: 'Optional[Union[str, Dict[str, Any]]]' = None, sink: 'Optional[Union[str, Dict[str, Any]]]' = None, *, mode: 'Mode' = &lt;Mode.COMBINE: 1&gt;, shortest_path: 'bool' = False, require_capacity: 'bool' = True, flow_placement: 'FlowPlacement' = &lt;FlowPlacement.PROPORTIONAL: 1&gt;, excluded_nodes: 'Optional[Set[str]]' = None, excluded_links: 'Optional[Set[str]]' = None) -&gt; 'Dict[Tuple[str, str], Dict[str, float]]'</code> - Analyze sensitivity of max flow to edge failures.</li> <li><code>shortest_path_cost(self, source: 'Optional[Union[str, Dict[str, Any]]]' = None, sink: 'Optional[Union[str, Dict[str, Any]]]' = None, *, mode: 'Mode' = &lt;Mode.COMBINE: 1&gt;, edge_select: 'EdgeSelect' = &lt;EdgeSelect.ALL_MIN_COST: 1&gt;, excluded_nodes: 'Optional[Set[str]]' = None, excluded_links: 'Optional[Set[str]]' = None) -&gt; 'Dict[Tuple[str, str], float]'</code> - Compute shortest path costs between node groups.</li> <li><code>shortest_paths(self, source: 'Optional[Union[str, Dict[str, Any]]]' = None, sink: 'Optional[Union[str, Dict[str, Any]]]' = None, *, mode: 'Mode' = &lt;Mode.COMBINE: 1&gt;, edge_select: 'EdgeSelect' = &lt;EdgeSelect.ALL_MIN_COST: 1&gt;, split_parallel_edges: 'bool' = False, excluded_nodes: 'Optional[Set[str]]' = None, excluded_links: 'Optional[Set[str]]' = None) -&gt; 'Dict[Tuple[str, str], List[Path]]'</code> - Compute concrete shortest paths between node groups.</li> </ul>"},{"location":"reference/api-full/#augmentationedge","title":"AugmentationEdge","text":"<p>Edge specification for graph augmentation.</p> <p>Augmentation edges are added to the graph as-is (unidirectional). Nodes referenced in augmentations that don't exist in the network are automatically treated as pseudo/virtual nodes.</p> <p>Attributes:     source: Source node name (real or pseudo)     target: Target node name (real or pseudo)     capacity: Edge capacity     cost: Edge cost (converted to int64 for Core)</p>"},{"location":"reference/api-full/#analyzenetwork-network-source-optionalunionstr-dictstr-any-none-sink-optionalunionstr-dictstr-any-none-mode-mode-augmentations-optionallistaugmentationedge-none-analysiscontext","title":"analyze(network: \"'Network'\", *, source: 'Optional[Union[str, Dict[str, Any]]]' = None, sink: 'Optional[Union[str, Dict[str, Any]]]' = None, mode: 'Mode' = , augmentations: 'Optional[List[AugmentationEdge]]' = None) -&gt; 'AnalysisContext' <p>Create an analysis context for the network.</p> <p>This is THE primary entry point for network analysis in NetGraph.</p> <p>Args:     network: Network topology to analyze.     source: Optional source node selector (string path or selector dict).             If provided with sink, creates bound context with pre-built             pseudo-nodes for efficient repeated flow analysis.     sink: Optional sink node selector (string path or selector dict).     mode: Group mode (COMBINE or PAIRWISE). Only used if bound.     augmentations: Optional custom augmentation edges.</p> <p>Returns:     AnalysisContext ready for analysis calls.</p> <p>Examples:     One-off analysis (unbound context):</p> <pre><code>    flow = analyze(network).max_flow(\"^A$\", \"^B$\")\n    paths = analyze(network).shortest_paths(\"^A$\", \"^B$\")\n\nEfficient repeated analysis (bound context):\n\n    ctx = analyze(network, source=\"^dc/\", sink=\"^edge/\")\n    baseline = ctx.max_flow()\n    degraded = ctx.max_flow(excluded_links=failed_links)\n\nMultiple exclusion scenarios:\n\n    ctx = analyze(network, source=\"^A$\", sink=\"^B$\")\n    for scenario in failure_scenarios:\n        result = ctx.max_flow(excluded_links=scenario)\n</code></pre>","text":""},{"location":"reference/api-full/#build_edge_maskctx-analysiscontext-excluded_links-optionalsetstr-none-npndarray","title":"build_edge_mask(ctx: 'AnalysisContext', excluded_links: 'Optional[Set[str]]' = None) -&gt; 'np.ndarray' <p>Build an edge mask array for Core algorithms.</p> <p>Uses O(|excluded| + |disabled|) time complexity. Core semantics: True = include, False = exclude.</p> <p>Args:     ctx: AnalysisContext with pre-computed edge index mapping.     excluded_links: Optional set of link IDs to exclude.</p> <p>Returns:     Boolean numpy array of shape (num_edges,) where True means included.</p>","text":""},{"location":"reference/api-full/#build_node_maskctx-analysiscontext-excluded_nodes-optionalsetstr-none-npndarray","title":"build_node_mask(ctx: 'AnalysisContext', excluded_nodes: 'Optional[Set[str]]' = None) -&gt; 'np.ndarray' <p>Build a node mask array for Core algorithms.</p> <p>Uses O(|excluded| + |disabled|) time complexity. Core semantics: True = include, False = exclude.</p> <p>Args:     ctx: AnalysisContext with pre-computed disabled node IDs.     excluded_nodes: Optional set of node names to exclude.</p> <p>Returns:     Boolean numpy array of shape (num_nodes,) where True means included.</p>","text":""},{"location":"reference/api-full/#ngraphanalysisdemand","title":"ngraph.analysis.demand","text":"<p>Demand expansion: converts TrafficDemand specs into concrete placement demands.</p> <p>Supports both pairwise and combine modes through augmentation-based pseudo nodes. Uses unified selectors for node selection.</p>"},{"location":"reference/api-full/#demandexpansion","title":"DemandExpansion <p>Demand expansion result.</p> <p>Attributes:     demands: Concrete demands ready for placement (sorted by priority).     augmentations: Augmentation edges for pseudo nodes (empty for pairwise).</p> <p>Attributes:</p> <ul> <li><code>demands</code> (List[ExpandedDemand])</li> <li><code>augmentations</code> (List[AugmentationEdge])</li> </ul>","text":""},{"location":"reference/api-full/#expandeddemand","title":"ExpandedDemand <p>Concrete demand ready for placement.</p> <p>Uses node names (not IDs) so expansion happens before graph building. Node IDs are resolved after the graph is built with pseudo nodes.</p> <p>Attributes:     src_name: Source node name (real or pseudo).     dst_name: Destination node name (real or pseudo).     volume: Traffic volume to place.     priority: Priority class (lower is higher priority).     policy_preset: FlowPolicy configuration preset.     demand_id: Parent TrafficDemand ID for tracking.</p> <p>Attributes:</p> <ul> <li><code>src_name</code> (str)</li> <li><code>dst_name</code> (str)</li> <li><code>volume</code> (float)</li> <li><code>priority</code> (int)</li> <li><code>policy_preset</code> (FlowPolicyPreset)</li> <li><code>demand_id</code> (str)</li> </ul>","text":""},{"location":"reference/api-full/#expand_demandsnetwork-network-traffic_demands-listtrafficdemand-default_policy_preset-flowpolicypreset-demandexpansion","title":"expand_demands(network: 'Network', traffic_demands: 'List[TrafficDemand]', default_policy_preset: 'FlowPolicyPreset' = ) -&gt; 'DemandExpansion' <p>Expand TrafficDemand specifications into concrete demands with augmentations.</p> <p>Pure function that:</p> <ol> <li>Normalizes and evaluates selectors to get node groups</li> <li>Distributes volume based on mode (combine/pairwise) and group_mode</li> <li>Generates augmentation edges for combine mode (pseudo nodes)</li> <li>Returns demands (node names) + augmentations</li> </ol> <p>Node names are used (not IDs) so expansion happens BEFORE graph building. IDs are resolved after graph is built with augmentations.</p> <p>Note: Variable expansion (expand: block) is handled during YAML parsing in build_demand_set(), so TrafficDemand objects here are already expanded.</p> <p>Args:     network: Network for node selection.     traffic_demands: High-level demand specifications.     default_policy_preset: Default policy if demand doesn't specify one.</p> <p>Returns:     DemandExpansion with demands and augmentations.</p> <p>Raises:     ValueError: If no demands could be expanded or unsupported mode.</p>","text":""},{"location":"reference/api-full/#ngraphanalysisfailure_manager","title":"ngraph.analysis.failure_manager","text":"<p>FailureManager for Monte Carlo failure analysis.</p> <p>Provides the failure analysis engine for NetGraph. Supports parallel processing, graph caching, and failure policy handling for workflow steps and direct programmatic use.</p> <p>Performance characteristics: Time complexity: O(S + I * A / P), where S is one-time graph setup cost, I is iteration count, A is per-iteration analysis cost, and P is parallelism. Graph caching amortizes expensive graph construction across all iterations, and O(|excluded|) mask building replaces O(V+E) iteration.</p> <p>Space complexity: O(V + E + I * R), where V and E are node and link counts, and R is result size per iteration. The pre-built graph is shared across all iterations.</p> <p>Parallelism: The C++ Core backend releases the GIL during computation, enabling true parallelism with Python threads. With graph caching, most per-iteration work runs in GIL-free C++ code; speedup depends on workload and parallelism level.</p>"},{"location":"reference/api-full/#analysisfunction","title":"AnalysisFunction <p>Protocol for analysis functions used with FailureManager.</p> <p>Analysis functions take a Network, exclusion sets, and analysis-specific parameters, returning results of any type.</p>","text":""},{"location":"reference/api-full/#failuremanager","title":"FailureManager <p>Failure analysis engine with Monte Carlo capabilities.</p> <p>This is the component for failure analysis in NetGraph. Provides parallel processing, worker caching, and failure policy handling for workflow steps and direct notebook usage.</p> <p>The FailureManager can execute any analysis function that takes a Network with exclusion sets and returns results, making it generic for different types of failure analysis (capacity, traffic, connectivity, etc.).</p> <p>Attributes:     network: The underlying network (not modified during analysis).     failure_policy_set: Set of named failure policies.     policy_name: Name of specific failure policy to use.</p> <p>Methods:</p> <ul> <li><code>compute_exclusions(self, policy: \"'FailurePolicy | None'\" = None, seed_offset: 'int | None' = None, failure_trace: 'Optional[Dict[str, Any]]' = None) -&gt; 'tuple[set[str], set[str]]'</code> - Compute set of nodes and links to exclude for a failure iteration.</li> <li><code>get_failure_policy(self) -&gt; \"'FailurePolicy | None'\"</code> - Get failure policy for analysis.</li> <li><code>run_demand_placement_monte_carlo(self, demands_config: 'list[dict[str, Any]] | Any', iterations: 'int' = 100, parallelism: 'int' = 1, placement_rounds: 'int | str' = 'auto', seed: 'int | None' = None, store_failure_patterns: 'bool' = False, include_flow_details: 'bool' = False, include_used_edges: 'bool' = False) -&gt; 'Any'</code> - Analyze traffic demand placement success under failures.</li> <li><code>run_max_flow_monte_carlo(self, source: 'str | dict[str, Any]', target: 'str | dict[str, Any]', mode: 'str' = 'combine', iterations: 'int' = 100, parallelism: 'int' = 1, shortest_path: 'bool' = False, require_capacity: 'bool' = True, flow_placement: 'FlowPlacement | str' = &lt;FlowPlacement.PROPORTIONAL: 1&gt;, seed: 'int | None' = None, store_failure_patterns: 'bool' = False, include_flow_summary: 'bool' = False, include_min_cut: 'bool' = False) -&gt; 'Any'</code> - Analyze maximum flow capacity envelopes between node groups under failures.</li> <li><code>run_monte_carlo_analysis(self, analysis_func: 'AnalysisFunction', iterations: 'int' = 1, parallelism: 'int' = 1, seed: 'int | None' = None, store_failure_patterns: 'bool' = False, **analysis_kwargs) -&gt; 'dict[str, Any]'</code> - Run Monte Carlo failure analysis with any analysis function.</li> <li><code>run_sensitivity_monte_carlo(self, source: 'str | dict[str, Any]', target: 'str | dict[str, Any]', mode: 'str' = 'combine', iterations: 'int' = 100, parallelism: 'int' = 1, shortest_path: 'bool' = False, flow_placement: 'FlowPlacement | str' = &lt;FlowPlacement.PROPORTIONAL: 1&gt;, seed: 'int | None' = None, store_failure_patterns: 'bool' = False) -&gt; 'dict[str, Any]'</code> - Analyze component criticality for flow capacity under failures.</li> <li><code>run_single_failure_scenario(self, analysis_func: 'AnalysisFunction', **kwargs) -&gt; 'Any'</code> - Run a single failure scenario for convenience.</li> </ul>","text":""},{"location":"reference/api-full/#ngraphanalysisfunctions","title":"ngraph.analysis.functions","text":"<p>Flow analysis functions for network evaluation.</p> <p>These functions are designed for use with FailureManager. Each analysis function takes a Network, exclusion sets, and analysis-specific parameters, returning results of type FlowIterationResult.</p> <p>Parameters should ideally be hashable for efficient caching in FailureManager; non-hashable objects are identified by memory address for cache key generation.</p> <p>Graph caching enables efficient repeated analysis with different exclusion sets by building the graph once and using O(|excluded|) masks for exclusions.</p> <p>SPF caching enables efficient demand placement by computing shortest paths once per unique source node rather than once per demand. For networks with many demands sharing the same sources, this can reduce SPF computations by an order of magnitude.</p>"},{"location":"reference/api-full/#build_demand_contextnetwork-network-demands_config-listdictstr-any-analysiscontext","title":"build_demand_context(network: \"'Network'\", demands_config: 'list[dict[str, Any]]') -&gt; 'AnalysisContext' <p>Build an AnalysisContext for repeated demand placement analysis.</p> <p>Pre-computes the graph with augmentations (pseudo source/target nodes) for efficient repeated analysis with different exclusion sets.</p> <p>Args:     network: Network instance.     demands_config: List of demand configurations (same format as demand_placement_analysis).</p> <p>Returns:     AnalysisContext ready for use with demand_placement_analysis.</p>","text":""},{"location":"reference/api-full/#build_maxflow_contextnetwork-network-source-str-dictstr-any-target-str-dictstr-any-mode-str-combine-analysiscontext","title":"build_maxflow_context(network: \"'Network'\", source: 'str | dict[str, Any]', target: 'str | dict[str, Any]', mode: 'str' = 'combine') -&gt; 'AnalysisContext' <p>Build an AnalysisContext for repeated max-flow analysis.</p> <p>Pre-computes the graph with pseudo source/target nodes for all source/target pairs, enabling O(|excluded|) mask building per iteration.</p> <p>Args:     network: Network instance.     source: Source node selector (string path or selector dict).     target: Target node selector (string path or selector dict).     mode: Flow analysis mode (\"combine\" or \"pairwise\").</p> <p>Returns:     AnalysisContext ready for use with max_flow_analysis or sensitivity_analysis.</p>","text":""},{"location":"reference/api-full/#demand_placement_analysisnetwork-network-excluded_nodes-setstr-excluded_links-setstr-demands_config-listdictstr-any-placement_rounds-int-str-auto-include_flow_details-bool-false-include_used_edges-bool-false-context-optionalanalysiscontext-none-flowiterationresult","title":"demand_placement_analysis(network: \"'Network'\", excluded_nodes: 'Set[str]', excluded_links: 'Set[str]', demands_config: 'list[dict[str, Any]]', placement_rounds: 'int | str' = 'auto', include_flow_details: 'bool' = False, include_used_edges: 'bool' = False, context: 'Optional[AnalysisContext]' = None) -&gt; 'FlowIterationResult' <p>Analyze traffic demand placement success rates using Core directly.</p> <p>This function:</p> <ol> <li>Builds Core infrastructure (graph, algorithms, flow_graph) or uses cached</li> <li>Expands demands into concrete (src, dst, volume) tuples</li> <li>Places each demand using SPF caching for cacheable policies</li> <li>Uses FlowPolicy for complex multi-flow policies</li> <li>Aggregates results into FlowIterationResult</li> </ol> <p>SPF Caching Optimization:     For cacheable policies (ECMP, WCMP, TE_WCMP_UNLIM), SPF results are     cached by source node. This reduces SPF computations from O(demands)     to O(unique_sources), typically a 5-10x reduction for workloads with     many demands sharing the same sources.</p> <p>Args:     network: Network instance.     excluded_nodes: Set of node names to exclude temporarily.     excluded_links: Set of link IDs to exclude temporarily.     demands_config: List of demand configurations (serializable dicts).     placement_rounds: Number of placement optimization rounds (unused - Core handles internally).     include_flow_details: When True, include cost_distribution per flow.     include_used_edges: When True, include set of used edges per demand in entry data.     context: Pre-built AnalysisContext for fast repeated analysis.</p> <p>Returns:     FlowIterationResult describing this iteration.</p>","text":""},{"location":"reference/api-full/#max_flow_analysisnetwork-network-excluded_nodes-setstr-excluded_links-setstr-source-str-dictstr-any-target-str-dictstr-any-mode-str-combine-shortest_path-bool-false-require_capacity-bool-true-flow_placement-flowplacement-include_flow_details-bool-false-include_min_cut-bool-false-context-optionalanalysiscontext-none-flowiterationresult","title":"max_flow_analysis(network: \"'Network'\", excluded_nodes: 'Set[str]', excluded_links: 'Set[str]', source: 'str | dict[str, Any]', target: 'str | dict[str, Any]', mode: 'str' = 'combine', shortest_path: 'bool' = False, require_capacity: 'bool' = True, flow_placement: 'FlowPlacement' = , include_flow_details: 'bool' = False, include_min_cut: 'bool' = False, context: 'Optional[AnalysisContext]' = None) -&gt; 'FlowIterationResult' <p>Analyze maximum flow capacity between node groups.</p> <p>Args:     network: Network instance.     excluded_nodes: Set of node names to exclude temporarily.     excluded_links: Set of link IDs to exclude temporarily.     source: Source node selector (string path or selector dict).     target: Target node selector (string path or selector dict).     mode: Flow analysis mode (\"combine\" or \"pairwise\").     shortest_path: Whether to use shortest paths only.     require_capacity: If True (default), path selection considers available         capacity. If False, path selection is cost-only (true IP/IGP semantics).     flow_placement: Flow placement strategy.     include_flow_details: Whether to collect cost distribution and similar details.     include_min_cut: Whether to include min-cut edge list in entry data.     context: Pre-built AnalysisContext for efficient repeated analysis.</p> <p>Returns:     FlowIterationResult describing this iteration.</p>","text":""},{"location":"reference/api-full/#sensitivity_analysisnetwork-network-excluded_nodes-setstr-excluded_links-setstr-source-str-dictstr-any-target-str-dictstr-any-mode-str-combine-shortest_path-bool-false-flow_placement-flowplacement-context-optionalanalysiscontext-none-flowiterationresult","title":"sensitivity_analysis(network: \"'Network'\", excluded_nodes: 'Set[str]', excluded_links: 'Set[str]', source: 'str | dict[str, Any]', target: 'str | dict[str, Any]', mode: 'str' = 'combine', shortest_path: 'bool' = False, flow_placement: 'FlowPlacement' = , context: 'Optional[AnalysisContext]' = None) -&gt; 'FlowIterationResult' <p>Analyze component sensitivity to failures.</p> <p>Identifies critical edges (saturated edges) and computes the flow reduction caused by removing each one. Returns a FlowIterationResult where each FlowEntry represents a source/target pair with:</p> <ul> <li>demand/placed = max flow value (the capacity being analyzed)</li> <li>dropped = 0.0 (baseline analysis, no failures applied)</li> <li>data[\"sensitivity\"] = {link_id:direction: flow_reduction} for critical edges</li> </ul> <p>Args:     network: Network instance.     excluded_nodes: Set of node names to exclude temporarily.     excluded_links: Set of link IDs to exclude temporarily.     source: Source node selector (string path or selector dict).     target: Target node selector (string path or selector dict).     mode: Flow analysis mode (\"combine\" or \"pairwise\").     shortest_path: If True, use single-tier shortest-path flow (IP/IGP mode).         Reports only edges used under ECMP routing. If False (default), use         full iterative max-flow (SDN/TE mode) and report all saturated edges.     flow_placement: Flow placement strategy.     context: Pre-built AnalysisContext for efficient repeated analysis.</p> <p>Returns:     FlowIterationResult with sensitivity data in each FlowEntry.data.</p>","text":""},{"location":"reference/api-full/#ngraphanalysisplacement","title":"ngraph.analysis.placement","text":"<p>Core demand placement with SPF caching.</p>"},{"location":"reference/api-full/#placemententry","title":"PlacementEntry <p>Single demand placement result.</p> <p>Attributes:</p> <ul> <li><code>src_name</code> (str)</li> <li><code>dst_name</code> (str)</li> <li><code>priority</code> (int)</li> <li><code>volume</code> (float)</li> <li><code>placed</code> (float)</li> <li><code>cost_distribution</code> (dict[float, float]) = {}</li> <li><code>used_edges</code> (set[str]) = set()</li> </ul>","text":""},{"location":"reference/api-full/#placementresult","title":"PlacementResult <p>Complete placement result.</p> <p>Attributes:</p> <ul> <li><code>summary</code> (PlacementSummary)</li> <li><code>entries</code> (list[PlacementEntry] | None)</li> </ul>","text":""},{"location":"reference/api-full/#placementsummary","title":"PlacementSummary <p>Aggregated placement totals.</p> <p>Attributes:</p> <ul> <li><code>total_demand</code> (float)</li> <li><code>total_placed</code> (float)</li> </ul>","text":""},{"location":"reference/api-full/#place_demandsdemands-sequenceexpandeddemand-volumes-sequencefloat-flow_graph-netgraph_coreflowgraph-ctx-analysiscontext-node_mask-npndarray-edge_mask-npndarray-resolved_ids-sequencetupleint-int-none-none-collect_entries-bool-false-include_cost_distribution-bool-false-include_used_edges-bool-false-placementresult","title":"place_demands(demands: \"Sequence['ExpandedDemand']\", volumes: 'Sequence[float]', flow_graph: 'netgraph_core.FlowGraph', ctx: \"'AnalysisContext'\", node_mask: 'np.ndarray', edge_mask: 'np.ndarray', *, resolved_ids: 'Sequence[tuple[int, int]] | None' = None, collect_entries: 'bool' = False, include_cost_distribution: 'bool' = False, include_used_edges: 'bool' = False) -&gt; 'PlacementResult' <p>Place demands on a flow graph with SPF caching.</p> <p>Args:     demands: Expanded demands (policy_preset, priority, names).     volumes: Demand volumes (allows scaling without modifying demands).     flow_graph: Target FlowGraph.     ctx: AnalysisContext with graph infrastructure.     node_mask: Node inclusion mask.     edge_mask: Edge inclusion mask.     resolved_ids: Pre-resolved (src_id, dst_id) pairs. Computed if None.     collect_entries: If True, populate result.entries.     include_cost_distribution: Include cost distribution in entries.     include_used_edges: Include used edges in entries.</p> <p>Returns:     PlacementResult with summary and optional entries.</p>","text":""},{"location":"reference/api-full/#ngraphlibnx","title":"ngraph.lib.nx","text":"<p>NetworkX graph conversion utilities.</p> <p>This module provides functions to convert between NetworkX graphs and the internal graph representation used by ngraph for high-performance algorithms.</p> <p>Example:     &gt;&gt;&gt; import networkx as nx     &gt;&gt;&gt; from ngraph.lib.nx import from_networkx, to_networkx     &gt;&gt;&gt;     &gt;&gt;&gt; # Create a NetworkX graph     &gt;&gt;&gt; G = nx.DiGraph()     &gt;&gt;&gt; G.add_edge(\"A\", \"B\", capacity=100.0, cost=10)     &gt;&gt;&gt; G.add_edge(\"B\", \"C\", capacity=50.0, cost=5)     &gt;&gt;&gt;     &gt;&gt;&gt; # Convert to ngraph format for analysis     &gt;&gt;&gt; graph, node_map, edge_map = from_networkx(G)     &gt;&gt;&gt;     &gt;&gt;&gt; # Use with ngraph algorithms...     &gt;&gt;&gt;     &gt;&gt;&gt; # Convert back to NetworkX     &gt;&gt;&gt; G_out = to_networkx(graph, node_map)</p>"},{"location":"reference/api-full/#edgemap","title":"EdgeMap <p>Bidirectional mapping between internal edge IDs and original edge references.</p> <p>When converting a NetworkX graph, each edge is assigned an internal integer ID (ext_edge_id). This class preserves the mapping for interpreting algorithm results and updating the original graph.</p> <p>Attributes:     to_ref: Maps internal edge ID to original (source, target, key) tuple     from_ref: Maps original (source, target, key) to list of internal edge IDs         (list because bidirectional=True creates two IDs per edge)</p> <p>Example:     &gt;&gt;&gt; graph, node_map, edge_map = from_networkx(G)     &gt;&gt;&gt; # After running algorithms, map flow results back to original edges     &gt;&gt;&gt; for ext_id, flow in enumerate(flow_state.edge_flow_view()):     ...     if flow &gt; 0:     ...         u, v, key = edge_map.to_ref[ext_id]     ...         G.edges[u, v, key][\"flow\"] = flow</p> <p>Attributes:</p> <ul> <li><code>to_ref</code> (Dict[int, NxEdgeTuple]) = {}</li> <li><code>from_ref</code> (Dict[NxEdgeTuple, List[int]]) = {}</li> </ul>","text":""},{"location":"reference/api-full/#nodemap","title":"NodeMap <p>Bidirectional mapping between node names and integer indices.</p> <p>When converting a NetworkX graph to the internal representation, node names (which can be any hashable type) are mapped to contiguous integer indices starting from 0. This class preserves the mapping for result interpretation and back-conversion.</p> <p>Attributes:     to_index: Maps original node names to integer indices     to_name: Maps integer indices back to original node names</p> <p>Example:     &gt;&gt;&gt; node_map = NodeMap.from_names([\"A\", \"B\", \"C\"])     &gt;&gt;&gt; node_map.to_index[\"A\"]     0     &gt;&gt;&gt; node_map.to_name[1]     'B'</p> <p>Attributes:</p> <ul> <li><code>to_index</code> (Dict[Hashable, int]) = {}</li> <li><code>to_name</code> (Dict[int, Hashable]) = {}</li> </ul> <p>Methods:</p> <ul> <li><code>from_names(names: 'List[Hashable]') -&gt; \"'NodeMap'\"</code> - Create a NodeMap from a list of node names.</li> </ul>","text":""},{"location":"reference/api-full/#from_networkxg-nxgraph-capacity_attr-str-capacity-cost_attr-str-cost-default_capacity-float-10-default_cost-int-1-bidirectional-bool-false-tuplenetgraph_corestrictmultidigraph-nodemap-edgemap","title":"from_networkx(G: 'NxGraph', *, capacity_attr: 'str' = 'capacity', cost_attr: 'str' = 'cost', default_capacity: 'float' = 1.0, default_cost: 'int' = 1, bidirectional: 'bool' = False) -&gt; 'Tuple[netgraph_core.StrictMultiDiGraph, NodeMap, EdgeMap]' <p>Convert a NetworkX graph to ngraph's internal graph format.</p> <p>Converts any NetworkX graph (DiGraph, MultiDiGraph, Graph, MultiGraph) to netgraph_core.StrictMultiDiGraph. Node names are mapped to integer indices; the returned NodeMap and EdgeMap preserve mappings for result interpretation.</p> <p>Args:     G: NetworkX graph (DiGraph, MultiDiGraph, Graph, or MultiGraph)     capacity_attr: Edge attribute name for capacity (default: \"capacity\")     cost_attr: Edge attribute name for cost (default: \"cost\")     default_capacity: Capacity value when attribute is missing (default: 1.0)     default_cost: Cost value when attribute is missing (default: 1)     bidirectional: If True, add reverse edge for each edge. Useful for         undirected connectivity analysis. (default: False)</p> <p>Returns:     Tuple of (graph, node_map, edge_map) where:</p> <ul> <li>graph: netgraph_core.StrictMultiDiGraph ready for algorithms</li> <li>node_map: NodeMap for converting node indices back to names</li> <li>edge_map: EdgeMap for converting edge IDs back to (u, v, key) refs</li> </ul> <p>Raises:     TypeError: If G is not a NetworkX graph     ValueError: If graph has no nodes</p> <p>Example:     &gt;&gt;&gt; import networkx as nx     &gt;&gt;&gt; G = nx.DiGraph()     &gt;&gt;&gt; G.add_edge(\"src\", \"dst\", capacity=100.0, cost=10)     &gt;&gt;&gt; graph, node_map, edge_map = from_networkx(G)     &gt;&gt;&gt; graph.num_nodes()     2     &gt;&gt;&gt; node_map.to_index[\"src\"]     0     &gt;&gt;&gt; edge_map.to_ref[0]  # First edge     ('dst', 'src', 0)  # sorted node order: dst &lt; src</p>","text":""},{"location":"reference/api-full/#to_networkxgraph-netgraph_corestrictmultidigraph-node_map-optionalnodemap-none-capacity_attr-str-capacity-cost_attr-str-cost-nxmultidigraph","title":"to_networkx(graph: 'netgraph_core.StrictMultiDiGraph', node_map: 'Optional[NodeMap]' = None, *, capacity_attr: 'str' = 'capacity', cost_attr: 'str' = 'cost') -&gt; \"'nx.MultiDiGraph'\" <p>Convert ngraph's internal graph format back to NetworkX MultiDiGraph.</p> <p>Reconstructs a NetworkX graph from the internal representation. If a NodeMap is provided, original node names are restored; otherwise, nodes are labeled with integer indices.</p> <p>Args:     graph: netgraph_core.StrictMultiDiGraph to convert     node_map: Optional NodeMap to restore original node names.         If None, nodes are labeled 0, 1, 2, ...     capacity_attr: Edge attribute name for capacity (default: \"capacity\")     cost_attr: Edge attribute name for cost (default: \"cost\")</p> <p>Returns:     nx.MultiDiGraph with edges and attributes from the internal graph</p> <p>Example:     &gt;&gt;&gt; graph, node_map, edge_map = from_networkx(G)     &gt;&gt;&gt; # ... run algorithms ...     &gt;&gt;&gt; G_out = to_networkx(graph, node_map)     &gt;&gt;&gt; list(G_out.nodes())     ['A', 'B', 'C']</p>","text":""},{"location":"reference/api-full/#error-handling","title":"Error Handling","text":"<p>NetGraph uses standard Python exceptions:</p> <ul> <li><code>ValueError</code> - For validation errors</li> <li><code>KeyError</code> - For missing required fields</li> <li><code>RuntimeError</code> - For runtime errors</li> </ul> <p>For complete method signatures and detailed documentation, use Python's help system:</p> <pre><code>help(ngraph.scenario.Scenario)\nhelp(ngraph.network.Network.max_flow)\n</code></pre> <p>This documentation was auto-generated from the NetGraph source code.</p>"},{"location":"reference/api/","title":"API Reference","text":"<p>Quick links:</p> <ul> <li>Design -- architecture, model, algorithms, workflow</li> <li>DSL Reference -- YAML syntax for scenario definition</li> <li>Workflow Reference -- analysis workflow configuration and execution</li> <li>CLI Reference -- command-line tools for running scenarios</li> <li>Auto-Generated API Reference -- complete class and method documentation</li> </ul> <p>This section provides a curated guide to NetGraph's Python API, organized by typical usage patterns.</p>"},{"location":"reference/api/#1-programmatic-quickstart","title":"1. Programmatic Quickstart","text":"<p>Minimal, copy-pastable start: build a tiny network, run max-flow, and reuse a bound context.</p> <pre><code>from ngraph import Network, Node, Link, analyze, Mode\n\n# Build a small directed network\nnet = Network()\nnet.add_node(Node(name=\"A\"))\nnet.add_node(Node(name=\"B\"))\nnet.add_node(Node(name=\"C\"))\nnet.add_link(Link(source=\"A\", target=\"B\", capacity=10.0, cost=1.0))\nnet.add_link(Link(source=\"B\", target=\"C\", capacity=5.0, cost=1.0))\n\n# One-off max-flow (unbound context)\nflow = analyze(net).max_flow(\"^A$\", \"^C$\", mode=Mode.COMBINE)\nprint(flow)  # {('^A$', '^C$'): 5.0}\n\n# Detailed flow with cost distribution\ndetailed = analyze(net).max_flow_detailed(\"^A$\", \"^C$\", mode=Mode.COMBINE)\n(_, _), summary = next(iter(detailed.items()))\nprint(summary.total_flow, summary.cost_distribution)\n\n# Bound context for repeated runs with exclusions\nctx = analyze(net, source=\"^A$\", sink=\"^C$\", mode=Mode.COMBINE)\nbaseline = ctx.max_flow()\n# Exclude first link by getting its ID from the network\nfirst_link_id = next(iter(net.links.keys()))\ndegraded = ctx.max_flow(excluded_links={first_link_id})\nprint(\"baseline\", baseline, \"degraded\", degraded)\n</code></pre>"},{"location":"reference/api/#2-fundamentals","title":"2. Fundamentals","text":"<p>The core components that form the foundation of most NetGraph programs.</p>"},{"location":"reference/api/#scenario","title":"Scenario","text":"<p>Purpose: Coordinates network topology, workflow execution, and result storage for complete analysis pipelines.</p> <p>When to use: Entry point for analysis workflows - load from YAML for declarative scenarios or construct programmatically for direct API access.</p> <pre><code>from pathlib import Path\nfrom ngraph import Scenario\n\n# Load complete scenario from YAML text\nyaml_text = Path(\"scenarios/square_mesh.yaml\").read_text()\nscenario = Scenario.from_yaml(yaml_text)\n\n# Execute the scenario\nscenario.run()\n\n# Export results\nexported = scenario.results.to_dict()\nprint(exported[\"workflow\"].keys())\n</code></pre> <p>Key Methods:</p> <ul> <li><code>from_yaml(yaml_str, default_components=None)</code> - Parse scenario from YAML string (use <code>Path.read_text()</code> for file loading)</li> <li><code>run()</code> - Execute workflow steps in sequence</li> </ul> <p>Integration: Scenario coordinates Network topology, workflow execution, and Results collection. Components can also be used independently for direct programmatic access.</p>"},{"location":"reference/api/#network","title":"Network","text":"<p>Purpose: Represents network topology.</p> <p>When to use: Core component for representing network structure. Used directly for programmatic topology creation or accessed via <code>scenario.network</code>.</p> <pre><code>from ngraph import Network, Node, Link, analyze\n\n# Create a tiny network\nnetwork = Network()\nnetwork.add_node(Node(name=\"n1\", risk_groups={\"rack1\"}))\nnetwork.add_node(Node(name=\"n2\", risk_groups={\"rack2\"}))\nnetwork.add_link(Link(source=\"n1\", target=\"n2\", capacity=100.0, risk_groups={\"fiber_bundle_A\"}))\n\n# Calculate maximum flow using analyze()\nflow_result = analyze(network).max_flow(\"^n1$\", \"^n2$\")\nprint(flow_result)  # {(\"^n1$\", \"^n2$\"): 100.0}\n</code></pre> <p>Key Methods:</p> <ul> <li><code>add_node(node)</code>, <code>add_link(link)</code> - Build topology programmatically</li> <li><code>nodes</code>, <code>links</code> - Access topology as dictionaries</li> </ul> <p>Key Concepts:</p> <ul> <li>disabled flags: Node.disabled and Link.disabled mark components as inactive in the scenario topology (use <code>excluded_nodes</code>/<code>excluded_links</code> parameters for temporary analysis-time exclusion)</li> <li>Risk Groups: Nodes and links can be tagged with risk group names (e.g., \"rack1\", \"fiber_bundle\") to model shared failure domains.</li> <li>Node selection: Use regex patterns anchored at start (e.g., <code>\"^datacenter.*\"</code>) or selector objects with <code>path</code>, <code>group_by</code>, and <code>match</code> fields to select and group nodes (see DSL Reference)</li> </ul>"},{"location":"reference/api/#results","title":"Results","text":"<p>Purpose: Centralized container for storing and retrieving analysis results from workflow steps.</p> <p>When to use: Managed by Scenario; stores workflow step outputs with metadata. Access via <code>scenario.results</code> for result retrieval and custom step implementation.</p> <pre><code># Access results from scenario\nresults = scenario.results\n\n# Export all results for serialization\nall_data = results.to_dict()\nprint(list(all_data[\"steps\"].keys()))\n</code></pre> <p>Key Methods:</p> <ul> <li><code>enter_step(step_name)</code> / <code>exit_step()</code> - Scope writes to a step (managed by WorkflowStep.execute())</li> <li><code>put(key, value)</code> - Store value under active step; key must be <code>\"metadata\"</code> or <code>\"data\"</code></li> <li><code>get(key, default=None)</code> - Retrieve value from active step scope</li> <li><code>get_step(step_name)</code> - Retrieve complete step dict for cross-step reads</li> <li><code>to_dict()</code> - Export results with shape <code>{workflow, steps, scenario}</code> (JSON-serializable)</li> </ul> <p>Integration: Used by all workflow steps for result storage. Provides consistent access pattern for analysis outputs.</p>"},{"location":"reference/api/#3-networkx-integration","title":"3. NetworkX Integration","text":"<p>Convert between NetworkX graphs and the internal graph format for algorithm execution.</p>"},{"location":"reference/api/#converting-from-networkx","title":"Converting from NetworkX","text":"<pre><code>import networkx as nx\nfrom ngraph import from_networkx, to_networkx\nimport netgraph_core\n\n# Create or load a NetworkX graph\nG = nx.DiGraph()\nG.add_edge(\"A\", \"B\", capacity=100.0, cost=10)\nG.add_edge(\"B\", \"C\", capacity=50.0, cost=5)\nG.add_edge(\"A\", \"C\", capacity=30.0, cost=25)\n\n# Convert to internal format\ngraph, node_map, edge_map = from_networkx(G)\n\n# Use with Core algorithms\nbackend = netgraph_core.Backend.cpu()\nalgorithms = netgraph_core.Algorithms(backend)\nhandle = algorithms.build_graph(graph)\n\n# Run shortest path\nsrc_idx = node_map.to_index[\"A\"]\ndst_idx = node_map.to_index[\"C\"]\ndists, _ = algorithms.spf(handle, src=src_idx, dst=dst_idx)\nprint(f\"Shortest path cost A-&gt;C: {dists[dst_idx]}\")  # 15 (via B)\n</code></pre> <p>Key Functions:</p> <ul> <li><code>from_networkx(G, *, capacity_attr, cost_attr, default_capacity, default_cost, bidirectional)</code> - Convert NetworkX graph to internal format</li> <li><code>to_networkx(graph, node_map, *, capacity_attr, cost_attr)</code> - Convert back to NetworkX MultiDiGraph</li> </ul> <p>Mapping Classes:</p> <ul> <li><code>NodeMap</code> - Bidirectional mapping between node names and integer indices</li> <li><code>to_index[name]</code> - Get integer index for node name</li> <li><code>to_name[idx]</code> - Get node name for integer index</li> <li><code>EdgeMap</code> - Bidirectional mapping between edge IDs and original edge references</li> <li><code>to_ref[edge_id]</code> - Get (source, target, key) tuple for edge ID</li> <li><code>from_ref[(u, v, key)]</code> - Get list of edge IDs for original edge</li> </ul> <p>Options:</p> <ul> <li><code>bidirectional=True</code> - Add reverse edge for each edge (for undirected analysis)</li> <li><code>capacity_attr</code> / <code>cost_attr</code> - Custom attribute names for capacity and cost</li> <li><code>default_capacity</code> / <code>default_cost</code> - Default values when attributes missing</li> </ul>"},{"location":"reference/api/#writing-results-back","title":"Writing Results Back","text":"<pre><code># After algorithm execution, map results back to original graph\nflow_state = netgraph_core.FlowState(graph)\n# ... place flow ...\n\n# Use edge_map to update original NetworkX graph\nedge_flows = flow_state.edge_flow_view()\nfor edge_id, flow in enumerate(edge_flows):\n    if flow &gt; 0:\n        u, v, key = edge_map.to_ref[edge_id]\n        G.edges[u, v, key][\"flow\"] = float(flow)\n</code></pre>"},{"location":"reference/api/#4-basic-analysis","title":"4. Basic Analysis","text":"<p>Essential analysis capabilities for network evaluation.</p>"},{"location":"reference/api/#flow-analysis-with-analyze","title":"Flow Analysis with <code>analyze()</code>","text":"<p>Purpose: Calculate network flows between source and sink groups with various policies and constraints.</p> <p>When to use: Compute network capacity between source and sink groups. Supports multiple flow placement policies and failure scenarios.</p> <p>Performance: Max-flow computation executes in C++ with the GIL released for concurrent execution. Algorithm uses successive shortest paths with blocking flow augmentation; complexity is O(V^2 E log V) worst-case.</p> <pre><code>from ngraph import analyze, Mode, FlowPlacement\n\n# Maximum flow between group patterns (combine all sources/sinks)\nflow_result = analyze(network).max_flow(\n    \"^metro1/.*\",\n    \"^metro5/.*\",\n    mode=Mode.COMBINE\n)\n\n# Detailed flow analysis with cost distribution\nresult = analyze(network).max_flow_detailed(\n    \"^metro1/.*\",\n    \"^metro5/.*\",\n    mode=Mode.COMBINE\n)\n(src_label, sink_label), summary = next(iter(result.items()))\nprint(summary.cost_distribution)  # Dict[float, float] mapping cost to flow volume\n</code></pre> <p>Key Functions:</p> <ul> <li><code>analyze(network, *, source=None, sink=None, mode=Mode.COMBINE)</code> - Create analysis context</li> <li><code>ctx.max_flow(source, sink, *, mode, shortest_path, require_capacity, flow_placement, excluded_nodes, excluded_links)</code> - Maximum flow</li> <li><code>ctx.max_flow_detailed(..., include_min_cut=False)</code> - Maximum flow with cost distribution and optional min-cut</li> <li><code>ctx.sensitivity(...)</code> - Identify critical edges and their impact on flow</li> <li><code>ctx.shortest_path_cost(source, sink, *, mode, edge_select=ALL_MIN_COST, excluded_nodes, excluded_links)</code> - Shortest path cost</li> <li><code>ctx.shortest_paths(source, sink, *, mode, edge_select, split_parallel_edges)</code> - Full Path objects</li> </ul> <p>Key Concepts:</p> <ul> <li>Mode.COMBINE: Aggregate sources into one super-source, sinks into one super-sink; returns single total flow</li> <li>Mode.PAIRWISE: Compute flow for each (source_group, sink_group) pair independently</li> <li>FlowPlacement.PROPORTIONAL (WCMP): Split flow proportional to edge capacity</li> <li>FlowPlacement.EQUAL_BALANCED (ECMP): Equal split across parallel paths</li> <li>shortest_path=True: Restricts flow to lowest-cost paths only (IP/IGP routing semantics)</li> <li>shortest_path=False: Uses all paths progressively (TE/SDN semantics)</li> <li>require_capacity=True: Flow cannot exceed link capacity (default)</li> <li>require_capacity=False: Unconstrained flow for capacity-free analysis</li> </ul>"},{"location":"reference/api/#efficient-repeated-analysis-bound-context","title":"Efficient Repeated Analysis (Bound Context)","text":"<p>For efficient repeated analysis with the same source/sink groups:</p> <pre><code>from ngraph import analyze, Mode\n\n# Create bound context - graph built once with pseudo-nodes\nctx = analyze(network, source=\"^dc/\", sink=\"^edge/\", mode=Mode.COMBINE)\n\n# Baseline capacity\nbaseline = ctx.max_flow()\n\n# Test with different failures - only mask building per call\nfor failed_links in failure_scenarios:\n    degraded = ctx.max_flow(excluded_links=failed_links)\n    print(f\"Capacity with {failed_links}: {degraded}\")\n</code></pre> <p>Benefits of Bound Context:</p> <ul> <li>Graph infrastructure built once at context creation</li> <li>Each analysis call only builds O(|excluded|) masks</li> <li>Thread-safe: can run concurrent analysis calls with different exclusions</li> </ul>"},{"location":"reference/api/#shortest-paths","title":"Shortest Paths","text":"<pre><code>from ngraph import analyze, Mode, EdgeSelect\n\n# Get shortest path cost between groups\ncosts = analyze(network).shortest_path_cost(\n    \"^dc1/.*\",\n    \"^dc2/.*\",\n    mode=Mode.PAIRWISE\n)\n\n# Get full path objects\npaths = analyze(network).shortest_paths(\n    \"^A$\",\n    \"^B$\",\n    mode=Mode.COMBINE,\n    edge_select=EdgeSelect.ALL_MIN_COST\n)\n\n# K-shortest paths with constraints\nk_paths = analyze(network).k_shortest_paths(\n    \"^A$\",\n    \"^B$\",\n    max_k=5,\n    mode=Mode.PAIRWISE,\n    max_path_cost_factor=1.5  # Limit to 1.5x best path cost\n)\n</code></pre> <p>Key Functions:</p> <ul> <li><code>ctx.shortest_path_cost(source, sink, *, mode, edge_select=ALL_MIN_COST)</code> - Cost only, no path objects</li> <li><code>ctx.shortest_paths(source, sink, *, mode, edge_select=ALL_MIN_COST, split_parallel_edges=False)</code> - Full Path objects</li> <li><code>ctx.k_shortest_paths(source, sink, *, mode=PAIRWISE, max_k=3, max_path_cost, max_path_cost_factor, excluded_nodes, excluded_links)</code> - Multiple paths per pair</li> </ul>"},{"location":"reference/api/#sensitivity-analysis","title":"Sensitivity Analysis","text":"<p>Identify critical edges and quantify their impact:</p> <pre><code>from ngraph import analyze, Mode\n\n# Get sensitivity map: which edges are critical and by how much\nsensitivity = analyze(network).sensitivity(\n    \"^metro1/.*\",\n    \"^metro5/.*\",\n    mode=Mode.COMBINE,\n    shortest_path=False  # Full max-flow mode\n)\n\nfor pair, edge_impacts in sensitivity.items():\n    print(f\"Critical edges for {pair}:\")\n    for edge_key, flow_reduction in edge_impacts.items():\n        print(f\"  {edge_key}: -{flow_reduction:.2f}\")\n</code></pre>"},{"location":"reference/api/#5-monte-carlo-analysis","title":"5. Monte Carlo Analysis","text":"<p>Probabilistic failure analysis using FailureManager.</p>"},{"location":"reference/api/#failuremanager","title":"FailureManager","text":"<p>Purpose: Execute Monte Carlo failure scenarios and aggregate results across multiple iterations.</p> <pre><code>from ngraph import Network, Node, Link, FailureManager\nfrom ngraph.model.failure.policy import FailurePolicy, FailureMode, FailureRule\nfrom ngraph.model.failure.policy_set import FailurePolicySet\n\n# Build a simple network\nnetwork = Network()\nfor name in [\"A\", \"B\", \"C\"]:\n    network.add_node(Node(name=name))\nnetwork.add_link(Link(\"A\", \"B\", capacity=100.0))\nnetwork.add_link(Link(\"B\", \"C\", capacity=100.0))\n\n# Define failure policy: randomly choose 1 link to fail\nrule = FailureRule(scope=\"link\", mode=\"choice\", count=1)  # scope can be \"node\", \"link\", or \"risk_group\"\nmode = FailureMode(weight=1.0, rules=[rule])\npolicy = FailurePolicy(modes=[mode])\npolicy_set = FailurePolicySet(policies={\"single_link\": policy})\n\n# Create failure manager\nfm = FailureManager(\n    network=network,\n    failure_policy_set=policy_set,\n    policy_name=\"single_link\"\n)\n\n# Run max-flow Monte Carlo analysis\nresults = fm.run_max_flow_monte_carlo(\n    source=\"^A$\",\n    target=\"^C$\",\n    mode=\"combine\",\n    iterations=100,\n    parallelism=1,\n    seed=42  # For reproducibility\n)\n\n# Access results\nfor iter_result in results[\"results\"]:\n    print(f\"Flow: {iter_result.summary.total_placed:.1f}\")\n</code></pre> <p>Key Methods:</p> <ul> <li><code>run_max_flow_monte_carlo(...)</code> - Max-flow capacity analysis under failures</li> <li><code>run_demand_placement_monte_carlo(...)</code> - Traffic demand placement under failures</li> <li><code>run_monte_carlo_analysis(analysis_func, ...)</code> - Generic Monte Carlo with custom function</li> </ul>"},{"location":"reference/api/#6-workflow-steps","title":"6. Workflow Steps","text":"<p>Pre-built analysis steps for YAML-driven workflows.</p>"},{"location":"reference/api/#maxflow-step","title":"MaxFlow Step","text":"<pre><code>workflow:\n  - type: MaxFlow\n    name: \"dc_to_edge_capacity\"\n    source: \"^datacenter/.*\"\n    target: \"^edge/.*\"\n    mode: \"combine\"\n    failure_policy: \"random_link_failures\"\n    iterations: 100\n    parallelism: auto\n    shortest_path: false\n    require_capacity: true       # false for true IP/IGP semantics\n    flow_placement: \"PROPORTIONAL\"\n</code></pre>"},{"location":"reference/api/#trafficmatrixplacement-step","title":"TrafficMatrixPlacement Step","text":"<pre><code>workflow:\n  - type: TrafficMatrixPlacement\n    name: \"tm_placement_analysis\"\n    demand_set: \"peak_traffic\"\n    failure_policy: \"dual_link_failures\"\n    iterations: 100\n    parallelism: auto\n    placement_rounds: auto\n</code></pre>"},{"location":"reference/api/#maximumsupporteddemand-step","title":"MaximumSupportedDemand Step","text":"<pre><code>workflow:\n  - type: MaximumSupportedDemand\n    name: \"find_alpha_star\"\n    demand_set: \"peak_traffic\"\n    alpha_start: 1.0\n    growth_factor: 2.0\n    resolution: 0.01\n</code></pre>"},{"location":"reference/api/#networkstats-step","title":"NetworkStats Step","text":"<pre><code>workflow:\n  - type: NetworkStats\n    name: \"baseline_stats\"\n    include_disabled: false\n    excluded_nodes: [\"n1\"]\n</code></pre>"},{"location":"reference/api/#costpower-step","title":"CostPower Step","text":"<pre><code>workflow:\n  - type: CostPower\n    name: \"cost_power_analysis\"\n    include_disabled: false\n    aggregation_level: 2\n</code></pre>"},{"location":"reference/api/#7-types-reference","title":"7. Types Reference","text":""},{"location":"reference/api/#enums","title":"Enums","text":"<pre><code>from ngraph import Mode, FlowPlacement, EdgeSelect\n\n# Mode - Source/sink group handling\nMode.COMBINE    # Aggregate all sources/sinks\nMode.PAIRWISE   # Each (src_group, sink_group) pair independently\n\n# FlowPlacement - Flow distribution strategy\nFlowPlacement.PROPORTIONAL   # WCMP: proportional to capacity\nFlowPlacement.EQUAL_BALANCED # ECMP: equal split\n\n# EdgeSelect - SPF edge selection\nEdgeSelect.ALL_MIN_COST     # All equal-cost edges (ECMP)\nEdgeSelect.SINGLE_MIN_COST  # Single lowest-cost edge\n</code></pre>"},{"location":"reference/api/#result-types","title":"Result Types","text":"<pre><code>from ngraph import MaxFlowResult, FlowEntry, FlowSummary, FlowIterationResult\n\n# MaxFlowResult - Detailed max-flow result\nresult.total_flow        # Total flow placed\nresult.cost_distribution # Dict[cost, flow_volume]\nresult.min_cut           # Optional tuple of EdgeRef (saturated edges)\n\n# FlowEntry - Single flow entry\nentry.source        # Source label\nentry.destination   # Destination label\nentry.demand        # Requested demand\nentry.placed        # Actually placed\nentry.dropped       # Unmet demand\n\n# FlowSummary - Aggregated statistics\nsummary.total_demand    # Sum of all demands\nsummary.total_placed    # Sum of placed flows\nsummary.overall_ratio   # placed / demand\n\n# FlowIterationResult - Full iteration result\niter_result.flows    # List[FlowEntry]\niter_result.summary  # FlowSummary\n</code></pre>"},{"location":"reference/api/#8-complete-example","title":"8. Complete Example","text":"<pre><code>from ngraph import Network, Node, Link, analyze, Mode\n\n# Build network\nnetwork = Network()\nfor name in [\"dc1\", \"dc2\", \"spine1\", \"spine2\", \"leaf1\", \"leaf2\"]:\n    network.add_node(Node(name))\n\n# Add links with varying capacities\nnetwork.add_link(Link(\"dc1\", \"spine1\", capacity=100.0, cost=1.0))\nnetwork.add_link(Link(\"dc1\", \"spine2\", capacity=100.0, cost=1.0))\nnetwork.add_link(Link(\"dc2\", \"spine1\", capacity=100.0, cost=1.0))\nnetwork.add_link(Link(\"dc2\", \"spine2\", capacity=100.0, cost=1.0))\nnetwork.add_link(Link(\"spine1\", \"leaf1\", capacity=50.0, cost=1.0))\nnetwork.add_link(Link(\"spine1\", \"leaf2\", capacity=50.0, cost=1.0))\nnetwork.add_link(Link(\"spine2\", \"leaf1\", capacity=50.0, cost=1.0))\nnetwork.add_link(Link(\"spine2\", \"leaf2\", capacity=50.0, cost=1.0))\n\n# One-off max flow analysis\nflow = analyze(network).max_flow(\"^dc\", \"^leaf\", mode=Mode.COMBINE)\nprint(f\"DC to Leaf capacity: {list(flow.values())[0]:.1f}\")\n\n# Efficient repeated analysis with bound context\nctx = analyze(network, source=\"^dc\", sink=\"^leaf\", mode=Mode.COMBINE)\n\n# Baseline\nbaseline = ctx.max_flow()\n\n# Test spine failures\nspine_links = [lid for lid, l in network.links.items() if \"spine\" in l.source]\nfor link_id in spine_links:\n    degraded = ctx.max_flow(excluded_links={link_id})\n    reduction = list(baseline.values())[0] - list(degraded.values())[0]\n    print(f\"If {link_id} fails: {reduction:.1f} capacity loss\")\n\n# Sensitivity analysis\nsensitivity = ctx.sensitivity()\nfor pair, impacts in sensitivity.items():\n    print(f\"\\nCritical edges for {pair}:\")\n    for edge, impact in sorted(impacts.items(), key=lambda x: -x[1])[:3]:\n        print(f\"  {edge}: {impact:.1f}\")\n</code></pre>"},{"location":"reference/api/#9-performance-notes","title":"9. Performance Notes","text":"<p>NetGraph uses a hybrid Python+C++ architecture:</p> <ul> <li>High-level APIs (Network, Scenario, Workflow) are pure Python</li> <li>Core algorithms (shortest paths, max-flow, K-shortest paths) execute in optimized C++ via NetGraph-Core</li> <li>GIL released during algorithm execution for parallel processing</li> <li>Transparent integration: You work with Python objects; Core acceleration is automatic</li> </ul> <p>All public APIs accept and return Python types (Network, Node, Link, FlowSummary, etc.). The C++ layer is an implementation detail you generally don't interact with directly.</p>"},{"location":"reference/cli/","title":"Command Line Interface","text":"<p>Quick links:</p> <ul> <li>Design \u2014 architecture, model, algorithms, workflow</li> <li>DSL Reference \u2014 YAML syntax for scenario definition</li> <li>Workflow Reference \u2014 analysis workflow configuration and execution</li> <li>API Reference \u2014 Python API for programmatic scenario creation</li> <li>Auto-Generated API Reference \u2014 complete class and method documentation</li> </ul> <p>NetGraph provides a command-line interface for inspecting, running, and analyzing scenarios from the terminal.</p>"},{"location":"reference/cli/#basic-usage","title":"Basic Usage","text":"<p>The CLI provides two primary commands:</p> <ul> <li><code>inspect</code>: Analyze and validate scenario files without running them</li> <li><code>run</code>: Execute scenario files and generate results</li> </ul> <p>Global options (must be placed before the command):</p> <ul> <li><code>--verbose</code>, <code>-v</code>: Enable debug logging</li> <li><code>--quiet</code>: Suppress console output (logs only)</li> </ul>"},{"location":"reference/cli/#quick-start","title":"Quick Start","text":"<pre><code># Inspect a provided scenario\nngraph inspect scenarios/square_mesh.yaml\n\n# Run a scenario (creates square_mesh.results.json by default)\nngraph run scenarios/square_mesh.yaml\n</code></pre>"},{"location":"reference/cli/#command-reference","title":"Command Reference","text":""},{"location":"reference/cli/#inspect","title":"<code>inspect</code>","text":"<p>Analyze and validate a NetGraph scenario file without executing it.</p> <p>Syntax:</p> <pre><code>ngraph [--verbose|--quiet] inspect &lt;scenario_file&gt; [options]\n</code></pre> <p>Arguments:</p> <ul> <li><code>scenario_file</code>: Path to the YAML scenario file to inspect</li> </ul> <p>Options:</p> <ul> <li><code>--detail</code>, <code>-d</code>: Show detailed information including complete node/link tables and step parameters</li> <li><code>--output</code>, <code>-o</code>: Output directory for generated artifacts (e.g., profiles)</li> </ul> <p>What it does:</p> <p>The <code>inspect</code> command loads and validates a scenario file, then provides information about:</p> <ul> <li>Scenario metadata: Seed configuration and deterministic behavior</li> <li>Network structure: Node/link counts, enabled/disabled breakdown, hierarchy analysis</li> <li>Capacity statistics: Link and node capacity analysis with min/max/mean/total values</li> <li>Risk groups: Network resilience groupings and their status</li> <li>Components library: Available components for network modeling</li> <li>Failure policies: Configured failure scenarios and their rules</li> <li>Traffic matrices: Demand patterns and traffic flows</li> <li>Workflow steps: Analysis pipeline and step-by-step execution plan</li> </ul> <p>In detail mode (<code>--detail</code>), shows complete tables for all nodes and links with capacity and connectivity information.</p> <p>Examples:</p> <pre><code># Basic inspection\nngraph inspect scenarios/backbone_clos.yml\n\n# Detailed inspection with complete node/link tables and step parameters\nngraph inspect scenarios/nsfnet.yaml --detail\n\n# Inspect with verbose logging (note: global option placement)\nngraph --verbose inspect scenarios/square_mesh.yaml\n</code></pre> <p>Use cases:</p> <ul> <li>Scenario validation: Verify YAML syntax and structure</li> <li>Network debugging: Analyze blueprint expansion and node/link creation</li> <li>Capacity analysis: Review network capacity distribution and connectivity</li> <li>Workflow preview: Examine analysis steps before execution</li> </ul>"},{"location":"reference/cli/#run","title":"<code>run</code>","text":"<p>Execute a NetGraph scenario file.</p> <p>Syntax:</p> <pre><code>ngraph [--verbose|--quiet] run &lt;scenario_file&gt; [options]\n</code></pre> <p>Arguments:</p> <ul> <li><code>scenario_file</code>: Path to the YAML scenario file to execute</li> </ul> <p>Options:</p> <ul> <li><code>--results</code>, <code>-r</code>: Path to export results as JSON (default: <code>&lt;scenario_name&gt;.results.json</code>)</li> <li><code>--no-results</code>: Disable results file generation</li> <li><code>--stdout</code>: Print results to stdout in addition to saving file</li> <li><code>--keys</code>, <code>-k</code>: Space-separated list of workflow step names to include in output</li> <li><code>--profile</code>: Enable performance profiling with CPU analysis and bottleneck detection</li> <li><code>--profile-memory</code>: Also track peak memory per step</li> <li><code>--output</code>, <code>-o</code>: Output directory for generated artifacts</li> </ul>"},{"location":"reference/cli/#examples","title":"Examples","text":""},{"location":"reference/cli/#basic-execution","title":"Basic Execution","text":"<pre><code># Run a scenario (creates square_mesh.results.json by default)\nngraph run scenarios/square_mesh.yaml\n\n# Run a scenario and save results to custom file\nngraph run scenarios/backbone_clos.yml --results clos_analysis.json\n\n# Run a scenario without creating any files\nngraph run scenarios/nsfnet.yaml --no-results\n</code></pre>"},{"location":"reference/cli/#save-results-to-file","title":"Save Results to File","text":"<pre><code># Save results to a custom JSON file\nngraph run scenarios/backbone_clos.yml --results analysis.json\n\n# Save to file AND print to stdout\nngraph run scenarios/backbone_clos.yml --results analysis.json --stdout\n\n# Use default filename and also print to stdout\nngraph run scenarios/square_mesh.yaml --stdout\n</code></pre>"},{"location":"reference/cli/#running-test-scenarios","title":"Running Test Scenarios","text":"<pre><code># Run one of the provided scenarios with results export\nngraph run scenarios/backbone_clos.yml --results results.json\n</code></pre>"},{"location":"reference/cli/#filtering-results-by-step-names","title":"Filtering Results by Step Names","text":"<p>You can filter the output to include only specific workflow steps using the <code>--keys</code> option:</p> <pre><code># Only include results from the MSD step\nngraph run scenarios/square_mesh.yaml --keys msd_baseline --stdout\n\n# Include multiple specific steps and save to custom file\nngraph run scenarios/backbone_clos.yml --keys network_statistics tm_placement --results filtered.json\n\n# Filter and print to stdout while using default file\nngraph run scenarios/backbone_clos.yml --keys network_statistics --stdout\n</code></pre> <p>The <code>--keys</code> option filters by the <code>name</code> field of workflow steps defined in your scenario YAML file. For example, if your scenario has:</p> <pre><code>workflow:\n  - type: NetworkStats\n    name: network_statistics\n  - type: MaximumSupportedDemand\n    name: msd_baseline\n</code></pre> <p>Then <code>--keys network_statistics</code> will include only the results from the NetworkStats step, and <code>--keys msd_baseline</code> will include only the MaximumSupportedDemand results.</p>"},{"location":"reference/cli/#performance-profiling","title":"Performance Profiling","text":"<p>Enable performance profiling to identify bottlenecks and analyze execution time:</p> <pre><code># Run scenario with profiling\nngraph run scenarios/backbone_clos.yml --profile\n\n# Combine profiling with results export\nngraph run scenarios/backbone_clos.yml --profile --results analysis.json\n\n# Profile specific workflow steps and track memory\nngraph run scenarios/backbone_clos.yml --profile --profile-memory --keys tm_placement\n</code></pre> <p>The profiling output includes:</p> <ul> <li>Summary: Total execution time, CPU efficiency, function call statistics</li> <li>Step timing: Time spent in each workflow step with percentage breakdown</li> <li>Bottlenecks: Steps consuming &gt;10% of total execution time</li> <li>Function analysis: Top CPU-consuming functions within bottlenecks</li> <li>Recommendations: Specific suggestions for each bottleneck</li> </ul> <p>When to use profiling:</p> <ul> <li>Performance analysis during development</li> <li>Identifying bottlenecks in complex workflows</li> <li>Benchmarking before/after changes</li> </ul>"},{"location":"reference/cli/#output-format","title":"Output Format","text":"<p>The CLI outputs results as JSON with a fixed top-level shape:</p> <pre><code>{\n  \"workflow\": { \"&lt;step&gt;\": { \"step_type\": \"...\", \"execution_order\": 0, \"step_name\": \"...\" } },\n  \"steps\": {\n    \"network_statistics\": { \"metadata\": {}, \"data\": { \"node_count\": 42, \"link_count\": 84 } },\n    \"msd_baseline\": { \"metadata\": {}, \"data\": { \"alpha_star\": 1.23, \"context\": { \"demand_set\": \"baseline_traffic_matrix\" } } },\n    \"tm_placement\": { \"metadata\": { \"iterations\": 1000 }, \"data\": { \"flow_results\": [ { \"flows\": [], \"summary\": {} } ], \"context\": { \"demand_set\": \"baseline_traffic_matrix\" } } }\n  },\n  \"scenario\": { \"seed\": 42, \"failures\": { }, \"demands\": { } }\n}\n</code></pre> <ul> <li>BuildGraph: stores <code>data.graph</code> in node-link JSON format</li> <li>MaxFlow and TrafficMatrixPlacement: store <code>data.flow_results</code> as lists of per-iteration results (flows + summary)</li> <li>NetworkStats: stores capacity and degree statistics under <code>data</code></li> </ul>"},{"location":"reference/cli/#output-behavior","title":"Output Behavior","text":"<p>NetGraph CLI generates results by default for analysis workflows:</p>"},{"location":"reference/cli/#default-behavior-results-generated","title":"Default Behavior (Results Generated)","text":"<pre><code>ngraph run scenarios/square_mesh.yaml\n</code></pre> <ul> <li>Executes the scenario</li> <li>Logs execution progress to the terminal</li> <li>Creates <code>&lt;scenario_name&gt;.results.json</code> by default</li> <li>Shows success message with file location</li> </ul>"},{"location":"reference/cli/#custom-results-file","title":"Custom Results File","text":"<pre><code># Save to custom file\nngraph run scenarios/square_mesh.yaml --results my_analysis.json\n</code></pre> <ul> <li>Creates specified JSON file instead of results.json</li> <li>Useful for organizing multiple analysis runs</li> </ul>"},{"location":"reference/cli/#print-to-terminal","title":"Print to Terminal","text":"<pre><code>ngraph run scenarios/square_mesh.yaml --stdout\n</code></pre> <ul> <li>Creates results.json AND prints JSON to stdout</li> <li>Useful for viewing results immediately while also saving them</li> </ul>"},{"location":"reference/cli/#combined-output","title":"Combined Output","text":"<pre><code>ngraph run scenarios/square_mesh.yaml --results analysis.json --stdout\n</code></pre> <ul> <li>Creates custom JSON file AND prints to stdout</li> <li>Provides flexibility for different workflows</li> </ul>"},{"location":"reference/cli/#disable-file-generation-edge-cases","title":"Disable File Generation (Edge Cases)","text":"<pre><code>ngraph run scenarios/square_mesh.yaml --no-results\n</code></pre> <ul> <li>Executes scenario without creating any output files</li> <li>Only shows execution logs and completion status</li> <li>Useful for testing, CI/CD validation, or when only logs are needed</li> </ul>"},{"location":"reference/cli/#integration-with-workflows","title":"Integration with Workflows","text":"<p>The CLI executes the complete workflow defined in your scenario file, running all steps in sequence and accumulating results. This runs complex network analysis tasks without manual intervention.</p>"},{"location":"reference/cli/#recommended-workflow","title":"Recommended Workflow","text":"<ol> <li>Inspect first: Always use <code>inspect</code> to validate and understand your scenario</li> <li>Debug issues: Use detailed inspection to troubleshoot network expansion problems</li> <li>Run after validation: Execute scenarios after successful inspection</li> <li>Iterate: Use inspection during scenario development to verify changes</li> </ol> <pre><code># Development workflow\nngraph inspect scenarios/backbone_clos.yml --detail\nngraph run scenarios/backbone_clos.yml\n</code></pre>"},{"location":"reference/cli/#debugging-scenarios","title":"Debugging Scenarios","text":"<p>When developing complex scenarios with blueprints and hierarchical structures:</p> <pre><code># Check if scenario loads correctly\nngraph inspect scenarios/square_mesh.yaml\n\n# Debug network expansion issues (note: global option placement)\nngraph --verbose inspect scenarios/backbone_clos.yml --detail\n\n# Verify workflow steps are configured correctly\nngraph inspect scenarios/backbone_clos.yml --detail | grep -A 5 \"WORKFLOW STEPS\"\n</code></pre> <p>The <code>inspect</code> command will catch common issues like:</p> <ul> <li>Invalid YAML syntax</li> <li>Missing blueprint references</li> <li>Incorrect node/link patterns</li> <li>Workflow step configuration errors</li> <li>Risk group and policy definition problems</li> </ul>"},{"location":"reference/design/","title":"NetGraph Design and Implementation","text":"<p>This document describes NetGraph's internal design: scenario DSL, data models, execution flow, algorithms, manager components, and result handling. It focuses on architecture and key implementation details.</p>"},{"location":"reference/design/#overview","title":"Overview","text":"<p>NetGraph is a network scenario analysis engine using a hybrid Python+C++ architecture. It takes a scenario (defined in a YAML DSL) as input, builds a directed multigraph model of the network, and runs a configurable workflow of analysis steps (like traffic placement or max-flow capacity) to produce structured results.</p>"},{"location":"reference/design/#architecture-layers","title":"Architecture Layers","text":"<p>Python Layer (NetGraph):</p> <ul> <li>CLI and API: Entry points to load scenarios and invoke analyses</li> <li>Scenario DSL Parser: Validates and expands the YAML scenario into an internal model</li> <li>Domain Model: In-memory representation of nodes, links, risk groups, etc., with selection and grouping utilities</li> <li>Managers: Orchestrators for higher-level behaviors (demand expansion, failure enumeration)</li> <li>Workflow Engine: Composes steps into end-to-end analyses, storing outputs in a results store</li> <li>Results Store: Collects outputs and metadata from each step, enabling structured JSON export</li> <li>Analysis bridge: <code>AnalysisContext</code> builds Core graphs from the model, manages name/ID mapping, and executes Core algorithms</li> <li>NetworkExplorer: Network hierarchy traversal and hardware cost/power aggregation</li> </ul> <p>C++ Layer (NetGraph-Core):</p> <ul> <li>StrictMultiDiGraph: Immutable directed multigraph with CSR adjacency representation</li> <li>Shortest Paths (SPF): Dijkstra's algorithm with multipath support and configurable edge selection</li> <li>K-Shortest Paths: Yen's algorithm for finding k-shortest simple paths</li> <li>Max-Flow: Successive shortest paths with blocking flow augmentation and configurable flow placement policies</li> <li>Backend Interface: Abstraction for algorithm execution (CPU backend provided)</li> </ul>"},{"location":"reference/design/#package-structure","title":"Package Structure","text":"<pre><code>ngraph/\n\u251c\u2500\u2500 analysis/       # AnalysisContext, FailureManager, placement\n\u251c\u2500\u2500 model/          # Network, Node, Link, demand/, failure/, flow/\n\u251c\u2500\u2500 dsl/            # YAML parsing (blueprints/, selectors/, expansion/)\n\u251c\u2500\u2500 workflow/       # WorkflowStep implementations\n\u251c\u2500\u2500 results/        # Results store and flow result types\n\u251c\u2500\u2500 schemas/        # JSON Schema for scenario validation\n\u251c\u2500\u2500 types/          # Enums, DTOs, type aliases\n\u251c\u2500\u2500 profiling/      # Performance profiling\n\u251c\u2500\u2500 lib/            # NetworkX integration\n\u251c\u2500\u2500 utils/          # Utilities (ids, yaml, seeds)\n\u251c\u2500\u2500 scenario.py     # Scenario orchestrator\n\u251c\u2500\u2500 explorer.py     # NetworkExplorer\n\u2514\u2500\u2500 cli.py          # Command-line interface\n</code></pre>"},{"location":"reference/design/#integration-points","title":"Integration Points","text":"<p>The Python layer uses the <code>analyze()</code> function and <code>AnalysisContext</code> class (<code>ngraph.analysis</code>) to:</p> <ol> <li>Build Core graphs from Network instances with optional pseudo-nodes for source/sink groups</li> <li>Map node names (str) to NodeId (int32) and link IDs (str) to EdgeId/ext_edge_id (int64)</li> <li>Execute analysis methods (max_flow, shortest_paths, sensitivity) with efficient masking</li> <li>Translate results (costs, flows, paths) back to scenario-level objects</li> </ol> <p>Core algorithms release the GIL during execution, enabling concurrent Python threads to execute analysis in parallel with minimal Python-level overhead.</p> <p>Primary API:</p> <pre><code>from ngraph import analyze, Mode\n\n# One-off analysis (unbound context)\nflow = analyze(network).max_flow(\"^src$\", \"^dst$\", mode=Mode.COMBINE)\n\n# Efficient repeated analysis (bound context)\nctx = analyze(network, source=\"^src$\", sink=\"^dst$\", mode=Mode.COMBINE)\nbaseline = ctx.max_flow()\ndegraded = ctx.max_flow(excluded_links=failed_links)\n</code></pre> <p>The <code>AnalysisContext</code> encapsulates all graph building and provides properties for advanced use by workflow steps and the FailureManager.</p>"},{"location":"reference/design/#execution-flow","title":"Execution Flow","text":"<p>The diagram below shows the architecture and end-to-end execution flow from scenario input through both Python and C++ layers to final results. The Python layer handles scenario loading, workflow orchestration, and result aggregation, while compute-intensive graph algorithms execute in C++ with the GIL released for parallel execution.</p> <p></p>"},{"location":"reference/design/#scenario-dsl-and-input-expansion","title":"Scenario DSL and Input Expansion","text":"<p>NetGraph scenarios are defined in YAML using a declarative DSL (see DSL Reference). The DSL allows concise specification of network topologies, traffic demands, failure policies, and analysis workflows. Before execution, scenario files are validated against a JSON Schema to catch errors early (unknown keys, type mismatches), enforcing strict definitions.</p> <p>Key elements of the DSL include:</p> <ul> <li> <p>Seed: A master random seed for the scenario to ensure deterministic behavior across runs.</p> </li> <li> <p>Blueprints: Reusable templates for subsets of the topology. A blueprint defines internal node types, roles, and optional internal links. Blueprints enable defining a complex multi-node topology once and instantiating it multiple times with different parameters.</p> </li> <li> <p>Node Groups: Definitions of node groups in the topology, either explicitly or via patterns. Groups can use a blueprint (<code>blueprint</code>) with parameters (<code>params</code>), or define a number of nodes (<code>count</code>) with a naming template (<code>template</code>).</p> </li> <li> <p>Links: Rules to generate links between node groups. Instead of enumerating every link, a link rule specifies source and target selectors (by path pattern), a wiring pattern (e.g. mesh for full mesh or one_to_one for paired links), number of parallel links (<code>count</code>), and link properties (capacity, cost, attributes like distance, hardware, risk group tags, etc.). Link properties are specified at the top level, not inside a wrapper. Advanced matching allows filtering nodes by attributes with logical conditions (AND/OR) to apply link rules to selected nodes only. A single rule can thus expand into many concrete links.</p> </li> <li> <p>Rules: Optional modifications applied after the initial expansion. <code>node_rules</code> or <code>link_rules</code> can match specific nodes or links (by path or endpoints) and change their attributes or disable them. This allows fine-tuning or simulating removals without changing the base definitions.</p> </li> <li> <p>Risk Groups: Named shared-risk groups (potentially nested) that nodes or links can belong to. These are used in failure scenarios to correlate failures (e.g. all links in a risk group fail together).</p> </li> <li> <p>Demands: Traffic demand definitions specifying source node sets, target node sets (by regex or attribute path selectors), and volume. Each demand can also include priority or custom flow placement policy.</p> </li> <li> <p>Failure Policies: Definitions of failure scenarios or modes, possibly with weights (probabilities). For example, a policy might say \"with 5% chance, fail any single core node\" or \"fail all links in risk_group X\". The failure manager uses these policies to generate specific failure combinations for simulation.</p> </li> <li> <p>Workflow: An ordered list of analysis steps to execute. Each step has a <code>type</code> (the analysis to perform, such as \"MaxFlow\" or \"TrafficMatrixPlacement\"), a unique name, and parameters (like number of iterations, etc.). The workflow definition orchestrates the analysis pipeline.</p> </li> </ul>"},{"location":"reference/design/#dsl-expansion-process","title":"DSL Expansion Process","text":"<p>The loader validates and expands DSL definitions into concrete nodes and links. Unknown fields or schema violations cause an immediate error before any expansion. After schema validation, blueprints are resolved (each blueprint group becomes actual Node objects), group name patterns are expanded into individual names, and adjacency rules are iterated over matching source-target node sets to create Link objects. All nodes and links are then validated in runtime to ensure they are valid (e.g., no duplicate node names, all link endpoints exist).</p>"},{"location":"reference/design/#data-model","title":"Data Model","text":"<p>Once the scenario is parsed and expanded, NetGraph represents the network with a set of core model classes. These define the in-memory representation of the scenario topology and enforce structural invariants (unique node names, valid link endpoints).</p>"},{"location":"reference/design/#node","title":"Node","text":"<p>A Node represents a network node (vertex). Each node has:</p> <ul> <li> <p>a unique name (string identifier),</p> </li> <li> <p>a disabled flag (if the node is turned off in the scenario),</p> </li> <li> <p>a set of risk_groups (associating the node with any failure domains), and</p> </li> <li> <p>an attrs dictionary for arbitrary metadata (e.g., region, device type, hardware info)</p> </li> </ul>"},{"location":"reference/design/#link","title":"Link","text":"<p>A Link represents a directed link between a source and target node. Each link has:</p> <ul> <li> <p>source and target node names,</p> </li> <li> <p>capacity (float, e.g. in some bandwidth unit),</p> </li> <li> <p>cost (float, e.g. distance or latency metric),</p> </li> <li> <p>disabled flag,</p> </li> <li> <p>risk_groups set,</p> </li> <li> <p>attrs dict for metadata (e.g. distance_km, fiber type), and</p> </li> <li> <p>an auto-generated unique id</p> </li> </ul> <p>The id is constructed as \"source|target|\", ensuring each link has a distinct identifier. The model stores each link as directed (source -&gt; target). When the analysis graph is built, a reverse edge is added by default so algorithms see bidirectional connectivity."},{"location":"reference/design/#riskgroup","title":"RiskGroup","text":"<p>A RiskGroup represents a named failure domain or shared-risk link group (SRLG). Risk groups can be hierarchical (a risk group may have children risk groups). Each RiskGroup has:</p> <ul> <li> <p>a name,</p> </li> <li> <p>list of children RiskGroups (which inherit the failure domain property),</p> </li> <li> <p>disabled flag (if the entire group is considered initially failed in the scenario), and</p> </li> <li> <p>an attrs dict for any metadata</p> </li> </ul> <p>Hierarchical risk groups allow, for example, defining a large domain composed of smaller sub-domains. A failure event could disable an entire group, implicitly affecting all its descendants.</p>"},{"location":"reference/design/#network","title":"Network","text":"<p>A Network is the container class that holds all nodes, links, and top-level risk groups for the scenario. The Network class maintains:</p> <ul> <li> <p>nodes: Dict[name, Node],</p> </li> <li> <p>links: Dict[id, Link],</p> </li> <li> <p>risk_groups: Dict[name, RiskGroup],</p> </li> </ul> <p>Network is the container for scenario topology. It enforces invariants during construction: adding a link validates that source and target nodes exist; adding a node rejects duplicates by name. Components are never removed from the Network; the <code>disabled</code> flag marks them inactive. The Network also maintains a selection cache for <code>select_node_groups_by_path</code> to avoid repeated regex/attribute queries.</p>"},{"location":"reference/design/#node-and-link-selection","title":"Node and Link Selection","text":"<p>The model supports selecting groups of nodes via a unified selector system used by algorithms to choose source/sink sets matching on structured names or attributes.</p> <p>Selector Forms:</p> <p>Selectors can be specified as:</p> <ol> <li>String pattern: A regex matched against node names (anchored at start via <code>re.match()</code>)</li> <li>Selector object: A dict with <code>path</code>, <code>group_by</code>, and/or <code>match</code> fields</li> </ol> <p>String Pattern Behavior:</p> <p>When using a regex pattern, if the regex contains capturing groups, the concatenated capture groups form the group label; otherwise, the entire pattern string is used as the label. For instance, the pattern <code>r\"(\\w+)-(\\d+)\"</code> on node names could produce group labels like \"metroA-1\" etc.</p> <p>Attribute-based Grouping:</p> <p>Use <code>group_by</code> in a selector object to group nodes by an attribute value:</p> <pre><code>source:\n  group_by: \"role\"\n</code></pre> <p>This groups nodes by the value of <code>node.attrs[\"role\"]</code> (e.g., \"core\", \"leaf\"), returning a dict mapping each distinct value to the list of nodes with that value. Nodes missing the attribute are excluded.</p> <p>Attribute-based Filtering:</p> <p>Use <code>match</code> in a selector object to filter nodes by attribute conditions:</p> <pre><code>source:\n  path: \"^dc1/.*\"\n  match:\n    conditions:\n      - attr: \"tier\"\n        op: \"==\"\n        value: \"leaf\"\n</code></pre> <p>This selection mechanism allows workflow steps and API calls to refer to nodes flexibly (using human-readable patterns instead of explicit lists), which is particularly useful in large topologies.</p>"},{"location":"reference/design/#disabled-elements","title":"Disabled Elements","text":"<p>Nodes or links marked as disabled=True represent elements present in the design but out of service for the analysis. The base model keeps them in the collection but analysis functions filter them out when selecting active nodes. This design preserves topology information (e.g., you know a link exists but is just turned off) and allows easily enabling it later if needed.</p>"},{"location":"reference/design/#filtered-analysis-exclusions","title":"Filtered Analysis (Exclusions)","text":"<p>To simulate failures or other what-if scenarios without modifying the base network, NetGraph uses analysis-time exclusions. Instead of creating a stateful view object, you pass sets of excluded nodes and links directly to analysis functions.</p> <pre><code># Analyze with specific exclusions\nresults = analyze(network).max_flow(\n    \"^A$\",\n    \"^B$\",\n    excluded_nodes={\"Node5\"},\n    excluded_links={\"A|B|xyz123\"}\n)\n</code></pre> <p>This approach avoids mutating the base graph when simulating failures (e.g., deleting nodes or toggling flags). It separates the static scenario (base network) from dynamic conditions (exclusions), enabling thread-safe parallel analyses and eliminating deep copies for each failure scenario.</p> <p>Implementation: For repeated analysis (Monte Carlo, FailureManager), exclusions are applied via boolean masks passed to Core algorithms. The graph is built once without exclusions, and masks disable specific elements at algorithm execution time. This enables O(|excluded|) mask updates rather than O(V+E) graph rebuilding. For one-off solver calls, exclusions may be applied during graph construction for simplicity.</p> <p>Multiple concurrent analyses can run on the same base network with different exclusion sets. This is important for performing parallel simulations (e.g., analyzing many failure combinations in a Monte Carlo) efficiently.</p>"},{"location":"reference/design/#graph-construction","title":"Graph Construction","text":"<p>NetGraph builds graphs through <code>AnalysisContext</code> which translates from the Python domain model to NetGraph-Core's C++ representation.</p> <p>Python Side (<code>ngraph.analysis.AnalysisContext</code>):</p> <p>A single construction method is provided:</p> <ul> <li><code>AnalysisContext.from_network()</code>: Constructs an immutable context with pre-built Core graph, mappers, algorithms instance, and pre-computed disabled topology. Exclusions are applied at algorithm call time via boolean masks rather than during graph construction.</li> </ul> <p>Graph Construction Steps:</p> <ul> <li>Collects nodes from Network (real + optional pseudo nodes for augmentation)</li> <li>Assigns stable node IDs (sorted by name for determinism)</li> <li>Encodes link_id + direction as ext_edge_id (packed int64)</li> <li>Constructs NumPy arrays (src, dst, capacity, cost, ext_edge_ids)</li> <li>Supports augmentation edges (e.g., pseudo-source/sink for multi-source max-flow)</li> </ul> <p>AnalysisContext Internals:</p> <p><code>AnalysisContext</code> encapsulates pre-built graph components as internal attributes:</p> <ul> <li><code>_handle</code>, <code>_multidigraph</code>: Core graph structures</li> <li><code>_node_mapper</code>, <code>_edge_mapper</code>: Name \u2194 ID translation</li> <li><code>_algorithms</code>: Core Algorithms instance</li> <li><code>_disabled_node_ids</code>, <code>_disabled_link_ids</code>: Pre-computed disabled topology</li> <li><code>_link_id_to_edge_indices</code>: Pre-computed mapping for O(|excluded|) mask building</li> <li><code>_pseudo_context</code>: Optional context for pseudo source/sink node mappings</li> </ul> <p>When analyzing many failure scenarios, the graph is built once via <code>AnalysisContext.from_network()</code> and exclusions are applied via boolean masks. The mask builders automatically include disabled nodes/links, ensuring disabled topology is always excluded. This avoids rebuilding the graph for each iteration, providing significant speedup for Monte Carlo simulations.</p> <p>Disabled Topology Handling:</p> <p>Disabled nodes and links from the Network are pre-computed during <code>AnalysisContext.from_network()</code> and stored in the context. The mask builders automatically include these disabled elements alongside any per-iteration exclusions.</p> <p>C++ Side (<code>netgraph_core.StrictMultiDiGraph</code>):</p> <ul> <li>Immutable directed multigraph using Compressed Sparse Row (CSR) adjacency</li> <li>Nodes identified by NodeId (int32), edges by EdgeId (int32)</li> <li>Each edge stores capacity (float64), cost (int64), and ext_edge_id (int64)</li> <li>Edges sorted by (cost, src, dst) for deterministic algorithm behavior</li> <li>Zero-copy NumPy views for array access (capacities, costs, ext_edge_ids)</li> <li>Efficient neighbor iteration via CSR structure</li> </ul> <p>Edge Direction Handling:</p> <p>If <code>add_reverse=True</code> (default), graph construction creates bidirectional edges for each network link:</p> <ul> <li>Forward edge: original link direction with ext_edge_id encoding (link_id, 'fwd')</li> <li>Reverse edge: opposite direction with ext_edge_id encoding (link_id, 'rev')</li> </ul> <p>This allows algorithms to consider traffic flowing in both directions on physical links. The Core graph itself is always directed; bidirectionality is achieved by explicit reverse edges.</p> <p>Augmentation Support:</p> <p>For algorithms requiring virtual source/sink nodes (e.g., multi-source max-flow), the adapter adds augmentation edges with ext_edge_id = -1 (sentinel for non-network edges). These edges are not mapped back to scenario links in results.</p>"},{"location":"reference/design/#analysis-algorithms","title":"Analysis Algorithms","text":"<p>NetGraph's core algorithms execute in C++ via NetGraph-Core. Algorithms operate on the immutable StrictMultiDiGraph and support masking (runtime exclusions via boolean arrays) for efficient repeated analysis under different failure scenarios without graph reconstruction.</p> <p>All Core algorithms release the Python GIL during execution, enabling concurrent execution across multiple Python threads without GIL contention.</p>"},{"location":"reference/design/#shortest-path-first-spf-algorithm","title":"Shortest-Path First (SPF) Algorithm","text":"<p>Implemented in C++ (<code>netgraph::core::shortest_paths</code>), using Dijkstra's algorithm with configurable edge selection and optional multipath predecessor recording.</p> <p>Core Features:</p> <p>Edge Selection Policies:</p> <p>The algorithm evaluates parallel edges per neighbor using <code>EdgeSelection</code> configuration:</p> <ul> <li><code>multi_edge=true</code> (default): Include all parallel edges u\u2192v with minimal cost among (u,v) pairs</li> <li><code>multi_edge=false</code>: Select single edge per (u,v) pair using tie-breaking:</li> <li><code>PreferHigherResidual</code>: Choose edge with highest residual capacity (secondary: lowest edge ID)</li> <li><code>Deterministic</code>: Choose edge with lowest edge ID for reproducibility</li> <li><code>require_capacity=true</code>: Only consider edges with residual capacity &gt; kMinCap (used in max-flow)</li> <li><code>require_capacity=false</code> (default): Consider all edges regardless of residual capacity</li> </ul> <p>Capacity-Aware Tie-Breaking:</p> <p>When multiple nodes or edges have equal cost, SPF uses residual capacity for tie-breaking to improve flow distribution:</p> <ul> <li>Node-level: Priority queue ordered by (cost, -residual, node). Among equal-cost nodes, prefers paths with higher bottleneck capacity. This naturally guides flow toward higher-capacity routes.</li> <li>Edge-level: When <code>multi_edge=false</code> and <code>tie_break=PreferHigherResidual</code>, selects the parallel edge with most available capacity among equal-cost options.</li> </ul> <p>This tie-breaking is applied even in IP/IGP mode (<code>require_capacity=false</code>) using static capacities, improving flow distribution without altering routing topology.</p> <p>Multipath Support:</p> <p>With <code>multipath=True</code>, SPF stores all minimal-cost predecessors forming a DAG: <code>pred[node] = {predecessor: [edge_ids...]}</code>. This DAG captures all equal-cost paths in a compact form, used by max-flow for flow splitting.</p> <p>Early Termination:</p> <p>If <code>dst</code> is provided, SPF stops expanding after popping <code>dst</code> from the priority queue (continuing only while heap front cost equals dst cost to capture equal-cost predecessors). This optimization reduces work when only source-to-sink distances are needed.</p> <p>Masking:</p> <p>Optional <code>node_mask</code> and <code>edge_mask</code> boolean arrays enable runtime exclusions without rebuilding the graph. Used by FailureManager for efficient Monte Carlo analysis.</p> <p>Complexity:</p> <p>Using binary heap with capacity-aware tie-breaking: (O((V+E) \\log V)) time, (O(V+E)) space for costs, predecessors, and residual tracking.</p>"},{"location":"reference/design/#pseudocode-simplified-see-implementation-for-complete-details","title":"Pseudocode (simplified, see implementation for complete details)","text":"<pre><code>function SPF(graph, src, dst=None, multipath=True, edge_selection):\n    costs = { src: 0 }\n    pred  = { src: {} }\n    min_residual_to_node = { src: infinity }  # Track bottleneck capacity for tie-breaking\n\n    # Priority queue with node-level tie-breaking by residual capacity\n    # QItem: (cost, -residual, node) - negated residual for max-heap behavior\n    pq = [(0, -infinity, src)]\n    best_dst_cost = None\n\n    while pq:\n        (c, neg_res, u) = heappop(pq)\n        if c &gt; costs[u]:\n            continue  # stale entry\n\n        if dst is not None and u == dst and best_dst_cost is None:\n            best_dst_cost = c\n        if dst is not None and u == dst:\n            if not pq or pq[0][0] &gt; best_dst_cost:\n                break\n            continue\n\n        # Relax edges from u\n        for v in neighbors(u):\n            # Edge selection among parallel edges u-&gt;v\n            min_cost = inf\n            selected_edges = []\n\n            for e_id in edges_between(u, v):\n                residual_cap = residual[e_id] if has_residual else capacity[e_id]\n\n                # Skip if capacity filtering enabled and edge has no residual\n                if edge_selection.require_capacity and residual_cap &lt; kMinCap:\n                    continue\n\n                edge_cost = cost[e_id]\n\n                if edge_cost &lt; min_cost:\n                    min_cost = edge_cost\n                    selected_edges = select_edge_by_policy(e_id, edge_selection, residual_cap)\n\n                elif edge_cost == min_cost:\n                    if edge_selection.multi_edge:\n                        selected_edges.append(e_id)  # Keep all equal-cost edges\n                    else:\n                        # Edge-level tie-breaking for single-edge selection\n                        selected_edges = tiebreak_edge(selected_edges, e_id,\n                                                      edge_selection.tie_break, residual_cap)\n\n            if not selected_edges:\n                continue  # no admissible edges to v\n\n            new_cost = c + min_cost\n\n            # Compute bottleneck capacity: min of path residual and max edge residual\n            max_edge_res = max(residual[e] for e in selected_edges)\n            path_residual = min(min_residual_to_node[u], max_edge_res)\n\n            # Relaxation: found shorter path\n            if new_cost &lt; costs[v]:\n                costs[v] = new_cost\n                min_residual_to_node[v] = path_residual\n                pred[v] = { u: selected_edges }\n                pq.push((new_cost, -path_residual, v))  # Node-level tie-breaking by capacity\n\n            # Multipath: found equal-cost alternative\n            elif multipath and new_cost == costs[v]:\n                pred[v][u] = selected_edges\n                # Don't update min_residual_to_node in multipath (collecting all paths)\n\n        if best_dst_cost is not None and (not pq or pq[0][0] &gt; best_dst_cost):\n            break\n\n    return costs, pred\n\n\n# Tie-breaking policies for edge selection when multi_edge=false:\nfunction tiebreak_edge(current_edges, new_edge, tie_break, new_residual):\n    if tie_break == PreferHigherResidual:\n        # Select edge with highest residual capacity\n        if new_residual &gt; current_best_residual + epsilon:\n            return [new_edge]\n        elif abs(new_residual - current_best_residual) &lt;= epsilon:\n            # Secondary tie-break: deterministic by edge ID\n            return [min(new_edge, current_edges[0])]\n    else:  # Deterministic\n        # Select edge with smallest ID for reproducibility\n        return [min(new_edge, current_edges[0])]\n</code></pre> <p>Key Tie-Breaking Mechanisms:</p> <ol> <li> <p>Node-level tie-breaking: When multiple nodes have equal cost in the priority queue, prefer nodes reachable via paths with higher bottleneck (residual) capacity. This naturally distributes flows across equal-cost paths based on available capacity.</p> </li> <li> <p>Edge-level tie-breaking (when <code>multi_edge=false</code>):    - <code>PreferHigherResidual</code>: Among parallel equal-cost edges (u,v), select the one with highest residual capacity    - <code>Deterministic</code>: Select edge with smallest ID for reproducible results</p> </li> <li> <p>Multipath behavior: When <code>multipath=true</code>, all equal-cost predecessors are retained without capacity-based filtering, enabling flow splitting across all equal-cost paths.</p> </li> </ol>"},{"location":"reference/design/#maximum-flow-algorithm","title":"Maximum Flow Algorithm","text":"<p>Implemented in C++ (<code>netgraph::core::max_flow</code>), using successive shortest paths with blocking flow augmentation. The algorithm blends Edmonds-Karp (augment along shortest paths) and Dinic (push blocking flows on a level graph) with cost awareness and configurable flow splitting across equal-cost parallel edges.</p> <p>Goal: Compute maximum feasible flow between source and sink under edge capacity constraints.</p> <p>Multi-source/multi-sink: Handled by <code>AnalysisContext</code> which creates pseudo-source and pseudo-sink nodes with large-capacity, zero-cost edges to/from real endpoints. The C++ algorithm operates on single source and single sink.</p> <p>Routing Semantics: The algorithm's behavior is controlled by <code>require_capacity</code> and <code>shortest_path</code>:</p> <ul> <li><code>require_capacity=true</code> + <code>shortest_path=false</code> (SDN/TE): SPF filters to edges with residual capacity, routes adapt iteratively during placement</li> <li><code>require_capacity=false</code> + <code>shortest_path=true</code> (IP/IGP): SPF uses all edges based on cost, single-pass flow placement over fixed equal-cost paths</li> </ul> <p>See \"Routing Semantics: IP/IGP vs SDN/TE\" section for detailed explanation.</p> <p>The residual network is maintained via <code>FlowState</code>, which tracks per-edge flow and computes residual capacities on demand. For each edge u\u2192v:</p> <ul> <li>Forward residual capacity: <code>capacity(u,v) - flow(u,v)</code></li> <li>Reverse residual capacity (for flow cancellation): <code>flow(u,v)</code></li> </ul> <p>SPF operates over the residual graph by requesting edges with <code>require_capacity=true</code>, which filters to edges with positive residual capacity. The <code>FlowState</code> provides a residual capacity view without graph mutation.</p> <p>Note: Reverse residual arcs for flow cancellation are distinct from physical reverse edges added via <code>add_reverse=True</code> during graph construction. Physical reverse edges model bidirectional links with independent capacity; residual reverse arcs enable flow augmentation/cancellation.</p> <p>The core loop finds augmenting paths using the cost-aware SPF described above:</p> <p>Run SPF from source to sink with <code>multi_edge=true</code> and <code>require_capacity=true</code> (filters to edges with positive residual capacity). This computes shortest-path distances and a predecessor DAG over forward residual edges. The edge cost can represent distance, latency, or preference; SPF selects paths minimizing cumulative cost.</p> <p>If the pseudo-sink is not reached (i.e., no augmenting path exists), stop: the max flow is achieved.</p> <p>Otherwise, determine how much flow can be sent along the found paths:</p> <p>Using the predecessor DAG from SPF, <code>FlowState.place_on_dag</code> computes blocking flow considering parallel edges and the splitting policy. For PROPORTIONAL: builds reversed residual graph, assigns BFS levels, uses DFS to push flow with capacity-proportional splits. For EQUAL_BALANCED: performs topological traversal with equal splits, computes global scale factor to prevent oversubscription.</p> <p>This yields flow amount <code>f</code> and per-edge flow assignments tracking which edges carry flow and their utilization.</p> <p>The algorithm then augments the flow: <code>FlowState</code> increases each edge's flow by its assigned portion. Per-edge flows and residual capacities are updated for the next iteration.</p> <p>Add f to the total flow counter.</p> <p>If <code>f</code> is below tolerance <code>kMinFlow</code> (negligible flow placed due to numerical limits or exhausted capacity), terminate iteration.</p> <p>Repeat to find the next augmenting path (back to step 1).</p> <p>If <code>shortest_path=True</code>, the algorithm performs only one augmentation pass and returns (useful when the goal is a single cheapest augmentation rather than maximum flow).</p> <p>After the loop, the C++ algorithm computes a FlowSummary which includes:</p> <ul> <li> <p>total_flow: the sum of flow from source to sink achieved</p> </li> <li> <p>edge_flows: per-edge flow assignments (optional, populated when requested)</p> </li> <li> <p>residual_capacity: remaining capacity on each edge = capacity - flow (optional, populated when requested)</p> </li> <li> <p>reachable_nodes: the set of nodes reachable from the source in the final residual network (optional, identifies the source side of the min-cut)</p> </li> <li> <p>min_cut: the list of edges that are saturated and go from reachable to non-reachable (these form the minimum cut)</p> </li> <li> <p>cost_distribution: flow volume placed at each path cost tier. Core returns parallel arrays (<code>costs</code>, <code>flows</code>); AnalysisContext converts these to <code>Dict[Cost, Flow]</code> mapping in <code>FlowSummary.cost_distribution</code>.</p> </li> </ul> <p>This is returned along with the total flow value.</p>"},{"location":"reference/design/#routing-semantics-ipigp-vs-sdnte","title":"Routing Semantics: IP/IGP vs SDN/TE","text":"<p>NetGraph models two fundamentally different routing paradigms through the <code>require_capacity</code> and <code>shortest_path</code> parameters:</p> <p>IP/IGP Semantics (<code>require_capacity=false</code> + <code>shortest_path=true</code>):</p> <p>Traditional IP routing with Interior Gateway Protocols (OSPF, IS-IS):</p> <ul> <li>Routes computed based on link costs/metrics only, ignoring available capacity</li> <li>Single SPF computation determines equal-cost paths; forwarding is fixed until topology/cost change</li> <li>Traffic follows predetermined paths even as links saturate</li> <li>Models best-effort forwarding with potential packet loss when demand exceeds capacity</li> <li>No iterative augmentation: flow placed in single pass over fixed equal-cost DAG</li> <li>Use case: Simulating production IP networks, validating IGP designs</li> </ul> <p>SDN/TE Semantics (<code>require_capacity=true</code> + <code>shortest_path=false</code>, default):</p> <p>Software-Defined Networking and Traffic Engineering:</p> <ul> <li>Routes adapt dynamically to residual link capacities during flow placement</li> <li>SPF recomputed after each flow placement iteration, excluding saturated links</li> <li>Iterative augmentation continues until max-flow achieved or capacity exhausted</li> <li>Flow placement respects capacity constraints, never oversubscribing links</li> <li>Models centralized traffic engineering with real-time capacity awareness</li> <li>Use case: Optimal demand placement, capacity planning, failure impact analysis</li> </ul> <p>This distinction is fundamental: IP networks route on cost alone with fixed forwarding tables (congestion managed via queuing/drops), while TE systems route dynamically on both cost and available capacity (congestion avoided via admission control). The <code>require_capacity</code> parameter controls whether SPF filters to available capacity; <code>shortest_path</code> controls whether routes are recomputed iteratively or fixed after initial SPF.</p>"},{"location":"reference/design/#flow-placement-strategies","title":"Flow Placement Strategies","text":"<p>Beyond routing semantics, NetGraph controls how flow splits across equal-cost parallel edges through <code>FlowPlacement</code>:</p> <ul> <li>PROPORTIONAL (default, models WCMP/Weighted ECMP):</li> <li>Splits flow across parallel equal-cost edges proportional to residual capacity</li> <li>Example: Two 100G links get 50/50 split; one 100G + one 10G get 91/9 split</li> <li>Maximizes utilization by preferring higher-capacity paths</li> <li>Used in networks with heterogeneous link speeds (common in fabrics with multi-generation hardware)</li> <li> <p>Can be used iteratively (e.g., successive max-flow augmentations)</p> </li> <li> <p>EQUAL_BALANCED (models traditional ECMP):</p> </li> <li>Splits flow equally across all parallel equal-cost edges regardless of capacity</li> <li>Example: Two 100G links get 50/50; one 100G + one 10G still attempt 50/50 (10G saturates first)</li> <li>Models IP hash-based load balancing (5-tuple hashing distributes flows uniformly)</li> <li>Single-pass admission: computes one global scale factor to avoid oversubscription</li> <li>For IP ECMP simulation: use with <code>require_capacity=false</code> + <code>shortest_path=true</code></li> </ul> <p><code>FlowState.place_on_dag</code> implements single-pass placement over a fixed SPF DAG:</p> <ul> <li> <p>PROPORTIONAL: Constructs reversed residual graph from predecessor DAG. Uses Dinic-style BFS leveling and DFS push from sink to source. Within each edge group (parallel edges between node pair), splits flow proportionally to residual capacity. Distributes pushed flow back to underlying edges maintaining proportional ratios. Can be called iteratively on updated residuals.</p> </li> <li> <p>EQUAL_BALANCED: Performs topological traversal (Kahn's algorithm) from source to sink over forward DAG. Assigns equal splits across all outgoing parallel edges from each node. Computes global scale factor as <code>min(edge_capacity / edge_assignment)</code> across all edges to prevent oversubscription. Applies scale uniformly and stops. This models single-pass ECMP admission where the forwarding DAG doesn't change mid-flow.</p> </li> </ul> <p>Configuration Examples:</p> <pre><code># IP/ECMP: Traditional router behavior (cost-based routing, equal splits)\nanalyze(network).max_flow(src, dst,\n    flow_placement=FlowPlacement.EQUAL_BALANCED,\n    shortest_path=True,  # Single SPF tier\n    require_capacity=False)  # Ignore capacity when routing\n\n# SDN/TE with WCMP: Capacity-aware routing with proportional splits\nanalyze(network).max_flow(src, dst,\n    flow_placement=FlowPlacement.PROPORTIONAL,\n    shortest_path=False,  # Iterative augmentation\n    require_capacity=True)  # Adapt routes to capacity\n\n# WCMP: Fixed equal-cost paths with bandwidth-weighted splits\nanalyze(network).max_flow(src, dst,\n    flow_placement=FlowPlacement.PROPORTIONAL,\n    shortest_path=True,  # Single tier of equal-cost paths\n    require_capacity=False)  # Fixed paths regardless of utilization\n</code></pre> <p>These configurations enable realistic modeling of diverse forwarding behaviors: from traditional IP networks with best-effort delivery to modern SDN deployments with capacity-aware traffic engineering.</p>"},{"location":"reference/design/#flow-policy-presets","title":"Flow Policy Presets","text":"<p>For traffic matrix placement, NetGraph provides <code>FlowPolicyPreset</code> values that bundle the routing semantics described above into convenient configurations. These presets map to real-world network behaviors:</p> Preset Behavior Use Case <code>SHORTEST_PATHS_ECMP</code> IP/IGP with hash-based ECMP Traditional routers (OSPF/IS-IS), equal splits across equal-cost paths <code>SHORTEST_PATHS_WCMP</code> IP/IGP with weighted ECMP Routers with WCMP support, proportional splits based on link capacity <code>TE_WCMP_UNLIM</code> MPLS-TE / SDN with WCMP Capacity-aware TE with unlimited tunnels, iterative placement <code>TE_ECMP_16_LSP</code> MPLS-TE with 16 LSPs Fixed 16 ECMP tunnels per demand, models RSVP-TE with LSP limits <code>TE_ECMP_UP_TO_256_LSP</code> MPLS-TE with up to 256 LSPs Scalable TE with tunnel limit, models SR-TE or large-scale RSVP <p>Detailed Configuration Mapping (preset internals):</p> Preset <code>require_capacity</code> <code>multi_edge</code> <code>max_flow_count</code> <code>flow_placement</code> <code>SHORTEST_PATHS_ECMP</code> <code>false</code> <code>true</code> <code>1</code> <code>EQUAL_BALANCED</code> <code>SHORTEST_PATHS_WCMP</code> <code>false</code> <code>true</code> <code>1</code> <code>PROPORTIONAL</code> <code>TE_WCMP_UNLIM</code> <code>true</code> <code>true</code> unlimited <code>PROPORTIONAL</code> <code>TE_ECMP_16_LSP</code> <code>true</code> <code>false</code> <code>16</code> <code>EQUAL_BALANCED</code> <code>TE_ECMP_UP_TO_256_LSP</code> <code>true</code> <code>false</code> <code>256</code> <code>EQUAL_BALANCED</code> <p>Key parameters (preset-managed):</p> <ul> <li><code>require_capacity</code>: When <code>false</code>, paths are selected based on link costs alone (models IP/IGP routing). When <code>true</code>, paths adapt to residual capacity during placement (models SDN/TE). See Routing Semantics for details.</li> <li><code>multi_edge</code>: When <code>true</code>, uses all parallel equal-cost edges (hop-by-hop ECMP); when <code>false</code>, each flow uses a single path (tunnel/LSP semantics).</li> <li><code>max_flow_count</code>: Internal per-preset limit on flows/LSPs for TE presets; not a user-facing parameter.</li> <li><code>flow_placement</code>: <code>EQUAL_BALANCED</code> splits equally across paths; <code>PROPORTIONAL</code> splits by residual capacity.</li> </ul> <p>Example: Modeling IP vs MPLS Networks</p> <pre><code># IP network with traditional ECMP (e.g., data center leaf-spine)\ndemands:\n  dc_traffic:\n    - source: ^rack1/\n      target: ^rack2/\n      volume: 1000.0\n      flow_policy: SHORTEST_PATHS_ECMP\n\n# MPLS-TE network with capacity-aware tunnel placement\ndemands:\n  backbone_traffic:\n    - source: ^metro1/\n      target: ^metro2/\n      volume: 5000.0\n      flow_policy: TE_WCMP_UNLIM\n</code></pre>"},{"location":"reference/design/#pseudocode-simplified-max-flow-loop","title":"Pseudocode (simplified max-flow loop)","text":"<pre><code>function MAX_FLOW(graph, S, T, placement=PROPORTIONAL, require_capacity=True):\n    flow_state = FlowState(graph)  # Tracks per-edge flow and residuals\n    total_flow = 0\n    cost_distribution = []\n\n    while True:\n        # Configure edge selection for SPF\n        edge_selection = EdgeSelection(\n            multi_edge=True,\n            require_capacity=require_capacity,\n            tie_break=Deterministic\n        )\n\n        # Find shortest augmenting paths in residual graph\n        residuals = flow_state.residual_view() if require_capacity else None\n        costs, dag = SPF(graph, S, T,\n                        multipath=True,\n                        edge_selection=edge_selection,\n                        residual=residuals)\n\n        if T not in dag:  # No augmenting path exists\n            break\n\n        # Push blocking flow through predecessor DAG\n        path_cost = costs[T]\n        placed = flow_state.place_on_dag(S, T, dag, infinity, placement)\n\n        if placed &lt; kMinFlow:  # Negligible flow placed\n            break\n\n        total_flow += placed\n        cost_distribution.append((path_cost, placed))\n\n    # Compute min-cut, reachability, cost distribution\n    min_cut = flow_state.compute_min_cut(S, node_mask, edge_mask)\n\n    return FlowSummary(\n        total_flow=total_flow,\n        cost_distribution=cost_distribution,\n        min_cut=min_cut,\n        edge_flows=...,  # optional\n        residual_capacity=...,  # optional\n        reachable_nodes=...  # optional\n    )\n</code></pre> <p>The flow tolerance constant <code>kMinFlow</code> (default 1/4096 \u2248 2.4e-4) determines when flow placement is considered negligible and iteration terminates.</p> <p>Each augmentation phase performs one SPF (O((V+E) \\log V)) and one blocking-flow computation (O(V+E)) over the predecessor DAG. With blocking flow augmentation, the shortest path distance (in hops) increases with each phase, bounding the number of phases by (O(V)). This yields an overall complexity of (O(V \\cdot (V+E) \\log V)) = (O(V^2 E \\log V)) for sparse graphs where (E = O(V)).</p> <p>Practical performance is significantly better than worst-case bounds due to early termination when residual capacity exhausts. For integer capacities, the bound becomes (O(F \\cdot (V+E) \\log V)) where (F) is the max-flow value, which dominates when (F \\ll V).</p>"},{"location":"reference/design/#managers-and-workflow-orchestration","title":"Managers and Workflow Orchestration","text":"<p>Managers handle scenario dynamics and prepare inputs for algorithmic steps.</p> <p>Demand Expansion (<code>ngraph.model.demand.builder</code>): Builds demand sets from DSL definitions, expanding source/target patterns into concrete node groups.</p> <ul> <li>Deterministic expansion: source/target node lists sorted alphabetically; no randomization</li> <li>Supports <code>combine</code> mode (aggregate via pseudo nodes) and <code>pairwise</code> mode (individual (src,dst) pairs with volume split)</li> <li>Demands sorted by ascending priority before placement (lower value = higher priority)</li> <li>Placement uses SPF caching for simple policies (ECMP, WCMP, TE_WCMP_UNLIM), FlowPolicy for complex multi-flow policies</li> <li>Non-mutating: operates on Core flow graphs with exclusions; Network remains unmodified</li> </ul> <p>Failure Manager (<code>ngraph.analysis.failure_manager</code>): Applies a <code>FailurePolicy</code> to compute exclusion sets and runs analyses with those exclusions.</p> <ul> <li>Parallel execution via <code>ThreadPoolExecutor</code> with zero-copy network sharing across worker threads</li> <li>Deterministic results when seed is provided (each iteration derives <code>seed + iteration_index</code>)</li> <li>Optional baseline execution (no failures) for comparing degraded vs. intact capacity</li> <li>Automatic parallelism adjustment: Forces serial execution when analysis function defined in <code>__main__</code> (notebook context) to avoid pickling failures</li> <li>Thread-safe analysis: Network shared by reference; exclusion sets passed per-iteration</li> <li>Automatic graph pre-building: Before parallel iterations, builds <code>AnalysisContext</code> to amortize graph construction cost; per-iteration exclusions applied via O(|excluded|) mask operations</li> </ul> <p>Both the demand expansion logic and failure manager separate policy (how to expand demands or pick failures) from core algorithms. They prepare concrete inputs (expanded demands or exclusion sets) for each workflow iteration.</p>"},{"location":"reference/design/#workflow-engine-and-steps","title":"Workflow Engine and Steps","text":"<p>NetGraph workflows (see Workflow Reference) are essentially recipes of analysis steps to run in sequence. Each step is typically a pure function: it takes the current model and possibly prior results, performs an analysis, and stores its outputs. The workflow engine coordinates these steps, using a Results store to record data.</p> <p>Common built-in steps:</p> <ul> <li> <p>BuildGraph: validates network topology and stores node-link JSON representation via NetworkX <code>MultiDiGraph</code>. Stores graph structure under <code>data.graph</code> and parameters under <code>data.context</code>. Primarily for validation and export; Core graph building happens in analysis functions.</p> </li> <li> <p>NetworkStats: computes node/link counts, capacity statistics, cost statistics, and degree statistics. Supports optional <code>excluded_nodes</code>/<code>excluded_links</code> and <code>include_disabled</code>.</p> </li> <li> <p>TrafficMatrixPlacement: runs Monte Carlo placement using a named demand set and the Failure Manager. Supports <code>baseline</code>, <code>iterations</code>, <code>parallelism</code>, <code>placement_rounds</code>, <code>store_failure_patterns</code>, <code>include_flow_details</code>, <code>include_used_edges</code>, and <code>alpha</code> or <code>alpha_from_step</code> (default <code>data.alpha_star</code>). Produces <code>data.flow_results</code> per iteration.</p> </li> <li> <p>MaxFlow: runs Monte Carlo maximum-flow analysis between node groups using the Failure Manager. Supports <code>mode</code> (combine/pairwise), <code>baseline</code>, <code>iterations</code>, <code>parallelism</code>, <code>shortest_path</code>, <code>flow_placement</code>, and optional <code>include_flow_details</code>/<code>include_min_cut</code>. Produces <code>data.flow_results</code> per iteration.</p> </li> <li> <p>MaximumSupportedDemand (MSD): uses bracketing and bisection on alpha to find the maximum multiplier such that alpha * volume is feasible. Stores <code>data.alpha_star</code>, <code>data.context</code>, <code>data.base_demands</code>, and <code>data.probes</code>.</p> </li> <li> <p>CostPower: aggregates platform and per-end optics capex/power by hierarchy level (0..N). Respects <code>include_disabled</code> and <code>aggregation_level</code>. Stores <code>data.levels</code> and <code>data.context</code>.</p> </li> </ul> <p>Each step is implemented in the code (in ngraph.workflow module) and has a corresponding <code>type</code> name. Steps are pure functions that don't modify the Network. They take inputs, often including references to prior steps' results (the workflow engine allows one step to use another step's output). For instance, a placement step might need the value of alpha* from an MSD step; the workflow definition can specify that link.</p>"},{"location":"reference/design/#results-storage","title":"Results storage","text":"<p>The Results object is a container that the workflow passes through steps. When a step runs, it \"enters\" a scope in the Results (by step name) and writes any outputs to either metadata or data within that scope.</p> <p>For example, the MaxFlow step named \"maxflow_between_metros\" will put the total flow and details under <code>results.steps[\"maxflow_between_metros\"][\"data\"]</code> and perhaps record parameters in metadata. The Results store also captures each step's execution metadata (like step order, type, seeds) in a workflow registry. At the end of the workflow, a single nested dictionary can be exported via Results.to_dict() containing all step outputs in a structured way.</p> <p>This design ensures consistency (every step has metadata and data keys) and JSON serialization (handles custom objects via to_dict() when available, converts keys to strings). The results often include artifacts like tables or lists of flows for reporting.</p>"},{"location":"reference/design/#design-elements-and-comparisons","title":"Design Elements and Comparisons","text":"<p>NetGraph's design includes several features that differentiate it from traditional network analysis tools:</p> <ul> <li> <p>Declarative Scenario DSL: A YAML DSL with blueprints and programmatic expansion allows abstract definitions (e.g., a fully meshed Clos) to be expanded into concrete nodes and links. Strict schema validation ensures that scenarios are well-formed and rejects unknown or invalid fields.</p> </li> <li> <p>Runtime Exclusions vs graph copying: Analysis-time exclusions avoid copying large structures for each scenario. The design separates static topology from dynamic failure states.</p> </li> <li> <p>Stable edge IDs: Links have auto-generated unique IDs (<code>source|target|&lt;base64_uuid&gt;</code>) that remain stable throughout analysis, simplifying correlation of results to original links.</p> </li> <li> <p>Dual routing semantics: Models both IP/IGP (cost-only, fixed paths via <code>require_capacity=false</code> + <code>shortest_path=true</code>) and SDN/TE (capacity-aware, iterative via <code>require_capacity=true</code> + <code>shortest_path=false</code>)</p> </li> <li> <p>Configurable flow placement: Proportional (WCMP-style, capacity-weighted) and Equal-Balanced (ECMP-style, uniform) splitting across parallel equal-cost edges</p> </li> <li> <p>Cost-aware augmentation: Prefer cheapest capacity first via successive shortest paths. Does not re-route previously placed flow.</p> </li> <li> <p>Deterministic simulation with seeding: Random aspects (e.g., failure sampling) are controlled by explicit seeds that propagate through steps. Runs are reproducible given the same scenario and seed.</p> </li> <li> <p>Structured results store: Collects results with metadata in a consistent format for JSON export and downstream analysis.</p> </li> </ul>"},{"location":"reference/design/#performance-considerations","title":"Performance Considerations","text":"<p>C++ Algorithm Implementation:</p> <ul> <li>Native C++ execution with optimized data structures (CSR adjacency, flat arrays)</li> <li>GIL released during algorithm execution, enabling concurrent analysis across Python threads</li> <li>Zero-copy NumPy integration for array inputs/outputs (via buffer protocol)</li> <li>Deterministic edge ordering for reproducible results</li> <li>Cache-friendly CSR representation for efficient neighbor traversal</li> </ul> <p>Graph Building and Reuse:</p> <p>For Monte Carlo analysis with many failure iterations, graph construction is amortized via <code>AnalysisContext</code>:</p> <ul> <li>Context built once before iterations begin (includes all nodes and augmentation edges)</li> <li>Per-iteration exclusions applied via boolean masks rather than graph rebuilding</li> <li>Mask building is O(|excluded|) using pre-computed <code>link_id_to_edge_indices</code> mapping</li> <li>FailureManager automatically pre-builds the <code>AnalysisContext</code> before parallel execution</li> </ul> <p>This optimization is critical for performance: graph construction involves Python processing, NumPy array creation, and C++ object initialization. Building the graph once eliminates this overhead from the per-iteration critical path, enabling the GIL-releasing C++ algorithms to execute with minimal Python overhead.</p> <p>SPF Caching for Demand Placement:</p> <p>Both TrafficMatrixPlacement and MaximumSupportedDemand (MSD) use a unified placement function (<code>place_demands()</code> in <code>ngraph.analysis.placement</code>) with SPF caching for cacheable policies (ECMP, WCMP, TE_WCMP_UNLIM):</p> <ul> <li>Initial SPF computed once per unique source; subsequent demands from the same source reuse the cached DAG</li> <li>For TE policies, DAG is recomputed when capacity constraints require alternate paths</li> <li>Complex multi-flow policies (TE_ECMP_16_LSP, TE_ECMP_UP_TO_256_LSP) use FlowPolicy directly</li> <li>MSD additionally pre-resolves node IDs once at cache build time and reuses them across all alpha probes</li> </ul> <p>This reduces SPF computations from O(demands) to O(unique_sources) for workloads where many demands share the same source nodes. For MSD, the optimization is particularly significant since it evaluates many alpha values during binary search.</p> <p>Monte Carlo Deduplication:</p> <p>FailureManager collapses identical failure patterns into single executions. Runtime scales with unique patterns U rather than requested iterations I; often U &lt;&lt; I for common failure policies.</p> <p>Complexity:</p> <ul> <li>SPF: (O((V+E) \\log V)) using binary heap</li> <li>Max-flow: (O(V^2 E \\log V)) worst-case for successive shortest paths with blocking flow</li> <li>Practical performance dominated by (O(F \\cdot (V+E) \\log V)) for integer capacities where (F) is max-flow value</li> <li>Early termination when residual capacity exhausts provides significant speedup in typical networks</li> </ul> <p>Scalability:</p> <p>Benchmarks on structured topologies (Clos, grid) and realistic network graphs demonstrate scalability to networks with thousands of nodes and tens of thousands of edges. C++ execution with CSR adjacency and GIL release provides order-of-magnitude speedups over pure Python graph libraries for compute-intensive analysis.</p>"},{"location":"reference/design/#summary","title":"Summary","text":"<p>NetGraph's hybrid architecture combines:</p> <p>Python Layer:</p> <ul> <li>Declarative scenario DSL with schema validation</li> <li>Domain model (Network, Node, Link, RiskGroup)</li> <li>Runtime exclusions for non-destructive failure simulation</li> <li>Workflow orchestration and result aggregation</li> <li>Managers for demand expansion and failure enumeration</li> </ul> <p>C++ Layer:</p> <ul> <li>High-performance graph algorithms (SPF, K-shortest paths, max-flow)</li> <li>Immutable StrictMultiDiGraph with CSR adjacency</li> <li>Configurable flow placement policies (ECMP/WCMP simulation)</li> <li>Runtime masking for efficient repeated analysis</li> </ul> <p>Integration:</p> <ul> <li><code>AnalysisContext</code> builds Core graphs, manages name/ID mapping, and bridges Python \u2194 C++</li> <li>Stable node/edge ID mapping for result traceability</li> <li>NumPy array interface for efficient data transfer</li> <li>GIL release during computation for concurrent thread execution</li> </ul> <p>This design adapts standard algorithms to network engineering use cases (flow splitting, failure simulation, cost-aware routing) while achieving high performance through native C++ execution and ergonomic interfaces through Python APIs.</p>"},{"location":"reference/design/#cross-references","title":"Cross-references","text":"<ul> <li>DSL Reference</li> <li>Workflow Reference</li> <li>CLI Reference</li> <li>API Reference</li> <li>Auto-Generated API Reference</li> </ul>"},{"location":"reference/dsl/","title":"Domain-Specific Language (DSL)","text":"<p>Quick links:</p> <ul> <li>Design \u2014 architecture, model, algorithms, workflow</li> <li>Workflow Reference \u2014 analysis workflow configuration and execution</li> <li>CLI Reference \u2014 command-line tools for running scenarios</li> <li>API Reference \u2014 Python API for programmatic scenario creation</li> <li>Auto-Generated API Reference \u2014 complete class and method documentation</li> </ul> <p>This document describes the DSL for defining network scenarios in NetGraph. Scenarios are YAML files that describe network topology, traffic demands, and analysis workflows.</p>"},{"location":"reference/dsl/#overview","title":"Overview","text":"<p>A scenario file defines a complete network simulation including:</p> <ul> <li>Network topology: Nodes, links, and their relationships, as well as risk groups</li> <li>Analysis configuration: Traffic demands, failure policies, workflows</li> <li>Reusable components: Blueprints, hardware definitions</li> </ul> <p>The DSL enables both simple direct definitions and complex hierarchical structures with templates and parameters.</p>"},{"location":"reference/dsl/#template-syntaxes","title":"Template Syntaxes","text":"<p>The DSL uses three distinct template syntaxes in different contexts:</p> Syntax Example Context Purpose <code>[1-3]</code> <code>dc[1-3]/rack[a,b]</code> Node/risk group names Generate multiple groups <code>$var</code> / <code>${var}</code> <code>pod${p}/leaf</code> Links, rules, demands Template expansion with <code>expand</code> block <code>{n}</code> <code>srv-{n}</code> <code>template</code> field Node naming (1-indexed counter) <p>These syntaxes are not interchangeable. Each works only in its designated context.</p> <p>Why different syntaxes? Each serves a distinct purpose:</p> Syntax Operation Key Difference <code>[1-3]</code> Static generation Creates multiple definitions at parse time <code>${var}</code> Template substitution Requires explicit <code>expand</code> block with <code>vars</code> <code>{n}</code> Sequential counter Auto-increments based on <code>count</code> <p>Bracket expansion generates structure; variable expansion parameterizes rules; node naming indexes instances.</p>"},{"location":"reference/dsl/#entity-creation-architecture","title":"Entity Creation Architecture","text":"<p>The DSL implements two fundamentally different selection patterns optimized for different use cases. Understanding these patterns is essential for effective scenario authoring.</p>"},{"location":"reference/dsl/#two-selection-models","title":"Two Selection Models","text":"<p>The DSL uses distinct selection strategies depending on the operation:</p> <p>1. Path-Based Node Selection (link rules, traffic demands, workflow steps)</p> <ul> <li>Uses regex patterns on hierarchical node names</li> <li>Supports capture group-based grouping</li> <li>Supports attribute-based grouping (<code>group_by</code>)</li> <li>Supports attribute filtering (<code>match</code> conditions)</li> <li>Supports <code>active_only</code> filtering</li> </ul> <p>2. Condition-Based Entity Selection (failure rules, membership rules, risk group generation)</p> <ul> <li>Works on nodes, links, or risk_groups (<code>scope</code>)</li> <li>Supports attribute-based filtering (<code>conditions</code>)</li> <li>Supports optional <code>path</code> regex filtering to narrow candidates before condition matching</li> </ul> <p>These patterns share common primitives (condition evaluation, match specification) but serve different purposes and should not be confused.</p>"},{"location":"reference/dsl/#link-creation-flow","title":"Link Creation Flow","text":"<p>Link definitions create links between nodes using path-based selection with optional filtering:</p> <pre><code>flowchart TD\n    Start[Link Definition] --&gt; VarExpand{Has expand block?}\n    VarExpand --&gt;|Yes| VarSubst[Variable Substitution]\n    VarSubst --&gt; PathFilter\n    VarExpand --&gt;|No| PathFilter[1. Path-Based Selection]\n    PathFilter --&gt; PathDesc[Select nodes via regex pattern&lt;br/&gt;Groups by capture groups]\n    PathDesc --&gt; MatchFilter{Has match conditions?}\n    MatchFilter --&gt;|Yes| AttrFilter[2. Attribute Filtering]\n    MatchFilter --&gt;|No| ActiveFilter\n    AttrFilter --&gt; AttrDesc[Filter by attribute conditions&lt;br/&gt;using logic and/or]\n    AttrDesc --&gt; ActiveFilter[3. Active/Excluded Filtering]\n    ActiveFilter --&gt; GroupBy{Has group_by?}\n    GroupBy --&gt;|Yes| Regroup[4. Re-group by Attribute]\n    GroupBy --&gt;|No| Pattern\n    Regroup --&gt; Pattern[5. Apply Pattern]\n    Pattern --&gt; PatternDesc[mesh or one_to_one&lt;br/&gt;Creates links between groups]\n</code></pre> <p>Processing Steps:</p> <ol> <li>Path Selection: Regex pattern matches nodes by hierarchical name    - Capture groups create initial grouping    - If no path specified, selects all nodes</li> <li>Attribute Filtering: Optional <code>match</code> conditions filter nodes    - Uses <code>logic: \"and\"</code> or <code>\"or\"</code> (default: <code>\"or\"</code>)    - Supports operators: <code>==</code>, <code>!=</code>, <code>&lt;</code>, <code>&gt;</code>, <code>contains</code>, <code>in</code>, etc.</li> <li>Active Filtering: Filters disabled nodes based on context    - Links default: <code>active_only=false</code> (creates links to disabled nodes)</li> <li>Attribute Grouping: Optional <code>group_by</code> overrides regex capture grouping</li> <li>Pattern Application: Creates links between selected node groups    - <code>mesh</code>: Every source to every target    - <code>one_to_one</code>: Pairwise with wrap-around</li> </ol> <p>Key Characteristics:</p> <ul> <li><code>default_active_only=False</code> (links are created to disabled nodes)</li> <li><code>match.logic</code> defaults to <code>\"or\"</code> (inclusive matching)</li> <li>Supports variable expansion via <code>expand</code> block</li> </ul>"},{"location":"reference/dsl/#traffic-demand-creation-flow","title":"Traffic Demand Creation Flow","text":"<p>Traffic demands follow a similar pattern but with important differences:</p> <pre><code>flowchart TD\n    Start[Traffic Demand Spec] --&gt; VarExpand{Has expand block?}\n    VarExpand --&gt;|Yes| VarSubst[Variable Substitution&lt;br/&gt;Creates multiple demand specs]\n    VarSubst --&gt; Process\n    VarExpand --&gt;|No| Process[Process Single Demand]\n    Process --&gt; SrcSelect[1. Select Source Nodes]\n    SrcSelect --&gt; TgtSelect[2. Select Target Nodes]\n    TgtSelect --&gt; SrcDesc[Uses same path + match + group_by&lt;br/&gt;selection as links]\n    SrcDesc --&gt; Mode{Demand Mode?}\n    Mode --&gt;|pairwise| Pairwise[3a. Pairwise Expansion]\n    Mode --&gt;|combine| Combine[3b. Combine Expansion]\n    Pairwise --&gt; PairDesc[Create demand for each src-tgt pair&lt;br/&gt;Volume distributed evenly&lt;br/&gt;No pseudo nodes]\n    Combine --&gt; CombDesc[Create pseudo-source and pseudo-target&lt;br/&gt;Single aggregated demand&lt;br/&gt;Augmentation edges connect real nodes]\n</code></pre> <p>Key Differences from Links:</p> <ol> <li>Active-only default: <code>default_active_only=True</code> (only active nodes participate)</li> <li>Two selection phases: Source nodes first, then target nodes (both use same selector logic)</li> <li>Expansion modes:    - Pairwise: Creates individual demands for each (source, target) pair    - Combine: Creates pseudo nodes and a single aggregated demand</li> <li>Group modes: Additional layer (<code>flatten</code>, <code>per_group</code>, <code>group_pairwise</code>) for handling grouped selections</li> </ol> <p>Processing Steps:</p> <ol> <li>Select source nodes using unified selector (path + match + group_by)</li> <li>Select target nodes using unified selector</li> <li>Apply mode-specific expansion:    - Pairwise: Volume evenly distributed across all pairs    - Combine: Single demand with pseudo nodes for aggregation</li> </ol>"},{"location":"reference/dsl/#risk-group-creation-flow","title":"Risk Group Creation Flow","text":"<p>Risk groups use the condition-based selection model:</p> <pre><code>flowchart TD\n    Start[Risk Groups Definition] --&gt; Three[Three Creation Methods]\n    Three --&gt; Direct[1. Direct Definition]\n    Three --&gt; Member[2. Membership Rules]\n    Three --&gt; Generate[3. Generate Blocks]\n\n    Direct --&gt; DirectDesc[Simply name the risk group&lt;br/&gt;Entities reference it explicitly]\n\n    Member --&gt; MemberScope[Specify scope&lt;br/&gt;node, link, or risk_group]\n    MemberScope --&gt; MemberCond[Define match conditions&lt;br/&gt;logic defaults to and]\n    MemberCond --&gt; MemberExec[Scan ALL entities of that scope&lt;br/&gt;Add matching entities to risk group]\n\n    Generate --&gt; GenScope[Specify scope&lt;br/&gt;node or link only]\n    GenScope --&gt; GenGroupBy[Specify group_by attribute]\n    GenGroupBy --&gt; GenExec[Collect unique values&lt;br/&gt;Create risk group for each value&lt;br/&gt;Add entities with that value]\n</code></pre> <p>Creation Methods:</p> <ol> <li>Direct Definition: Explicitly name risk groups, entities reference them</li> <li>Membership Rules: Auto-assign entities based on attribute matching</li> <li>Generate Blocks: Auto-create risk groups from unique attribute values</li> </ol> <p>Key Characteristics:</p> <ul> <li>No path patterns: Operates on ALL entities of specified scope</li> <li>Only attribute-based: Uses <code>conditions</code> exclusively</li> <li>Logic defaults to \"and\" for membership (stricter matching)</li> <li>Hierarchical support: Risk groups can contain other risk groups as children</li> </ul>"},{"location":"reference/dsl/#comparison-table","title":"Comparison Table","text":"Feature Links Traffic Demands Risk Groups Selection Type Path-based Path-based Condition-based Regex Patterns Yes Yes Yes (optional) Capture Groups Yes Yes No <code>group_by</code> Yes Yes Yes (generate only) <code>match</code> Conditions Yes Yes Yes (membership/generate) <code>active_only</code> Default False True N/A <code>match.logic</code> Default \"or\" \"or\" \"and\" (membership) Variable Expansion Yes Yes No Entity Scope Nodes only Nodes only Nodes, links, risk_groups"},{"location":"reference/dsl/#shared-evaluation-primitives","title":"Shared Evaluation Primitives","text":"<p>All selection mechanisms share common evaluation primitives:</p> <p>1. Condition Structure</p> <p>Each condition has three fields:</p> <pre><code>conditions:\n  - attr: \"role\"           # Attribute name (supports dot-notation)\n    op: \"==\"               # Operator\n    value: \"leaf\"          # Expected value\n</code></pre> <p>2. Condition Operators</p> Operator Description Example <code>==</code> Equals <code>attr: \"role\", op: \"==\", value: \"leaf\"</code> <code>!=</code> Not equals <code>attr: \"tier\", op: \"!=\", value: 1</code> <code>&lt;</code> Less than (numeric) <code>attr: \"cost\", op: \"&lt;\", value: 100</code> <code>&lt;=</code> Less than or equal <code>attr: \"priority\", op: \"&lt;=\", value: 5</code> <code>&gt;</code> Greater than (numeric) <code>attr: \"capacity\", op: \"&gt;\", value: 1000</code> <code>&gt;=</code> Greater than or equal <code>attr: \"tier\", op: \"&gt;=\", value: 2</code> <code>contains</code> String contains or collection includes <code>attr: \"name\", op: \"contains\", value: \"spine\"</code> <code>not_contains</code> String/collection does not include <code>attr: \"tags\", op: \"not_contains\", value: \"deprecated\"</code> <code>in</code> Value is in provided list <code>attr: \"role\", op: \"in\", value: [\"leaf\", \"spine\"]</code> <code>not_in</code> Value is not in provided list <code>attr: \"dc\", op: \"not_in\", value: [\"dc3\", \"dc4\"]</code> <code>exists</code> Attribute exists and is not null <code>attr: \"hardware.vendor\", op: \"exists\"</code> <code>not_exists</code> Attribute missing or null <code>attr: \"deprecated\", op: \"not_exists\"</code> <p>3. Condition Combining (<code>logic</code>)</p> <ul> <li><code>\"or\"</code> (default in most contexts): Any condition must match</li> <li><code>\"and\"</code>: All conditions must match</li> </ul> <pre><code>match:\n  logic: \"and\"\n  conditions:\n    - attr: \"role\", op: \"==\", value: \"leaf\"\n    - attr: \"tier\", op: \"&gt;=\", value: 2\n</code></pre> <p>4. Attribute Access</p> <p>Conditions evaluate against a flattened view of entity attributes: - Top-level fields: <code>name</code>, <code>disabled</code>, <code>capacity</code>, <code>cost</code>, <code>risk_groups</code> - Custom attributes from <code>attrs</code> block</p> <p>5. Dot-Notation for Nested Attributes</p> <p>Access nested attributes using dots:</p> <pre><code>conditions:\n  - attr: \"hardware.vendor\"      # Resolves to attrs[\"hardware\"][\"vendor\"]\n    op: \"==\"\n    value: \"Acme\"\n</code></pre> <p>6. Variable Expansion</p> <p>Template expansion (<code>$var</code>, <code>${var}</code>) is processed before condition evaluation (see Variable Expansion).</p>"},{"location":"reference/dsl/#context-aware-defaults","title":"Context-Aware Defaults","text":"<p>The DSL uses context-aware defaults to optimize for common use cases:</p> Context Selection Type Active Only Match Logic Rationale Links Path-based False \"or\" Create links to all nodes, including disabled Demands Path-based True \"or\" Only route traffic through active nodes Node Rules Path-based False \"or\" Modify all matching nodes Workflow Steps Path-based True \"or\" Analyze only active topology Membership Rules Condition-based N/A \"and\" Precise matching for risk assignment Failure Rules Condition-based N/A \"or\" Inclusive matching for failure scenarios Generate Blocks Condition-based N/A N/A No conditions, groups by values <p>These defaults ensure intuitive behavior while remaining overridable when needed.</p>"},{"location":"reference/dsl/#top-level-keys","title":"Top-Level Keys","text":"<pre><code>network:                 # Network topology (required)\nblueprints:              # Reusable network templates\ncomponents:              # Hardware component library\nrisk_groups:             # Failure correlation groups\nvars:                    # YAML anchors and variables for reuse\ndemands:                 # Traffic demand definitions\nfailures:                # Failure simulation policies\nworkflow:                # Analysis execution steps\nseed:                    # Master seed for reproducibility (integer)\n</code></pre> Key Required Description <code>network</code> Yes Network topology: nodes, links, and rules <code>blueprints</code> No Reusable topology templates <code>components</code> No Hardware component library for cost/power modeling <code>risk_groups</code> No Failure correlation groups for resilience analysis <code>vars</code> No YAML anchors for reuse within the scenario <code>demands</code> No Traffic demand patterns for capacity analysis <code>failures</code> No Failure policies for simulation <code>workflow</code> No Analysis workflow steps to execute <code>seed</code> No Master seed (integer) for reproducible random operations <p>Seed: When specified, the <code>seed</code> value is used to derive deterministic per-component seeds (via SHA-256 hashing) for failure sampling and workflow steps, ensuring reproducible results across runs. Each failure iteration creates a single isolated random number generator from its derived seed. Without a seed, results may vary between executions.</p>"},{"location":"reference/dsl/#network-core-foundation","title":"<code>network</code> - Core Foundation","text":"<p>The only required section. Defines network topology through nodes and links.</p> <p>Network metadata fields:</p> <pre><code>network:\n  name: \"my-network\"       # Optional network name (stored in network.attrs)\n  version: \"1.0\"           # Optional version (stored in network.attrs)\n  nodes: { ... }\n  links: [ ... ]\n</code></pre>"},{"location":"reference/dsl/#direct-node-and-link-definitions","title":"Direct Node and Link Definitions","text":"<p>Individual Nodes:</p> <pre><code>network:\n  nodes:\n    SEA:\n      disabled: true\n      attrs:\n        coords: [47.6062, -122.3321]\n        hardware:\n          component: \"LeafRouter\"\n          count: 1\n    SFO:\n      attrs:\n        coords: [37.7749, -122.4194]\n        hardware:\n          component: \"SpineRouter\"\n          count: 1\n</code></pre> <p>Recognized keys for each node entry:</p> <ul> <li><code>disabled</code>: boolean (optional)</li> <li><code>attrs</code>: mapping of attributes (optional)</li> <li><code>risk_groups</code>: list of risk-group names (optional)</li> </ul> <p>Individual Links:</p> <pre><code>network:\n  links:\n    - source: SEA\n      target: SFO\n      capacity: 200\n      cost: 6846\n      attrs:\n        distance_km: 1369.13\n        media_type: \"fiber\"\n        hardware:\n          source: {component: \"800G-ZR+\", count: 1}\n          target: {component: \"1600G-2xDR4\", count: 1}\n</code></pre> <p>Recognized keys for each link entry:</p> <ul> <li><code>source</code>, <code>target</code>: node names (required)</li> <li><code>capacity</code>: link capacity (optional; default 1.0)</li> <li><code>cost</code>: link cost (optional; default 1.0)</li> <li><code>disabled</code>: boolean (optional)</li> <li><code>risk_groups</code>: list of risk-group names (optional)</li> <li><code>attrs</code>: mapping of attributes (optional)</li> <li><code>count</code>: integer number of parallel links to create (optional; default 1)</li> </ul>"},{"location":"reference/dsl/#node-groups","title":"Node Groups","text":"<p>Node Groups with Count/Template:</p> <pre><code>network:\n  nodes:\n    leaf:\n      count: 4\n      template: \"leaf-{n}\"\n      attrs:\n        role: \"leaf\"\n    spine:\n      count: 2\n      template: \"spine-{n}\"\n      attrs:\n        role: \"spine\"\n</code></pre> <p>Creates: <code>leaf/leaf-1</code>, <code>leaf/leaf-2</code>, <code>leaf/leaf-3</code>, <code>leaf/leaf-4</code>, <code>spine/spine-1</code>, <code>spine/spine-2</code></p> <p>The <code>{n}</code> placeholder is replaced with a 1-indexed counter (1, 2, 3, ...) up to <code>count</code>. The group name becomes the parent path, and template generates child node names.</p> <p>Nested Nodes (Inline Hierarchy):</p> <p>Create hierarchical node structures without blueprints using nested <code>nodes</code>:</p> <pre><code>network:\n  nodes:\n    dc1:\n      nodes:                    # Inline nested hierarchy\n        rack1:\n          count: 4\n          template: \"srv-{n}\"\n          attrs:\n            role: \"server\"\n        rack2:\n          count: 4\n          template: \"srv-{n}\"\n          attrs:\n            role: \"server\"\n        tor:\n          count: 2\n          template: \"tor-{n}\"\n          attrs:\n            role: \"switch\"\n</code></pre> <p>Creates: <code>dc1/rack1/srv-1</code>, <code>dc1/rack1/srv-2</code>, ..., <code>dc1/tor/tor-1</code>, <code>dc1/tor/tor-2</code></p> <p>Nested nodes are useful for creating simple hierarchies without defining reusable blueprints. For complex or repeated structures, prefer blueprints.</p> <p>Link Definitions:</p> <pre><code>network:\n  links:\n    - source: /leaf\n      target: /spine\n      pattern: \"mesh\"           # Connect every leaf to every spine\n      capacity: 3200\n      cost: 1\n    - source: /spine\n      target: /spine\n      pattern: \"one_to_one\"     # Connect spines pairwise\n      count: 2                   # Create 2 parallel links per pair (optional)\n      capacity: 1600\n      cost: 1\n      attrs:\n        hardware:\n          source: {component: \"800G-DR4\", count: 2}\n          target: {component: \"800G-DR4\", count: 2}\n</code></pre>"},{"location":"reference/dsl/#attribute-filtered-links-selector-objects","title":"Attribute-filtered Links (selector objects)","text":"<p>You can filter the source or target node sets by attributes using the same condition syntax as failure policies. Replace a string <code>source</code>/<code>target</code> with an object that has <code>path</code> and optional <code>match</code>:</p> <pre><code>network:\n  links:\n    - source:\n        path: \"/leaf\"\n        match:\n          logic: \"and\"         # default: \"or\"\n          conditions:\n            - attr: \"role\"\n              op: \"==\"\n              value: \"leaf\"\n      target:\n        path: \"/spine\"\n        match:\n          conditions:\n            - attr: \"role\"\n              op: \"==\"\n              value: \"spine\"\n      pattern: \"mesh\"\n      capacity: 100\n      cost: 1\n</code></pre> <p>Notes:</p> <ul> <li><code>path</code> is a regex pattern matched against node names (anchored at start via Python <code>re.match</code>).</li> <li><code>match.conditions</code> uses the shared condition operators: <code>==</code>, <code>!=</code>, <code>&lt;</code>, <code>&lt;=</code>, <code>&gt;</code>, <code>&gt;=</code>, <code>contains</code>, <code>not_contains</code>, <code>exists</code>, <code>not_exists</code>.</li> <li>Conditions evaluate over a flat view of node attributes combining top-level fields (<code>name</code>, <code>disabled</code>, <code>risk_groups</code>) and <code>node.attrs</code>.</li> <li><code>logic</code> in the <code>match</code> block accepts \"and\" or \"or\" (default \"or\").</li> <li>Selectors filter node candidates before the link <code>pattern</code> is applied.</li> <li>Cross-endpoint predicates (e.g., comparing a source attribute to a target attribute) are not supported.</li> <li>Node rules run before link expansion; link rules run after link creation.</li> </ul> <p>Path semantics:</p> <ul> <li>All paths are relative to the current scope. There is no concept of absolute paths.</li> <li>Leading <code>/</code> is stripped and has no functional effect - <code>/leaf</code> and <code>leaf</code> are equivalent.</li> <li>Within a blueprint, paths resolve relative to the instantiation path. For example, if a blueprint is used under group <code>pod1</code>, then <code>source: /leaf</code> resolves to <code>pod1/leaf</code>.</li> <li>At top-level <code>network.links</code>, the parent path is empty, so patterns match against full node names.</li> </ul> <p>Example with OR logic to match multiple roles:</p> <pre><code>network:\n  links:\n    - source:\n        path: \"/metro1/dc[1-1]\"\n        match:\n          conditions:\n            - attr: \"role\"\n              op: \"==\"\n              value: \"dc\"\n      target:\n        path: \"/metro1/pop[1-2]\"\n        match:\n          logic: \"or\"\n          conditions:\n            - attr: \"role\"\n              op: \"==\"\n              value: \"leaf\"\n            - attr: \"role\"\n              op: \"==\"\n              value: \"core\"\n      pattern: \"mesh\"\n</code></pre> <p>Connectivity Patterns:</p> <ul> <li><code>mesh</code>: Full connectivity between all source and target nodes</li> <li><code>one_to_one</code>: Pairwise connections. Compatible sizes means max(|S|,|T|) must be an integer multiple of min(|S|,|T|); mapping wraps modulo the smaller set (e.g., 4x2 and 6x3 valid; 3x2 invalid).</li> </ul>"},{"location":"reference/dsl/#bracket-expansion","title":"Bracket Expansion","text":"<p>Create multiple similar node groups using bracket notation:</p> <pre><code>network:\n  nodes:\n    dc[1-3]/rack[a,b]:     # Creates dc1/racka, dc1/rackb, dc2/racka, etc.\n      count: 4\n      template: \"srv-{n}\"\n</code></pre> <p>Expansion Types:</p> <ul> <li>Numeric ranges: <code>[1-4]</code> -&gt; 1, 2, 3, 4</li> <li>Explicit lists: <code>[red,blue,green]</code> -&gt; red, blue, green</li> <li>Mixed expressions: <code>[1,3,5-7]</code> -&gt; 1, 3, 5, 6, 7</li> <li>Multiple brackets: Cartesian product of all brackets</li> </ul> <pre><code># Multiple brackets produce cartesian product\ndc[1-2]/rack[a,b]:   # Creates: dc1/racka, dc1/rackb, dc2/racka, dc2/rackb\n</code></pre> <p>Scope: Bracket expansion applies to:</p> <ul> <li>Node group names under <code>network.nodes</code> and <code>blueprints.*.nodes</code></li> <li>Risk group names in top-level <code>risk_groups</code> definitions (including children)</li> <li>Risk group membership arrays on nodes, links, and node groups</li> </ul> <p>Component names, direct node names (<code>network.nodes</code> without count/template), and other string fields treat brackets as literal characters.</p> <p>Risk Group Expansion Examples:</p> <pre><code># Definition expansion - creates DC1_Power, DC2_Power, DC3_Power\nrisk_groups:\n  - name: \"DC[1-3]_Power\"\n\n# Membership expansion - assigns to RG1, RG2, RG3\nnetwork:\n  nodes:\n    Server:\n      risk_groups: [\"RG[1-3]\"]\n</code></pre> <p>Limitations and Workarounds:</p> Pattern Behavior Workaround <code>[01-03]</code> Produces <code>1, 2, 3</code> (no leading zeros) Use explicit list: <code>[01,02,03]</code> <code>[A-C]</code> Error (letter ranges not supported) Use explicit list: <code>[A,B,C]</code> <code>[1-10]</code> Produces <code>1, 2, ..., 10</code> Works correctly <p>The range syntax <code>[start-end]</code> only supports integers. For letters, mixed sequences, or zero-padded numbers, use comma-separated explicit lists.</p>"},{"location":"reference/dsl/#variable-expansion","title":"Variable Expansion","text":"<p>Use <code>$var</code> or <code>${var}</code> syntax with an <code>expand</code> block for template substitution. Variables are recursively substituted in all string fields within the block, including nested <code>attrs</code>.</p> <p>Supported contexts:</p> <ul> <li>Link definitions (<code>network.links</code>)</li> <li>Link rules (<code>network.link_rules</code>)</li> <li>Node rules (<code>network.node_rules</code>)</li> <li>Traffic demands (<code>demands.*</code>)</li> </ul> <p>Expansion modes:</p> Mode Behavior Example <code>cartesian</code> (default) All combinations of variable values <code>p:[1,2]</code>, <code>r:[a,b]</code> \u2192 4 expansions <code>zip</code> Pair values by index (lists must have equal length) <code>a:[1,2]</code>, <code>b:[x,y]</code> \u2192 2 expansions <p>Example in links:</p> <pre><code>links:\n  - source: \"plane${p}/rack${r}\"\n    target: \"spine${s}\"\n    expand:\n      vars:\n        p: [1, 2]\n        r: [\"a\", \"b\"]\n        s: [1, 2, 3]\n      mode: \"cartesian\"  # 2 \u00d7 2 \u00d7 3 = 12 link definitions\n    pattern: \"mesh\"\n\n  - source: \"server${idx}\"\n    target: \"switch${idx}\"\n    expand:\n      vars:\n        idx: [1, 2, 3, 4]\n      mode: \"zip\"        # 4 paired link definitions\n    pattern: \"one_to_one\"\n</code></pre> <p>Variables in nested attributes:</p> <pre><code>links:\n  - source: \"${dc}/leaf\"\n    target: \"${dc}/spine\"\n    expand:\n      vars:\n        dc: [\"dc1\", \"dc2\"]\n    attrs:\n      datacenter: \"${dc}\"      # Also substituted\n      corridor: \"${dc}_internal\"\n    pattern: \"mesh\"\n</code></pre> <p>Limits: Expansion is capped at 10,000 items per block to prevent accidental combinatorial explosion.</p>"},{"location":"reference/dsl/#blueprints-reusable-templates","title":"<code>blueprints</code> - Reusable Templates","text":"<p>Templates for network segments that can be instantiated multiple times:</p> <pre><code>blueprints:\n  leaf_spine:\n    nodes:\n      leaf:\n        count: 4\n        template: \"leaf-{n}\"\n      spine:\n        count: 2\n        template: \"spine-{n}\"\n    links:\n      - source: /leaf\n        target: /spine\n        pattern: mesh\n        capacity: 40\n        cost: 1\n\nnetwork:\n  nodes:\n    pod1:\n      blueprint: leaf_spine\n    pod2:\n      blueprint: leaf_spine\n      params:                    # Override blueprint parameters\n        leaf.count: 6\n        spine.template: \"core-{n}\"\n</code></pre> <p>Blueprint Features:</p> <ul> <li>Define nodes and link rules once, reuse multiple times</li> <li>Override parameters using dot notation during instantiation</li> <li>Hierarchical naming: <code>pod1/leaf/leaf-1</code>, <code>pod2/spine/core-1</code></li> </ul>"},{"location":"reference/dsl/#node-and-link-rules","title":"Node and Link Rules","text":"<p>Modify specific nodes or links after initial creation. Rules run post-expansion and can override properties, add attributes, or disable elements.</p>"},{"location":"reference/dsl/#node-rules","title":"Node Rules","text":"<pre><code>network:\n  node_rules:\n    # Simple path-based matching\n    - path: \"^pod1/spine/.*$\"\n      disabled: true\n      attrs:\n        maintenance_mode: \"active\"\n\n    # Path with attribute filtering\n    - path: \"^dc1/.*\"\n      match:\n        logic: \"and\"\n        conditions:\n          - attr: \"role\"\n            op: \"==\"\n            value: \"leaf\"\n          - attr: \"tier\"\n            op: \"&gt;=\"\n            value: 2\n      attrs:\n        priority: \"high\"\n\n    # Variable expansion for repeated rules\n    - path: \"^${dc}/rack1/.*\"\n      expand:\n        vars:\n          dc: [\"dc1\", \"dc2\", \"dc3\"]\n      attrs:\n        zone: \"${dc}_zone1\"\n</code></pre> <p>Node rule fields:</p> <ul> <li><code>path</code>: Regex pattern matched against node names (required)</li> <li><code>match</code>: Optional attribute conditions to filter matched nodes</li> <li><code>disabled</code>: Set node disabled state</li> <li><code>attrs</code>: Attributes to merge into matched nodes</li> <li><code>risk_groups</code>: Risk groups to add to matched nodes</li> <li><code>expand</code>: Variable expansion block for templated rules</li> </ul>"},{"location":"reference/dsl/#link-rules","title":"Link Rules","text":"<pre><code>network:\n  link_rules:\n    # Basic endpoint matching\n    - source: \"^pod1/leaf/.*$\"\n      target: \"^pod1/spine/.*$\"\n      capacity: 100\n\n    # Bidirectional matching (default: true)\n    - source: \".*/spine/.*\"\n      target: \".*/spine/.*\"\n      bidirectional: true\n      cost: 5\n      attrs:\n        link_type: \"backbone\"\n\n    # Filter by link's own attributes using link_match\n    - source: \"^dc1/.*\"\n      target: \"^dc2/.*\"\n      link_match:\n        logic: \"and\"\n        conditions:\n          - attr: \"link_type\"\n            op: \"==\"\n            value: \"inter_dc\"\n          - attr: \"capacity\"\n            op: \"&gt;=\"\n            value: 100\n      cost: 10\n      attrs:\n        priority: \"high\"\n\n    # Variable expansion\n    - source: \"^${src}/.*\"\n      target: \"^${dst}/.*\"\n      expand:\n        vars:\n          src: [\"dc1\", \"dc2\"]\n          dst: [\"dc2\", \"dc3\"]\n        mode: \"zip\"\n      attrs:\n        corridor: \"${src}_to_${dst}\"\n</code></pre> <p>Link rule fields:</p> <ul> <li><code>source</code>, <code>target</code>: Regex patterns or selector objects for endpoint matching</li> <li><code>bidirectional</code>: Match links in both directions (default: <code>true</code>)</li> <li><code>link_match</code>: Filter by link's own attributes (not endpoint attributes)</li> <li><code>capacity</code>, <code>cost</code>, <code>disabled</code>: Override link properties</li> <li><code>attrs</code>: Attributes to merge into matched links</li> <li><code>risk_groups</code>: Risk groups to add to matched links</li> <li><code>expand</code>: Variable expansion block for templated rules</li> </ul> <p>Execution order:</p> <ol> <li><code>node_rules</code> run after node creation but before link expansion</li> <li><code>link_rules</code> run after all links are created</li> </ol>"},{"location":"reference/dsl/#components-hardware-library","title":"<code>components</code> - Hardware Library","text":"<p>Define hardware components with attributes for cost and power modeling:</p> <pre><code>components:\n  SpineRouter:\n    component_type: \"chassis\"\n    description: \"64-port spine router\"\n    capex: 50000.0\n    power_watts: 2500.0\n    power_watts_max: 3000.0\n    capacity: 64000.0           # Gbps\n    ports: 64\n    attrs:\n      vendor: \"VendorName\"\n      model: \"Model-9000\"\n    children:\n      LineCard400G:\n        component_type: \"linecard\"\n        capex: 8000.0\n        power_watts: 400.0\n        capacity: 12800.0\n        ports: 32\n        count: 4\n\n  Optic400G:\n    component_type: \"optic\"\n    description: \"400G pluggable optic\"\n    capex: 2500.0\n    power_watts: 12.0\n    capacity: 400.0\n    attrs:\n      reach: \"10km\"\n      wavelength: \"1310nm\"\n</code></pre> <p>Component Usage:</p> <pre><code>network:\n  nodes:\n    spine-1:\n      attrs:\n        hardware:\n          component: \"SpineRouter\"\n          count: 2   # Optional multiplier; defaults to 1 if not set\n  links:\n    - source: spine-1\n      target: leaf-1\n      attrs:\n        hardware:\n          source: {component: \"Optic400G\", count: 4}\n          target: {component: \"Optic400G\", count: 4}\n</code></pre>"},{"location":"reference/dsl/#risk_groups-risk-modeling","title":"<code>risk_groups</code> - Risk Modeling","text":"<p>Define hierarchical failure correlation groups for modeling correlated failures. Risk groups can represent any failure correlation pattern: physical infrastructure, geographic regions, vendor dependencies, or custom domains.</p>"},{"location":"reference/dsl/#understanding-hierarchy","title":"Understanding Hierarchy","text":"<p>Risk groups form parent-child trees that model cascading failures:</p> <pre><code>risk_groups:\n  - name: \"Region_West\"\n    children:\n      - name: \"Site_Seattle\"\n      - name: \"Site_Portland\"\n</code></pre> <p>Cascading semantics: When a parent fails, all descendants also fail. This models real-world correlations where a regional outage affects all sites in that region.</p> <p>Storage model: Children are nested within parents, not in the top-level dictionary:</p> <pre><code># Top-level only\nscenario.network.risk_groups.keys()  # {'Region_West'}\n\n# Access children via parent\nregion = scenario.network.risk_groups[\"Region_West\"]\nfor child in region.children:\n    print(child.name)  # Site_Seattle, Site_Portland\n</code></pre> <p>Entity references: Nodes and links reference risk groups by name. To reference a group, it must be defined at top level (children alone are not sufficient):</p> <pre><code>risk_groups:\n  - name: \"Site_Seattle\"      # Top-level definition enables references\n  - name: \"Region_West\"\n    children:\n      - name: \"Site_Seattle\"  # Also a child for hierarchy\n\nnetwork:\n  nodes:\n    Router_SEA:\n      risk_groups: [\"Site_Seattle\"]\n</code></pre>"},{"location":"reference/dsl/#common-use-cases","title":"Common Use Cases","text":"<p>Risk groups can model any failure correlation pattern. Below are some common examples:</p> <p>Physical Infrastructure (fiber paths, power zones, cooling systems) Geographic/Administrative (regions, availability zones, maintenance windows) Vendor/Software Dependencies (shared components, software versions) Logical Grouping (service tiers, customer segments, custom domains)</p>"},{"location":"reference/dsl/#example-1-physical-infrastructure-fiber-links","title":"Example 1: Physical Infrastructure (Fiber Links)","text":"<p>One common use case is modeling physical infrastructure. For fiber links, you might use a hierarchy like Path -&gt; Conduit -&gt; Fiber Pair.</p> <pre><code>risk_groups:\n  # Top-level definitions for referenceable groups\n  - name: \"Conduit_NYC_CHI_C[1-2]\"\n    attrs:\n      type: fiber_conduit\n\n  # Hierarchy defines cascading relationships\n  - name: \"Path_NYC_CHI\"\n    attrs:\n      type: fiber_path\n      distance_km: 1200\n    children:\n      - name: \"Conduit_NYC_CHI_C[1-2]\"\n\nnetwork:\n  nodes:\n    NYC: {}\n    CHI: {}\n  links:\n    - source: NYC\n      target: CHI\n      risk_groups: [\"Conduit_NYC_CHI_C1\"]\n      attrs:\n        fiber:\n          path_id: \"NYC-CHI\"\n          conduit_id: \"NYC-CHI-C1\"\n</code></pre> <p>Cascading behavior:</p> <ul> <li>Fiber pair failure affects only that pair</li> <li>Conduit failure affects all pairs in that conduit</li> <li>Path failure affects all conduits in that path</li> </ul>"},{"location":"reference/dsl/#example-2-physical-infrastructure-data-center-nodes","title":"Example 2: Physical Infrastructure (Data Center Nodes)","text":"<p>For data center nodes, you might model facility infrastructure with a hierarchy like Building -&gt; Room -&gt; Power Zone.</p> <pre><code>risk_groups:\n  # Top-level definitions for referenceable groups\n  - name: \"PowerZone_DC1_R1_PZ[A,B]\"\n    attrs:\n      type: power_zone\n\n  # Hierarchy defines cascading relationships\n  - name: \"Building_DC1\"\n    attrs:\n      type: building\n      location: \"Ashburn, VA\"\n    children:\n      - name: \"Room_DC1_R[1-3]\"\n        attrs:\n          type: room\n        children:\n          - name: \"PowerZone_DC1_R1_PZ[A,B]\"\n\nnetwork:\n  nodes:\n    Router_DC1_R1_RK01:\n      risk_groups: [\"PowerZone_DC1_R1_PZA\"]\n      attrs:\n        facility:\n          building_id: \"DC1\"\n          room_id: \"DC1-R1\"\n          power_zone: \"DC1-R1-PZ-A\"\n</code></pre> <p>Cascading behavior:</p> <ul> <li>Power zone failure affects equipment in that zone</li> <li>Room failure affects all zones in that room</li> <li>Building failure affects entire site</li> </ul>"},{"location":"reference/dsl/#example-3-geographicadministrative-grouping","title":"Example 3: Geographic/Administrative Grouping","text":"<p>For geographic or administrative modeling:</p> <pre><code>risk_groups:\n  - name: \"Region_West\"\n    attrs:\n      type: geographic\n    children:\n      - name: \"AZ_US_West_1a\"\n      - name: \"AZ_US_West_1b\"\n  - name: \"MaintenanceWindow_Weekend\"\n    attrs:\n      type: operational\n      schedule: \"Sat-Sun 02:00-06:00 UTC\"\n</code></pre>"},{"location":"reference/dsl/#membership-rules","title":"Membership Rules","text":"<p>Dynamically assign entities to risk groups based on attributes:</p> <pre><code>risk_groups:\n  - name: Conduit_NYC_CHI_C1\n    membership:\n      scope: link\n      match:\n        logic: and           # \"and\" or \"or\" (default: \"and\")\n        conditions:\n          - attr: fiber.conduit_id\n            op: \"==\"\n            value: \"NYC-CHI-C1\"\n\n  - name: PowerZone_DC1_R1_PZA\n    membership:\n      scope: node\n      match:\n        logic: and\n        conditions:\n          - attr: facility.power_zone\n            op: \"==\"\n            value: \"DC1-R1-PZ-A\"\n</code></pre> <p>Note: Membership rules default to <code>logic: \"and\"</code> (stricter than link/demand selectors which default to <code>\"or\"</code>). This ensures precise entity matching for failure correlation.</p>"},{"location":"reference/dsl/#generated-risk-groups","title":"Generated Risk Groups","text":"<p>Automatically create risk groups from entity attributes:</p> <pre><code>risk_groups:\n  # Generate risk groups from fiber path attributes on links\n  - generate:\n      scope: link\n      group_by: fiber.path_id\n      name: Path_${value}\n      attrs:\n        type: fiber_path\n\n  # Generate risk groups from facility attributes on nodes\n  - generate:\n      scope: node\n      group_by: facility.building_id\n      name: Building_${value}\n      attrs:\n        type: building\n</code></pre>"},{"location":"reference/dsl/#validation","title":"Validation","text":"<p>Risk group references are validated at scenario load time:</p> <p>Undefined Reference Detection: All risk group names referenced by nodes and links must exist in the <code>risk_groups</code> section. This catches typos and missing definitions early:</p> <pre><code># This will fail validation\nnetwork:\n  nodes:\n    Router1:\n      risk_groups: [\"PowerZone_A\"]  # References undefined risk group\n\nrisk_groups:\n  - name: \"PowerZone_B\"  # Only PowerZone_B is defined\n</code></pre> <p>Circular Hierarchy Detection: Parent-child relationships cannot form cycles:</p> <pre><code># This will fail validation\nrisk_groups:\n  - name: \"GroupA\"\n    children:\n      - name: \"GroupB\"\n        children:\n          - name: \"GroupA\"  # Error: circular reference\n</code></pre> <p>Validation errors list affected entities and undefined groups to aid debugging.</p>"},{"location":"reference/dsl/#vars-yaml-anchors","title":"<code>vars</code> - YAML Anchors","text":"<p>Defines reusable values using YAML anchors (<code>&amp;name</code>) and aliases (<code>*name</code>) for deduplicating complex scenarios:</p> <pre><code>vars:\n  default_cap: &amp;cap 10000\n  base_attrs: &amp;attrs {cost: 100, region: \"dc1\"}\n  spine_config: &amp;spine_cfg\n    hardware:\n      component: \"SpineRouter\"\n      count: 1\n    power_budget: 2500\n\nnetwork:\n  nodes:\n    spine-1: {attrs: {&lt;&lt;: *attrs, &lt;&lt;: *spine_cfg, capacity: *cap}}\n    spine-2: {attrs: {&lt;&lt;: *attrs, &lt;&lt;: *spine_cfg, capacity: *cap, region: \"dc2\"}}\n</code></pre> <p>Anchor Types:</p> <ul> <li>Scalar: <code>&amp;cap 10000</code> - Reference primitive values</li> <li>Mapping: <code>&amp;attrs {cost: 100}</code> - Reference objects</li> <li>Merge: <code>&lt;&lt;: *attrs</code> - Merge properties with override capability</li> </ul> <p>Processing Behavior:</p> <ul> <li>Anchors are resolved during YAML parsing, before schema validation</li> <li>The <code>vars</code> section itself is ignored by NetGraph runtime logic</li> <li>Anchors can be defined in any section, not just <code>vars</code></li> <li>Merge operations follow YAML 1.1 semantics (later keys override earlier ones)</li> </ul>"},{"location":"reference/dsl/#demands-traffic-analysis","title":"<code>demands</code> - Traffic Analysis","text":"<p>Define traffic demand patterns for capacity analysis:</p> <pre><code>demands:\n  production:\n    # Simple string pattern selectors\n    - source: \"^servers/.*\"\n      target: \"^storage/.*\"\n      volume: 1000\n      mode: \"combine\"\n      priority: 1\n      flow_policy: \"SHORTEST_PATHS_ECMP\"\n\n    # Dict selectors with attribute-based grouping\n    - source:\n        group_by: \"dc\"           # Group nodes by datacenter attribute\n      target:\n        group_by: \"dc\"\n      volume: 500\n      mode: \"pairwise\"\n      priority: 2\n\n    # Dict selectors with filtering\n    - source:\n        path: \"^dc1/.*\"\n        match:\n          conditions:\n            - attr: \"role\"\n              op: \"==\"\n              value: \"leaf\"\n      target:\n        path: \"^dc2/.*\"\n        match:\n          conditions:\n            - attr: \"role\"\n              op: \"==\"\n              value: \"spine\"\n      volume: 200\n      mode: \"combine\"\n</code></pre>"},{"location":"reference/dsl/#variable-expansion-in-demands","title":"Variable Expansion in Demands","text":"<p>Use an <code>expand</code> block to generate multiple demands from a template:</p> <pre><code>demands:\n  inter_dc:\n    - source: \"^${src_dc}/.*\"\n      target: \"^${dst_dc}/.*\"\n      volume: 100\n      mode: \"combine\"\n      expand:\n        vars:\n          src_dc: [\"dc1\", \"dc2\"]\n          dst_dc: [\"dc2\", \"dc3\"]\n        mode: \"cartesian\"  # All combinations (default)\n\n    - source: \"^${dc}/leaf/.*\"\n      target: \"^${dc}/spine/.*\"\n      volume: 50\n      mode: \"pairwise\"\n      expand:\n        vars:\n          dc: [\"dc1\", \"dc2\", \"dc3\"]\n        mode: \"zip\"        # Paired by index\n</code></pre> <p>Expansion Modes:</p> <ul> <li><code>cartesian</code>: All combinations of variable values (default)</li> <li><code>zip</code>: Pair values by index (lists must have equal length)</li> </ul>"},{"location":"reference/dsl/#demand-fields","title":"Demand Fields","text":"Field Type Description <code>source</code> string or selector Source node selector (required) <code>target</code> string or selector Target node selector (required) <code>volume</code> number Traffic demand volume (default: 0) <code>priority</code> integer Priority class; lower = higher priority (default: 0) <code>mode</code> string Node pairing mode: <code>combine</code> or <code>pairwise</code> (default: <code>combine</code>) <code>group_mode</code> string How grouped nodes produce demands (default: <code>flatten</code>) <code>flow_policy</code> string Routing policy preset name <code>id</code> string Unique demand identifier (auto-generated if omitted) <code>attrs</code> object Arbitrary metadata <code>expand</code> object Variable expansion block"},{"location":"reference/dsl/#selector-fields","title":"Selector Fields","text":"<p>The <code>source</code> and <code>target</code> fields accept either:</p> <ul> <li>A string regex pattern matched against node names</li> <li>A selector object with <code>path</code>, <code>group_by</code>, and/or <code>match</code> fields</li> </ul>"},{"location":"reference/dsl/#traffic-modes-mode","title":"Traffic Modes (<code>mode</code>)","text":"<p>Controls how source and target node sets are paired:</p> <ul> <li><code>combine</code>: Aggregate all sources into one virtual source, all targets into one virtual target. Produces a single flow.</li> <li><code>pairwise</code>: Create individual flows between all source-target node pairs. Volume is distributed across pairs.</li> </ul>"},{"location":"reference/dsl/#group-modes-group_mode","title":"Group Modes (<code>group_mode</code>)","text":"<p>When using <code>group_by</code> selectors, controls how grouped nodes produce demands:</p> <ul> <li><code>flatten</code> (default): Flatten all groups into a single source/target set, then apply <code>mode</code></li> <li><code>per_group</code>: Create separate demands for each group independently</li> <li><code>group_pairwise</code>: Create demands between each source group and each target group pair</li> </ul> <p>Example with <code>group_mode</code>:</p> <pre><code>demands:\n  inter_dc_traffic:\n    # Each DC pair gets its own demand\n    - source:\n        group_by: \"dc\"\n      target:\n        group_by: \"dc\"\n      volume: 1000\n      mode: \"combine\"\n      group_mode: \"group_pairwise\"    # Creates dc1-&gt;dc2, dc1-&gt;dc3, dc2-&gt;dc3, etc.\n</code></pre> <code>mode</code> <code>group_mode</code> Result <code>combine</code> <code>flatten</code> Single aggregated demand across all nodes <code>combine</code> <code>per_group</code> One demand per source group (targets combined) <code>combine</code> <code>group_pairwise</code> One demand per (source_group, target_group) pair <code>pairwise</code> <code>flatten</code> Individual demands for each (src_node, tgt_node) pair <code>pairwise</code> <code>per_group</code> Pairwise within each group <code>pairwise</code> <code>group_pairwise</code> Pairwise for each group pair combination"},{"location":"reference/dsl/#flow-policies","title":"Flow Policies","text":"<ul> <li><code>SHORTEST_PATHS_ECMP</code>: IP/IGP routing with hash-based ECMP; equal split across equal-cost paths</li> <li><code>SHORTEST_PATHS_WCMP</code>: IP/IGP routing with weighted ECMP; proportional split by link capacity</li> <li><code>TE_WCMP_UNLIM</code>: MPLS-TE / SDN with capacity-aware WCMP; unlimited tunnels</li> <li><code>TE_ECMP_16_LSP</code>: MPLS-TE with exactly 16 ECMP LSPs per demand</li> <li><code>TE_ECMP_UP_TO_256_LSP</code>: MPLS-TE with up to 256 ECMP LSPs per demand</li> </ul> <p>See Flow Policy Presets for detailed configuration mapping and real-world network behavior.</p>"},{"location":"reference/dsl/#failures-failure-simulation","title":"<code>failures</code> - Failure Simulation","text":"<p>Define failure policies for resilience testing:</p> <pre><code>failures:\n  single_link_failure:\n    modes:                       # Weighted modes; exactly one mode fires per iteration\n      - weight: 1.0\n        rules:\n          - scope: \"link\"\n            mode: \"choice\"\n            count: 1\n\n  weighted_modes:                # Example of weighted multi-mode policy\n    modes:\n      - weight: 0.30\n        rules:\n          - scope: \"risk_group\"\n            mode: \"choice\"\n            count: 1\n            weight_by: distance_km\n\n      - weight: 0.35\n        rules:\n          - scope: \"link\"\n            mode: \"choice\"\n            count: 3\n            match:                       # Attribute conditions inside match block\n              logic: \"and\"\n              conditions:\n                - attr: \"link_type\"\n                  op: \"==\"\n                  value: \"dc_to_pop\"\n            weight_by: target_capacity\n\n      - weight: 0.25\n        rules:\n          - scope: \"node\"\n            path: \"^dc[1-3]/.*\"          # Path filter (regex)\n            mode: \"choice\"\n            count: 1\n            match:\n              logic: \"and\"\n              conditions:\n                - attr: \"node_type\"\n                  op: \"!=\"\n                  value: \"dc_region\"\n            weight_by: attached_capacity_gbps\n\n      - weight: 0.10\n        rules:\n          - scope: \"link\"\n            mode: \"choice\"\n            count: 4\n            match:\n              logic: \"or\"                # Any condition matches\n              conditions:\n                - attr: \"link_type\"\n                  op: \"==\"\n                  value: \"leaf_spine\"\n                - attr: \"link_type\"\n                  op: \"==\"\n                  value: \"intra_group\"\n                - attr: \"link_type\"\n                  op: \"==\"\n                  value: \"inter_group\"\n</code></pre>"},{"location":"reference/dsl/#policy-level-fields","title":"Policy-Level Fields","text":"Field Type Default Description <code>modes</code> array required List of weighted failure modes <code>attrs</code> object <code>{}</code> Policy metadata (e.g., description) <code>expand_groups</code> boolean <code>false</code> When a risk group is selected, also fail its member nodes/links <code>expand_children</code> boolean <code>false</code> When a risk group is selected, recursively fail child risk groups <p>Risk group expansion example:</p> <pre><code>failures:\n  srlg_failures:\n    expand_groups: true      # Fail all links in selected risk group\n    expand_children: true    # Also fail nested child groups\n    attrs:\n      description: \"Shared-risk link group failure simulation\"\n    modes:\n      - weight: 1.0\n        rules:\n          - scope: \"risk_group\"\n            mode: \"choice\"\n            count: 1\n</code></pre>"},{"location":"reference/dsl/#selection-modes","title":"Selection Modes","text":"<ul> <li><code>all</code>: Select all matching entities</li> <li><code>choice</code>: Select specific count of entities (optionally weighted)</li> <li><code>random</code>: Select each entity independently with given probability</li> </ul>"},{"location":"reference/dsl/#rule-fields","title":"Rule Fields","text":"Field Type Description <code>scope</code> string Entity type: <code>node</code>, <code>link</code>, or <code>risk_group</code> (required) <code>mode</code> string Selection mode: <code>all</code>, <code>choice</code>, or <code>random</code> (default: <code>all</code>) <code>count</code> integer Number of entities to select (for <code>choice</code> mode) <code>probability</code> number Selection probability 0-1 (for <code>random</code> mode) <code>path</code> string Regex filter on entity name <code>match</code> object Attribute conditions block with <code>logic</code> and <code>conditions</code> <code>weight_by</code> string Attribute name for weighted sampling in <code>choice</code> mode"},{"location":"reference/dsl/#notes","title":"Notes","text":"<ul> <li>Policies are mode-based. Each mode has a non-negative <code>weight</code>. One mode is chosen per iteration with probability proportional to weights, then all rules in that mode are applied and their selections are unioned.</li> <li>Condition syntax uses the same operators as link/demand selectors. See Condition Operators for the full reference.</li> </ul>"},{"location":"reference/dsl/#workflow-execution-steps","title":"<code>workflow</code> - Execution Steps","text":"<p>Define analysis workflow steps:</p> <pre><code>workflow:\n  - type: NetworkStats\n    name: network_statistics\n  - type: MaximumSupportedDemand\n    name: msd_baseline\n    demand_set: baseline_traffic_matrix\n  - type: TrafficMatrixPlacement\n    name: tm_placement\n    demand_set: baseline_traffic_matrix\n    failure_policy: weighted_modes\n    iterations: 1000\n</code></pre> <p>Common Steps:</p> <ul> <li><code>BuildGraph</code>: Export graph to JSON (node-link) for external analysis</li> <li><code>NetworkStats</code>: Compute basic statistics</li> <li><code>MaxFlow</code>: Monte Carlo capacity analysis between node groups</li> <li><code>TrafficMatrixPlacement</code>: Monte Carlo demand placement for a named demand set</li> <li><code>MaximumSupportedDemand</code>: Search for <code>alpha_star</code> (scaling factor) for a named demand set</li> <li><code>CostPower</code>: Aggregate capex and power by network hierarchy level</li> </ul> <p>See Workflow Reference for detailed configuration.</p>"},{"location":"reference/dsl/#node-selection","title":"Node Selection","text":"<p>NetGraph provides a unified selector system for selecting and grouping nodes across links, demands, and workflow steps.</p>"},{"location":"reference/dsl/#selector-forms","title":"Selector Forms","text":"<p>Selectors can be specified as:</p> <ol> <li>String pattern: A regex matched against node names (anchored at start via <code>re.match()</code>)</li> <li>Selector object: A dict with <code>path</code>, <code>group_by</code>, and/or <code>match</code> fields</li> </ol> <p>At least one of <code>path</code>, <code>group_by</code>, or <code>match</code> must be specified in a selector object.</p>"},{"location":"reference/dsl/#string-pattern-examples","title":"String Pattern Examples","text":"<pre><code># Exact match\nsource: \"spine-1\"\n\n# Prefix match\nsource: \"dc1/spine/\"\n\n# Wildcard patterns\nsource: \"dc1/leaf.*\"\n\n# Anchored patterns\nsource: \"^dc1/spine/switch-[1-3]$\"\n\n# Alternation\nsource: \"^dc1/(spine|leaf)/.*$\"\n</code></pre>"},{"location":"reference/dsl/#capturing-groups-for-node-grouping","title":"Capturing Groups for Node Grouping","text":"<p>Regex capturing groups create node groupings for analysis:</p> <pre><code># Single group: (dc\\d+)\n# Creates groups: \"dc1\", \"dc2\", etc.\n\n# Multiple groups: (dc\\d+)/(spine|leaf)/switch-(\\d+)\n# Creates groups: \"dc1|spine|1\", \"dc1|leaf|2\", etc.\n</code></pre> <p>Group Behavior:</p> <ul> <li>Single capturing group: Group by captured value</li> <li>Multiple capturing groups: Join with <code>|</code> separator</li> <li>No capturing groups: Group by original pattern string</li> </ul>"},{"location":"reference/dsl/#attribute-based-grouping","title":"Attribute-based Grouping","text":"<p>Use the <code>group_by</code> field to group nodes by an attribute value:</p> <pre><code># Group by metro attribute\nsource:\n  group_by: \"metro\"\n\n# Combine with path filtering\nsource:\n  path: \"^dc1/.*\"\n  group_by: \"role\"\n</code></pre> <p>Notes:</p> <ul> <li><code>group_by</code> refers to a key in <code>node.attrs</code>. Nested keys are not supported.</li> <li>Nodes without the specified attribute are omitted.</li> <li>Group labels are the string form of the attribute value.</li> </ul>"},{"location":"reference/dsl/#attribute-based-filtering","title":"Attribute-based Filtering","text":"<p>Use the <code>match</code> field to filter nodes by attribute conditions:</p> <pre><code>source:\n  path: \"^dc1/.*\"\n  match:\n    logic: \"and\"           # \"and\" or \"or\" (default: \"or\")\n    conditions:\n      - attr: \"role\"\n        op: \"==\"\n        value: \"leaf\"\n      - attr: \"tier\"\n        op: \"&gt;=\"\n        value: 2\n</code></pre> <p>See Condition Operators for the full list of supported operators and condition syntax.</p>"},{"location":"reference/dsl/#active-only-filtering","title":"Active-Only Filtering","text":"<p>Use <code>active_only</code> to control whether disabled nodes are included:</p> <pre><code># Override default to include disabled nodes in demands\nsource:\n  path: \"^dc1/.*\"\n  active_only: false    # Include disabled nodes (overrides demand default of true)\n\n# Override default to exclude disabled nodes in links\nsource:\n  path: \"^dc1/.*\"\n  active_only: true     # Exclude disabled nodes (overrides link default of false)\n</code></pre> <p>Context defaults (see Context-Aware Defaults):</p> Context Default <code>active_only</code> Rationale Links <code>false</code> Create links to disabled nodes Demands <code>true</code> Only route traffic through active nodes Workflow <code>true</code> Analyze only active topology"},{"location":"reference/dsl/#workflow-examples","title":"Workflow Examples","text":"<pre><code>workflow:\n  - type: MaxFlow\n    source:\n      group_by: \"metro\"      # Group by metro attribute\n    target: \"^metro2/.*\"     # String pattern\n    mode: \"pairwise\"\n</code></pre>"},{"location":"reference/dsl/#link-examples","title":"Link Examples","text":"<pre><code>network:\n  links:\n    - source:\n        group_by: \"role\"\n      target:\n        path: \"^dc2/leaf/.*\"\n      pattern: mesh\n</code></pre>"},{"location":"reference/dsl/#notes_1","title":"Notes","text":"<ul> <li>For links, risk groups, and failure policies, use <code>conditions</code> with an <code>attr</code> field in rules (see Failure Simulation).</li> <li>Blueprint scoping: In blueprints, paths are relative to the blueprint instantiation path.</li> </ul>"},{"location":"reference/schemas/","title":"JSON Schema Validation","text":"<p>Quick links:</p> <ul> <li>Design \u2014 architecture, model, algorithms, workflow</li> <li>DSL Reference \u2014 YAML syntax for scenario definition</li> <li>Workflow Reference \u2014 analysis workflow configuration and execution</li> <li>CLI Reference \u2014 command-line tools for running scenarios</li> <li>API Reference \u2014 Python API for programmatic scenario creation</li> <li>Auto-Generated API Reference \u2014 complete class and method documentation</li> </ul> <p>NetGraph includes JSON Schema definitions for YAML scenario files, providing IDE validation, autocompletion, and automated testing.</p>"},{"location":"reference/schemas/#schema-location","title":"Schema Location","text":"<p>The schema is packaged with the library at: <code>ngraph/schemas/scenario.json</code>.</p> <p>This file validates NetGraph scenario YAML structure including network topology, blueprints, risk groups, failure policies, traffic matrices, workflows, and components.</p>"},{"location":"reference/schemas/#validation-scope","title":"Validation Scope","text":"<p>The schema validates:</p> <ul> <li>YAML syntax and data types</li> <li>Required fields and property structure</li> <li>Top-level section organization</li> <li>Basic constraint checking</li> </ul> <p>Runtime: The schema is applied unconditionally during load in <code>ngraph.scenario.Scenario.from_yaml</code>. Additional business rules are enforced in code (e.g., blueprint expansion) and may still raise errors for semantically invalid inputs.</p>"},{"location":"reference/schemas/#ide-integration-vs-code","title":"IDE Integration (VS Code)","text":"<p>Automatic configuration via <code>.vscode/settings.json</code>:</p> <pre><code>{\n  \"yaml.schemas\": {\n    \"./ngraph/schemas/scenario.json\": [\n      \"scenarios/**/*.yaml\",\n      \"scenarios/**/*.yml\"\n    ]\n  }\n}\n</code></pre> <p>Provides real-time validation, autocompletion, inline documentation, and error highlighting.</p>"},{"location":"reference/schemas/#automated-validation","title":"Automated Validation","text":""},{"location":"reference/schemas/#development-workflow","title":"Development Workflow","text":"<pre><code># Validate all scenarios\nmake validate\n\n# Full validation and tests\nmake check\n</code></pre>"},{"location":"reference/schemas/#integration-points","title":"Integration Points","text":"<ul> <li>Pre-commit hooks: Validates modified <code>scenarios/*.yaml</code> files</li> <li>CI pipeline: Validates scenarios on push/PR</li> <li>Test suite: Validation exercised in integration tests</li> </ul>"},{"location":"reference/schemas/#python-api","title":"Python API","text":"<pre><code>import json\nimport yaml\nimport jsonschema\n\n# Load and validate\nfrom importlib import resources as res\n\nwith res.files('ngraph.schemas').joinpath('scenario.json').open('r', encoding='utf-8') as f:\n    schema = json.load(f)\n\nwith open('scenarios/square_mesh.yaml') as f:\n    data = yaml.safe_load(f)\n\njsonschema.validate(data, schema)\n</code></pre>"},{"location":"reference/schemas/#schema-structure","title":"Schema Structure","text":"<p>Top-level sections (only these keys allowed):</p> <ul> <li><code>network</code> - Network topology definition</li> <li><code>blueprints</code> - Reusable network templates</li> <li><code>risk_groups</code> - Risk group definitions</li> <li><code>failures</code> - Named failure policies</li> <li><code>demands</code> - Named demand sets</li> <li><code>workflow</code> - Workflow step definitions</li> <li><code>components</code> - Hardware component library</li> <li><code>vars</code> - YAML anchors and variables for reuse</li> <li><code>seed</code> - Master random seed for reproducibility</li> </ul>"},{"location":"reference/schemas/#schema-maintenance","title":"Schema Maintenance","text":"<p>Update triggers:</p> <ul> <li>New top-level sections added</li> <li>Property types or validation rules change</li> <li>New workflow step types</li> </ul> <p>Update process:</p> <ol> <li>Implement feature in <code>ngraph/scenario.py</code> validation logic</li> <li>Test runtime validation</li> <li>Update JSON Schema to match implementation</li> <li>Run <code>make test</code> to verify schema tests</li> <li>Update documentation</li> </ol> <p>Authority: code implementation in <code>ngraph/scenario.py</code> and <code>ngraph/dsl/blueprints/expand.py</code> is authoritative, not the schema.</p>"},{"location":"reference/workflow/","title":"Workflow Reference","text":"<p>Quick links:</p> <ul> <li>Design \u2014 architecture, model, algorithms, workflow</li> <li>DSL Reference \u2014 YAML syntax for scenario definition</li> <li>CLI Reference \u2014 command-line tools for running scenarios</li> <li>API Reference \u2014 Python API for programmatic scenario creation</li> <li>Auto-Generated API Reference \u2014 complete class and method documentation</li> </ul> <p>This document describes NetGraph workflows \u2013 analysis execution pipelines that perform capacity analysis, demand placement, and statistics computation.</p>"},{"location":"reference/workflow/#overview","title":"Overview","text":"<p>Workflows are ordered steps executed on a scenario. Each step computes a result (e.g., stats, Monte Carlo analysis, export) and writes it under its step name in the results store.</p> <pre><code>workflow:\n  - type: NetworkStats\n    name: network_statistics\n  - type: MaximumSupportedDemand\n    name: msd_baseline\n    demand_set: baseline_traffic_matrix\n  - type: TrafficMatrixPlacement\n    name: tm_placement\n    demand_set: baseline_traffic_matrix\n    failure_policy: random_failures\n    iterations: 1000\n</code></pre>"},{"location":"reference/workflow/#execution-model","title":"Execution Model","text":"<ul> <li>Steps run sequentially via <code>WorkflowStep.execute()</code>, which records timing and metadata and stores outputs under <code>{metadata, data}</code> for the step.</li> <li>Monte Carlo steps (<code>MaxFlow</code>, <code>TrafficMatrixPlacement</code>) execute iterations using the Failure Manager. Each iteration analyzes the network with exclusion sets applied to mask failed nodes/links without mutating the base network. Workers are controlled by <code>parallelism: auto|int</code>.</li> <li>Seeding: a scenario-level <code>seed</code> derives per-step seeds unless a step sets an explicit <code>seed</code>. Metadata includes <code>scenario_seed</code>, <code>step_seed</code>, <code>seed_source</code>, and <code>active_seed</code>.</li> </ul>"},{"location":"reference/workflow/#core-workflow-steps","title":"Core Workflow Steps","text":""},{"location":"reference/workflow/#buildgraph","title":"BuildGraph","text":"<p>Validates network topology and exports node-link JSON for external analysis. Optional for other workflow steps.</p> <pre><code>- type: BuildGraph\n  name: build_graph\n  add_reverse: true  # Add reverse edges for bidirectional connectivity (default: true)\n</code></pre> <p>Parameters:</p> <ul> <li><code>add_reverse</code>: If <code>true</code>, adds reverse edges for each link to enable bidirectional connectivity. Set to <code>false</code> for directed-only graphs. Default: <code>true</code>.</li> </ul>"},{"location":"reference/workflow/#networkstats","title":"NetworkStats","text":"<p>Compute node, link, and degree metrics. Supports temporary exclusions without modifying the base network.</p> <pre><code>- type: NetworkStats\n  name: baseline_stats\n  include_disabled: false           # Include disabled nodes/links in stats\n  excluded_nodes: []                # Optional: Temporary node exclusions\n  excluded_links: []                # Optional: Temporary link exclusions\n</code></pre> <p>Parameters:</p> <ul> <li><code>include_disabled</code>: If <code>true</code>, include disabled nodes and links in statistics. Default: <code>false</code>.</li> <li><code>excluded_nodes</code>: Optional list of node names to exclude temporarily (does not modify network).</li> <li><code>excluded_links</code>: Optional list of link IDs to exclude temporarily (does not modify network).</li> </ul>"},{"location":"reference/workflow/#maxflow","title":"MaxFlow","text":"<p>Monte Carlo maximum flow analysis between node groups. Baseline (no failures) is always run first as a separate reference.</p> <pre><code>- type: MaxFlow\n  name: capacity_analysis\n  source: \"^servers/.*\"\n  target: \"^storage/.*\"\n  mode: \"combine\"              # combine | pairwise\n  failure_policy: random_failures\n  iterations: 1000             # Number of failure iterations\n  parallelism: auto             # or an integer\n  shortest_path: false\n  require_capacity: true        # false for true IP/IGP semantics\n  flow_placement: PROPORTIONAL  # or EQUAL_BALANCED\n  store_failure_patterns: false\n  include_flow_details: false   # cost_distribution per flow\n  include_min_cut: false        # per-flow min-cut edge list\n</code></pre>"},{"location":"reference/workflow/#trafficmatrixplacement","title":"TrafficMatrixPlacement","text":"<p>Monte Carlo placement of a named demand set with optional alpha scaling. Baseline (no failures) is always run first as a separate reference.</p> <pre><code>- type: TrafficMatrixPlacement\n  name: tm_placement\n  demand_set: default\n  failure_policy: random_failures        # Optional: policy name in failures section\n  iterations: 100                # Number of failure iterations\n  parallelism: auto\n  placement_rounds: auto         # or an integer\n  include_flow_details: true     # cost_distribution per flow\n  include_used_edges: false      # include per-demand used edge lists\n  store_failure_patterns: false\n  # Alpha scaling \u2013 explicit or from another step\n  alpha: 1.0\n  # alpha_from_step: msd_default\n  # alpha_from_field: data.alpha_star\n</code></pre> <p>Outputs:</p> <ul> <li>metadata: iterations, parallelism, analysis_function, policy_name,   execution_time, unique_patterns</li> <li>data.context: demand_set, placement_rounds, include_flow_details,   include_used_edges, base_demands, alpha, alpha_source</li> </ul>"},{"location":"reference/workflow/#maximumsupporteddemand","title":"MaximumSupportedDemand","text":"<p>Search for the maximum uniform traffic multiplier <code>alpha_star</code> that is fully placeable.</p> <pre><code>- type: MaximumSupportedDemand\n  name: msd_default\n  demand_set: default\n  acceptance_rule: hard          # Currently only \"hard\" is supported\n  alpha_start: 1.0               # Starting alpha value for search\n  growth_factor: 2.0             # Growth factor for bracketing (must be &gt; 1.0)\n  alpha_min: 0.000001            # Minimum alpha bound (default: 1e-6)\n  alpha_max: 1000000000.0        # Maximum alpha bound (default: 1e9)\n  resolution: 0.01               # Convergence resolution for bisection\n  max_bracket_iters: 32          # Maximum bracketing iterations\n  max_bisect_iters: 32           # Maximum bisection iterations\n  placement_rounds: auto         # Placement optimization rounds\n</code></pre> <p>Parameters:</p> <ul> <li><code>demand_set</code>: Name of the demand set to analyze (default: \"default\").</li> <li><code>acceptance_rule</code>: Acceptance rule for feasibility (currently only \"hard\" is supported).</li> <li><code>alpha_start</code>: Initial alpha value to probe.</li> <li><code>growth_factor</code>: Multiplier for bracketing phase (must be &gt; 1.0).</li> <li><code>alpha_min</code>: Minimum alpha bound for search.</li> <li><code>alpha_max</code>: Maximum alpha bound for search.</li> <li><code>resolution</code>: Convergence threshold for bisection.</li> <li><code>max_bracket_iters</code>: Maximum iterations for bracketing phase.</li> <li><code>max_bisect_iters</code>: Maximum iterations for bisection phase.</li> <li><code>placement_rounds</code>: Number of placement optimization rounds (<code>int</code> or <code>\"auto\"</code>).</li> </ul> <p>Outputs:</p> <ul> <li>data.alpha_star: maximum uniform scaling factor</li> <li>data.context: search parameters</li> <li>data.base_demands: serialized base demands prior to scaling</li> <li>data.probes: bracket/bisect evaluations with feasibility and min ratios</li> </ul>"},{"location":"reference/workflow/#costpower","title":"CostPower","text":"<p>Aggregate platform and optics capex/power by hierarchy level (split by <code>/</code>).</p> <pre><code>- type: CostPower\n  name: cost_power\n  include_disabled: false\n  aggregation_level: 2\n</code></pre> <p>Outputs:</p> <ul> <li>data.context: include_disabled, aggregation_level</li> <li>data.levels: mapping level-&gt;list of {path, platform_capex, platform_power_watts,   optics_capex, optics_power_watts, capex_total, power_total_watts}</li> </ul>"},{"location":"reference/workflow/#node-selection-mechanism","title":"Node Selection Mechanism","text":"<p>Workflow steps use a unified selector system for node selection. Selectors can be specified as string patterns or selector objects.</p>"},{"location":"reference/workflow/#string-pattern-matching","title":"String Pattern Matching","text":"<pre><code># Exact match\nsource: \"spine-1\"\n\n# Prefix match\nsource: \"datacenter/servers/\"\n\n# Pattern match\nsource: \"^pod[1-3]/leaf/.*$\"\n</code></pre>"},{"location":"reference/workflow/#selector-objects","title":"Selector Objects","text":"<pre><code># Attribute-based grouping\nsource:\n  group_by: \"dc\"\n\n# Combined path and grouping\nsource:\n  path: \"^datacenter/.*\"\n  group_by: \"role\"\n\n# With attribute filtering\nsource:\n  path: \"^pod[1-3]/.*\"\n  match:\n    conditions:\n      - attr: \"tier\"\n        op: \"==\"\n        value: \"leaf\"\n</code></pre>"},{"location":"reference/workflow/#capturing-groups-for-node-grouping","title":"Capturing Groups for Node Grouping","text":"<p>No Capturing Groups: All matching nodes form one group labeled by the pattern.</p> <pre><code>source: \"edge/.*\"\n# Creates one group: \"edge/.*\" containing all matching nodes\n</code></pre> <p>Single Capturing Group: Each unique captured value creates a separate group.</p> <pre><code>source: \"(dc[1-3])/servers/.*\"\n# Creates groups: \"dc1\", \"dc2\", \"dc3\"\n# Each group contains servers from that datacenter\n</code></pre> <p>Multiple Capturing Groups: Group labels join captured values with <code>|</code>.</p> <pre><code>source: \"(dc[1-3])/(spine|leaf)/switch-(\\d+)\"\n# Creates groups: \"dc1|spine|1\", \"dc1|leaf|2\", \"dc2|spine|1\", etc.\n</code></pre>"},{"location":"reference/workflow/#attribute-based-grouping","title":"Attribute-based Grouping","text":"<pre><code># Group by node attribute value (e.g., node.attrs[\"dc\"])\nsource:\n  group_by: \"dc\"\n</code></pre>"},{"location":"reference/workflow/#flow-analysis-modes","title":"Flow Analysis Modes","text":"<p><code>combine</code> Mode: Aggregates all source matches into one virtual source, all target matches into one virtual target. Produces single flow value.</p> <p><code>pairwise</code> Mode: Computes flow between each source group and target group pair. Produces flow matrix keyed by <code>(source_group, target_group)</code>.</p>"},{"location":"reference/workflow/#maxflow-parameters","title":"MaxFlow Parameters","text":""},{"location":"reference/workflow/#required-parameters","title":"Required Parameters","text":"<ul> <li><code>source</code>: Node selector for source nodes (string pattern or selector object)</li> <li><code>target</code>: Node selector for target nodes (string pattern or selector object)</li> </ul>"},{"location":"reference/workflow/#analysis-configuration","title":"Analysis Configuration","text":"<pre><code>mode: combine                    # combine | pairwise (default: combine)\niterations: 1000                 # Failure iterations to run (default: 1)\nfailure_policy: policy_name      # Name in failures section (default: null)\nparallelism: auto                # Worker processes (default: auto)\nshortest_path: false             # Restrict to shortest paths (default: false)\nrequire_capacity: true           # Path selection considers capacity (default: true)\n                                 # Set false for true IP/IGP semantics (cost-only routing)\nflow_placement: PROPORTIONAL     # PROPORTIONAL | EQUAL_BALANCED\nstore_failure_patterns: false    # Store failure patterns in results\ninclude_flow_details: false      # Emit cost_distribution per flow\ninclude_min_cut: false           # Emit min-cut edge list per flow\n</code></pre> <p>Note: Baseline (no failures) is always run first as a separate reference. The <code>iterations</code> parameter specifies the number of failure scenarios to run.</p>"},{"location":"reference/workflow/#results-export-shape","title":"Results Export Shape","text":"<p>Exported results have a fixed top-level structure. Keys under <code>workflow</code> and <code>steps</code> are step names.</p> <pre><code>{\n  \"workflow\": {\n    \"network_statistics\": {\n      \"step_type\": \"NetworkStats\",\n      \"step_name\": \"network_statistics\",\n      \"execution_order\": 0,\n      \"scenario_seed\": 42,\n      \"step_seed\": 42,\n      \"seed_source\": \"scenario-derived\",\n      \"active_seed\": 42\n    }\n  },\n  \"steps\": {\n    \"network_statistics\": {\n      \"metadata\": { \"duration_sec\": 0.012 },\n      \"data\": { \"node_count\": 42, \"link_count\": 84 }\n    },\n    \"msd_baseline\": {\n      \"metadata\": { \"duration_sec\": 1.234 },\n      \"data\": {\n        \"alpha_star\": 1.37,\n        \"context\": { \"demand_set\": \"baseline_traffic_matrix\" }\n      }\n    },\n    \"tm_placement\": {\n      \"metadata\": { \"iterations\": 1000, \"parallelism\": 8 },\n      \"data\": {\n        \"baseline\": {\n          \"failure_id\": \"\",\n          \"failure_state\": { \"excluded_nodes\": [], \"excluded_links\": [] },\n          \"flows\": [],\n          \"summary\": { \"total_demand\": 0.0, \"total_placed\": 0.0, \"overall_ratio\": 1.0, \"dropped_flows\": 0, \"num_flows\": 0 }\n        },\n        \"flow_results\": [],\n        \"context\": { \"demand_set\": \"baseline_traffic_matrix\" }\n      }\n    }\n  },\n  \"scenario\": { \"seed\": 42, \"failures\": { }, \"demands\": { } }\n}\n</code></pre> <ul> <li><code>MaxFlow</code> and <code>TrafficMatrixPlacement</code> write results with baseline separate from failure iterations:</li> </ul> <pre><code>{\n  \"baseline\": {\n    \"failure_id\": \"\",\n    \"failure_state\": { \"excluded_nodes\": [], \"excluded_links\": [] },\n    \"failure_trace\": null,\n    \"occurrence_count\": 1,\n    \"flows\": [ ... ],\n    \"summary\": { \"total_demand\": 10.0, \"total_placed\": 10.0, \"overall_ratio\": 1.0 }\n  },\n  \"flow_results\": [\n    {\n      \"failure_id\": \"d0eea3f4d06413a2\",\n      \"failure_state\": { \"excluded_nodes\": [\"nodeA\"], \"excluded_links\": [] },\n      \"failure_trace\": { \"mode_index\": 0, \"selections\": [...], ... },\n      \"occurrence_count\": 5,\n      \"flows\": [ ... ],\n      \"summary\": { \"total_demand\": 10.0, \"total_placed\": 8.0, \"overall_ratio\": 0.8 }\n    }\n  ],\n  \"context\": { ... }\n}\n</code></pre> <p>Notes:</p> <ul> <li>Baseline is always returned separately in the <code>baseline</code> field.</li> <li><code>flow_results</code> contains K unique failure patterns (deduplicated), not N iterations.</li> <li><code>occurrence_count</code> indicates how many iterations produced each unique failure pattern.</li> <li><code>failure_id</code> is a hash of exclusions (empty string for no exclusions).</li> <li><code>failure_trace</code> contains policy selection details when <code>store_failure_patterns: true</code>.</li> <li><code>failure_state</code> contains <code>excluded_nodes</code> and <code>excluded_links</code> lists.</li> <li><code>cost_distribution</code> uses string keys for JSON stability; values are numeric.</li> <li>Effective <code>parallelism</code> and other execution fields are recorded in step metadata.</li> </ul>"}]}