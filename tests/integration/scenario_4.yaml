# Test scenario 4: Advanced DSL features demonstration
# Tests components system, variable expansion, bracket expansion, complex overrides
# and risk groups in a realistic data center fabric scenario
seed: 4004

# Component library for hardware modeling
components:
  ToRSwitch48p:
    component_type: "switch"
    description: "48-port Top of Rack switch"
    capex: 8000.0
    power_watts: 350.0
    power_watts_max: 600.0
    capacity: 2400.0 # 48 * 50G aggregate
    ports: 48
    count: 1
    attrs:
      vendor: "Arista"
      model: "7050SX3-48YC8"
      form_factor: "1RU"
    children:
      SFP28_25G:
        component_type: "optic"
        description: "25G SFP28 optic"
        capex: 150.0
        power_watts: 1.5
        capacity: 25.0
        count: 48
        attrs:
          reach: "100m"
          wavelength: "850nm"

  SpineSwitch32p:
    component_type: "switch"
    description: "32-port spine switch"
    capex: 25000.0
    power_watts: 800.0
    power_watts_max: 1000.0
    capacity: 12800.0 # 32 * 400G aggregate
    ports: 32
    count: 1
    attrs:
      vendor: "Arista"
      model: "7800R3-36DM"
      form_factor: "2RU"
    children:
      QSFP_DD_400G:
        component_type: "optic"
        description: "400G QSFP-DD optic"
        capex: 2500.0
        power_watts: 15.0
        capacity: 400.0
        count: 32
        attrs:
          reach: "2km"
          wavelength: "1310nm"

  ServerNode:
    component_type: "server"
    description: "Dual-socket server node"
    capex: 12000.0
    power_watts: 400.0
    power_watts_max: 500.0
    ports: 2
    count: 1
    attrs:
      cpu_cores: 64
      memory_gb: 512
      storage_tb: 4

# Risk groups using fiber and facility domain models
risk_groups:
  # Facility domain: Building-level power zone hierarchy
  - name: "Building_DC1"
    attrs:
      type: building
      facility:
        building_id: "DC1"
      location: "Ashburn, VA"
    children:
      - name: "PowerZone_DC1_R1_PZ[A,B]"
        attrs:
          type: power_zone
          facility:
            building_id: "DC1"
            room_id: "R1"

  - name: "Building_DC2"
    attrs:
      type: building
      facility:
        building_id: "DC2"
      location: "Ashburn, VA"

  # Facility domain: Room-level groups for spine fabric equipment
  - name: "Room_DC1_Spine"
    attrs:
      type: room
      facility:
        building_id: "DC1"
        room_id: "SpineRoom"

  - name: "Room_DC2_Spine"
    attrs:
      type: room
      facility:
        building_id: "DC2"
        room_id: "SpineRoom"

  # Facility domain: Cooling zones for racks
  - name: "CoolingZone_DC1_R1_CZA"
    attrs:
      type: cooling_zone
      facility:
        building_id: "DC1"
        room_id: "R1"
        cooling_zone: "DC1-R1-CZ-A"

  # Facility domain: Leaf switch power zone
  - name: "PowerZone_DC1_Leaf"
    attrs:
      type: power_zone
      facility:
        building_id: "DC1"
        power_zone: "DC1-Leaf-PZ-A"

  - name: "PowerZone_DC2_Leaf"
    attrs:
      type: power_zone
      facility:
        building_id: "DC2"
        power_zone: "DC2-Leaf-PZ-A"

  # Fiber domain: Inter-DC fiber conduit (physical cut affects all inter-DC links)
  - name: "Conduit_DC1_DC2_C1"
    attrs:
      type: fiber_conduit
      fiber:
        path_id: "DC1-DC2"
        conduit_id: "DC1-DC2-C1"
      distance_km: 50

  # Fiber domain: Path-level group (all conduits between DC1 and DC2)
  - name: "Path_DC1_DC2"
    attrs:
      type: fiber_path
      fiber:
        path_id: "DC1-DC2"
      distance_km: 50
    children:
      - name: "Conduit_DC1_DC2_C1"

blueprints:
  # Basic server rack with ToR switch
  server_rack:
    nodes:
      tor:
        count: 1
        template: "tor-{n}"
        attrs:
          hardware:
            component: "ToRSwitch48p"
            count: 1
          role: "top_of_rack"
        risk_groups: ["CoolingZone_DC1_R1_CZA"]
      servers:
        count: 8 # 8 servers per rack for test efficiency
        template: "srv-{n}"
        attrs:
          hardware:
            component: "ServerNode"
            count: 1
          role: "compute"
        risk_groups: ["CoolingZone_DC1_R1_CZA"]
    links:
      - source: /servers
        target: /tor
        pattern: "one_to_one"
        capacity: 25.0 # 25 Gb/s server uplinks
        cost: 1
        attrs:
          media_type: "copper"

  # Spine-leaf fabric with variable expansion
  leaf_spine_fabric:
    nodes:
      leaf:
        count: 2 # 2 leaf switches per fabric
        template: "leaf-{n}"
        attrs:
          hardware:
            component: "ToRSwitch48p"
            count: 1
          role: "leaf"
        # Leaf switches in DC1 and DC2 have their own power zones
      spine:
        count: 2 # 2 spine switches per fabric
        template: "spine-{n}"
        attrs:
          hardware:
            component: "SpineSwitch32p"
            count: 1
          role: "spine"
        # Spine switches share room-level risk (cooling/power in spine room)
    links:
      # Variable expansion for leaf-spine connectivity using $var syntax
      - source: "leaf-${leaf_id}"
        target: "spine-${spine_id}"
        expand:
          vars:
            leaf_id: [1, 2]
            spine_id: [1, 2]
          mode: "cartesian"
        pattern: "mesh"
        capacity: 400.0 # 400 Gb/s leaf-spine links
        cost: 1
        attrs:
          media_type: "fiber"
          link_type: "leaf_spine"

network:
  name: "Advanced DSL Demonstration"
  version: "2.0"

  nodes:
    # Multi-datacenter pod and rack expansion
    # Each rack group inherits building-level power zone risk
    dc[1-2]_pod[a,b]_rack[1-2]:
      blueprint: server_rack
      attrs:
        datacenter: "dc1"
        pod: "poda"
        rack: "rack1"
      risk_groups: ["Building_DC1"]

    # Fabric per DC using bracket expansion
    # Fabric equipment shares building-level risk
    dc[1-2]_fabric:
      blueprint: leaf_spine_fabric
      attrs:
        datacenter: "dc1"
      risk_groups: ["Building_DC1"]

  # Top-level adjacency with variable expansion using $var syntax
  links:
    # Connect racks to fabric using variable expansion
    - source: "dc${dc}_pod${pod}_rack${rack}/tor"
      target: "dc${dc}_fabric/leaf"
      expand:
        vars:
          dc: [1, 2]
          pod: ["a", "b"]
          rack: [1, 2]
        mode: "cartesian"
      pattern: "one_to_one"
      capacity: 100.0 # 100 Gb/s rack-to-fabric uplinks
      cost: 2
      attrs:
        connection_type: "rack_to_fabric"

    # Inter-DC spine connectivity - links traverse fiber conduit between buildings
    - source: "dc1_fabric/spine"
      target: "dc2_fabric/spine"
      pattern: "mesh"
      capacity: 400.0 # 400 Gb/s inter-DC links
      cost: 10
      risk_groups: ["Conduit_DC1_DC2_C1"]
      attrs:
        connection_type: "inter_dc"
        fiber:
          path_id: "DC1-DC2"
          conduit_id: "DC1-DC2-C1"
        distance_km: 50

  # Complex node overrides with regex patterns
  node_rules:
    # Override DC1 spine switches - assign to DC1 spine room
    - path: "dc1_fabric/spine/spine-[1-2]"
      attrs:
        hardware:
          component: "SpineSwitch32p"
          count: 1
        role: "spine"
        facility:
          building_id: "DC1"
          room_id: "SpineRoom"
      risk_groups: ["Room_DC1_Spine"]

    # Override DC2 spine switches - assign to DC2 spine room
    - path: "dc2_fabric/spine/spine-[1-2]"
      attrs:
        hardware:
          component: "SpineSwitch32p"
          count: 1
        role: "spine"
        facility:
          building_id: "DC2"
          room_id: "SpineRoom"
      risk_groups: ["Room_DC2_Spine"]

    # Override DC1 leaf switches - assign to DC1 leaf power zone
    - path: "dc1_fabric/leaf/leaf-[1-2]"
      risk_groups: ["PowerZone_DC1_Leaf"]

    # Override DC2 leaf switches - assign to DC2 leaf power zone
    - path: "dc2_fabric/leaf/leaf-[1-2]"
      risk_groups: ["PowerZone_DC2_Leaf"]

    # Override servers in specific pods for GPU workloads
    - path: "dc1_pod[ab]_rack[12]/servers/srv-[1-4]"
      attrs:
        role: "gpu_compute"
        gpu_count: 8
        hardware:
          component: "ServerNode"
          count: 1

    # Mark certain racks as disabled for maintenance
    - path: "dc2_podb_rack2/.*"
      disabled: true
      attrs:
        maintenance_status: "scheduled"

  # Complex link overrides
  link_rules:
    # Higher capacity for inter-DC links - both conduit and path level risk
    - source: "dc1_fabric/spine/.*"
      target: "dc2_fabric/spine/.*"
      bidirectional: true
      capacity: 800.0 # 800 Gb/s inter-DC links
      cost: 5
      risk_groups: ["Conduit_DC1_DC2_C1", "Path_DC1_DC2"]
      attrs:
        link_class: "inter_dc"
        encryption: "enabled"
        fiber:
          path_id: "DC1-DC2"
          conduit_id: "DC1-DC2-C1"

    # Higher capacity uplinks for specific racks
    - source: "dc1_pod[ab]_rack1/tor/.*"
      target: "dc1_fabric/leaf/.*"
      capacity: 200.0 # 200 Gb/s uplinks
      cost: 1

# Traffic patterns for realistic workloads
demands:
  default:
    # East-west traffic within DC
    - source: "dc1_pod[ab]_rack.*/servers/.*"
      target: "dc1_pod[ab]_rack.*/servers/.*"
      volume: 5.0 # 5 Gb/s east-west traffic
      mode: "pairwise"
      attrs:
        traffic_type: "east_west"

    # North-south traffic to external
    - source: "dc1_.*servers/.*"
      target: "dc2_.*servers/.*"
      volume: 10.0 # 10 Gb/s inter-DC traffic
      mode: "combine"
      attrs:
        traffic_type: "inter_dc"

  # High-performance computing workload
  hpc_workload:
    - source: "dc1_poda_rack1/servers/srv-[1-4]"
      target: "dc1_poda_rack1/servers/srv-[1-4]"
      volume: 20.0 # 20 Gb/s HPC collective communication
      mode: "pairwise"
      attrs:
        traffic_type: "hpc_collective"

# Failure policies for realistic failure scenarios
failures:
  single_link_failure:
    attrs:
      description: "Single link failure"
    modes:
      - weight: 1.0
        rules:
          - scope: "link"
            mode: "choice"
            count: 1

  single_node_failure:
    attrs:
      description: "Single node failure"
    modes:
      - weight: 1.0
        rules:
          - scope: "node"
            mode: "choice"
            count: 1

  default:
    attrs:
      description: "Random single link failure"
    modes:
      - weight: 1.0
        rules:
          - scope: "link"
            mode: "choice"
            count: 1

# Multi-step workflow demonstrating various workflow steps
workflow:
  - type: BuildGraph
    name: build_graph

  # Capacity analysis with different traffic patterns
  # Forward intra-DC capacity analysis
  - type: MaxFlow
    name: intra_dc_capacity_forward
    source: "dc1_pod[ab]_rack.*/servers/.*"
    target: "dc1_pod[ab]_rack.*/servers/.*"
    mode: "combine"
    shortest_path: false
    flow_placement: "PROPORTIONAL"
    iterations: 1
    failure_policy: null

  # Reverse intra-DC capacity analysis
  - type: MaxFlow
    name: intra_dc_capacity_reverse
    source: "dc1_pod[ab]_rack.*/servers/.*"
    target: "dc1_pod[ab]_rack.*/servers/.*"
    mode: "combine"
    shortest_path: false
    flow_placement: "PROPORTIONAL"
    iterations: 1
    failure_policy: null

  # Forward inter-DC capacity analysis
  - type: MaxFlow
    name: inter_dc_capacity_forward
    source: "dc1_.*servers/.*"
    target: "dc2_.*servers/.*"
    mode: "combine"
    shortest_path: false
    flow_placement: "EQUAL_BALANCED"
    iterations: 1
    failure_policy: null

  # Reverse inter-DC capacity analysis
  - type: MaxFlow
    name: inter_dc_capacity_reverse
    source: "dc2_.*servers/.*"
    target: "dc1_.*servers/.*"
    mode: "combine"
    shortest_path: false
    flow_placement: "EQUAL_BALANCED"
    iterations: 1
    failure_policy: null

  # Failure analysis with different policies
  - type: MaxFlow
    name: rack_failure_analysis
    source: "dc1_pod[ab]_rack.*/servers/.*"
    target: "dc1_pod[ab]_rack.*/servers/.*"
    mode: "combine"
    failure_policy: "single_link_failure"
    iterations: 10 # 10 iterations for test efficiency
    parallelism: 2 # 2-way parallelism
    shortest_path: false
    flow_placement: "PROPORTIONAL"

  - type: MaxFlow
    name: spine_failure_analysis
    source: "dc1_.*servers/.*"
    target: "dc2_.*servers/.*"
    mode: "combine"
    failure_policy: "single_node_failure"
    iterations: 20 # 20 iterations for test efficiency
    parallelism: 2 # 2-way parallelism
    shortest_path: false
    flow_placement: "EQUAL_BALANCED"
